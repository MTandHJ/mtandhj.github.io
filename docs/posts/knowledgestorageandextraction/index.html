<!DOCTYPE html>
<html lang="zh" dir="ltr">

<head>

	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />

	<title>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction | MTandHJ</title>
	<meta name="keywords" content="Note, LLM, Knowledge, Seminal, Empirical, ICML, 2024">
	<meta name="description" content="探究 LLM 如何记忆和提取知识的实验性文章">
	<link rel="canonical"
		href="https://www.mtandhj.com/posts/knowledgestorageandextraction/">



	<link rel="stylesheet" href="/css/output.css" type="text/css" media="all" />
	<link rel="stylesheet" href="/css/private.css" type="text/css" media="all" />

	
	<link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" 
    integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" 
    crossorigin="anonymous"
/>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="
    window.addEventListener('DOMContentLoaded', function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true}, // 块级公式
                {left: '$', right: '$', display: false},  // 行内公式
                {left: '\\[', right: '\\]', display: true}, // 支持 \[ \] 块级公式
                {left: '\\(', right: '\\)', display: false} // 支持 \( \) 行内公式
            ]
        });
    });
"></script>
</head>

<body class="bg-stone-50 font-custom flex flex-col justify-between min-h-screen dark:bg-stone-800 dark:text-white">

	<header
  class="site-header w-full bg-stone-100 dark:bg-stone-900 dark:text-white border-stone-400 border-b px-3 lg:px-3"
  style="padding-top: 7px; padding-bottom: 7px;">
  <nav class="site-nav flex items-center justify-between gap-3 container mx-auto " >
    <div class="flex items-center  flex-wrap">
      <a class="logo text-2xl md:text-3xl font-black" href="https://www.mtandhj.com/">
        <img src="/images/logo.png" alt="MTandHJ" 
        class="w-[35px] h-[35px] md:w-[38px] md:h-[38px] lg:w-[40px] lg:h-[40px]"
        style="border-radius:50%; border:2px solid #DCD9D6;">
      </a>
      <ul class="main-menu flex items-center gap-3 mr-5">
        
        <li>
          <a class=" text-lg" href="/pubs">俺的论文</a>
        </li>
        
        <li>
          <a class=" text-lg" href="/posts">随笔</a>
        </li>
        
        <li>
          <a class=" text-lg" href="/tags">Tags</a>
        </li>
        
        <li>
          <a class=" text-lg" href="/slides">Slides</a>
        </li>
        
      </ul>
    </div>
    

<li id="search-click" class="menu-item" style="display: none;">
    <a class="menu-item-link" href="javascript:void(0)">Search</a>
</li>

<script>
    
    document.addEventListener("keydown", function (event) {
        if ((event.ctrlKey || event.metaKey) && event.key.toLowerCase() === "k") {
            event.preventDefault();
            document.getElementById("search-click").click();
        }
    });
</script>


<div id="fastSearch">
    <input id="searchInput" autocomplete="off">
    <ul id="searchResults"></ul>
</div>

<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6"></script>

<script type="text/javascript" src="/js/search.min.243d998bd0c610c96bcfbb6d9059a4586cb855efea68a7db9409441ebaf81161.js"></script>
    <button aria-label="theme-toggle" id="toggle-theme">
      <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
    >
    <path
    class="fill-stone-800"
        d="M12 10.999c1.437.438 2.562 1.564 2.999 3.001.44-1.437 1.565-2.562 3.001-3-1.436-.439-2.561-1.563-3.001-3-.437 1.436-1.562 2.561-2.999 2.999zm8.001.001c.958.293 1.707 1.042 2 2.001.291-.959 1.042-1.709 1.999-2.001-.957-.292-1.707-1.042-2-2-.293.958-1.042 1.708-1.999 2zm-1-9c-.437 1.437-1.563 2.562-2.998 3.001 1.438.44 2.561 1.564 3.001 3.002.437-1.438 1.563-2.563 2.996-3.002-1.433-.437-2.559-1.564-2.999-3.001zm-7.001 22c-6.617 0-12-5.383-12-12s5.383-12 12-12c1.894 0 3.63.497 5.37 1.179-2.948.504-9.37 3.266-9.37 10.821 0 7.454 5.917 10.208 9.37 10.821-1.5.846-3.476 1.179-5.37 1.179z">
    </path>
</svg>
      <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
   >
    <path
    class="fill-white"
        d="M4.069 13h-4.069v-2h4.069c-.041.328-.069.661-.069 1s.028.672.069 1zm3.034-7.312l-2.881-2.881-1.414 1.414 2.881 2.881c.411-.529.885-1.003 1.414-1.414zm11.209 1.414l2.881-2.881-1.414-1.414-2.881 2.881c.528.411 1.002.886 1.414 1.414zm-6.312-3.102c.339 0 .672.028 1 .069v-4.069h-2v4.069c.328-.041.661-.069 1-.069zm0 16c-.339 0-.672-.028-1-.069v4.069h2v-4.069c-.328.041-.661.069-1 .069zm7.931-9c.041.328.069.661.069 1s-.028.672-.069 1h4.069v-2h-4.069zm-3.033 7.312l2.88 2.88 1.415-1.414-2.88-2.88c-.412.528-.886 1.002-1.415 1.414zm-11.21-1.415l-2.88 2.88 1.414 1.414 2.88-2.88c-.528-.411-1.003-.885-1.414-1.414zm2.312-4.897c0 2.206 1.794 4 4 4s4-1.794 4-4-1.794-4-4-4-4 1.794-4 4zm10 0c0 3.314-2.686 6-6 6s-6-2.686-6-6 2.686-6 6-6 6 2.686 6 6z">
    </path>
</svg>
    </button>
  </nav>
</header>

	<main class="content container max-w-prose mx-auto flex-1 py-5 px-4 md:px-0">


<div dir="" class="prose mx-auto break-normal px-5 mt-8 dark:prose-invert">
    

    <h1 class="text-2xl mb-0 md:text-3xl font-black ">Physics of Language Models: Part 3.1, Knowledge Storage and Extraction</h1>
    <time class="text-sm my-2 text-muted mb-5" datetime=" 2025-04-02T00:00:00Z">
        April 2, 2025
    </time>
    <div class="mt-3"></div>
    <div class="toc select-none p-3 bg-stone-100 text-black dark:bg-stone-900 dark:text-white rounded">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details cursor-pointer">Content</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3" aria-label="核心思想">核心思想</a><ul>
                <li>
                    <a href="#setting" aria-label="Setting">Setting</a><ul>
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="数据集">数据集</a></li>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5" aria-label="训练策略">训练策略</a></li></ul>
                </li>
                <li>
                    <a href="#mixed-training-enables-knowledge-extraction" aria-label="Mixed Training Enables Knowledge Extraction">Mixed Training Enables Knowledge Extraction</a></li>
                <li>
                    <a href="#model-fails-to-extract-knowledge-after-bio-pretrain" aria-label="Model Fails to Extract Knowledge After BIO Pretrain">Model Fails to Extract Knowledge After BIO Pretrain</a></li>
                <li>
                    <a href="#knowledge-augmentation" aria-label="Knowledge Augmentation">Knowledge Augmentation</a></li>
                <li>
                    <a href="#position-based-probing" aria-label="Position-Based Probing">Position-Based Probing</a></li>
                <li>
                    <a href="#query-based-probing" aria-label="Query-Based Probing">Query-Based Probing</a></li>
                <li>
                    <a href="#celebrity-can-help-minority" aria-label="Celebrity Can Help Minority">Celebrity Can Help Minority</a></li>
                <li>
                    <a href="#knowledge-storage-for-bidirectional-models" aria-label="Knowledge Storage for Bidirectional Models">Knowledge Storage for Bidirectional Models</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" aria-label="参考文献">参考文献</a>
                </li>
            </ul>
        </div>
    </details>
</div>

    <h2 id="核心思想">核心思想</h2>
<ul>
<li>
<p>LLM 已经惊艳了所有人, 尤其是它广博的知识面, 几乎可以说是博古通今 (当然了, 有幻觉问题). 所以, 一个很自然的问题是, LLM 存储和提取知识的机制是怎么样的呢? 虽然已经有一些工作在现有的 LLM 的基础上进行探索, 但是并没有严格控制变量, 导致其得出的结论并不那么严谨. 比如询问 &ldquo;高斯的出生日期?&rdquo;, LLM 得到的答案可能来自两种: 1. 记忆了 wikipedia 等知识库并从中抽取; 2. 训练语料里中恰好有这个问题, 从而能够很好地回答.</p>
</li>
<li>
<p>为了避免上述第二种情况引发的一个干扰, 作者人为构造一些数据集, 并从头训练以严格控制变量.</p>
</li>
</ul>
<h3 id="setting">Setting</h3>
<h4 id="数据集">数据集</h4>
<ul>
<li><strong>bioS:</strong> 从 $N=100,000$ 个体中随机生成 profiles: 每个个体的出生日期, 出生的城市, 毕业院校, 就职公司, 工作城市等独立随机生成. 每个个体的 full name 是独一无二的. 如下是一个例子,</li>
</ul>
<blockquote>
<p><u>Anya Briar Forger</u> was born on <u>October 2, 1996</u>. She spent her early years in <u>Princeton, NJ</u>. She received mentorship and guidance from faculty members at <u>Massachusetts Institute of Technology</u>. She completed her education with a focus on Communications. She had a professional role at <u>Meta Platforms</u>. She was employed in <u>Menlo Park, CA</u>.</p></blockquote>
<ul>
<li>
<p>对于 <strong>bioS</strong>, 在后续的实验中可能会涉及 3 种不同的数据增强方法:</p>
<ol>
<li><strong>multiM:</strong> 即用 $M$ 种模板为每个个体生成多样的人物传记;</li>
<li><strong>fullname:</strong> 将 he/she/they 等代词替换为个体的 fullname;</li>
<li><strong>permute:</strong> 上述传记有 6 个句子, 这个数据增强就是将 6 个句子进行一个随机打乱.</li>
</ol>
</li>
<li>
<p><strong>bioR:</strong> 这个数据集借助 Llama 生成更为接近现实的任务传记 (风格上更为符合), 如下是一个例子:</p>
</li>
</ul>
<blockquote>
<p><u>Anya Briar Forger</u> is a renowned social media strategist and community manager. She is currently working as a Marketing Manager at <u>Meta Platforms</u>. She completed her graduation from <u>MIT</u> with a degree in Communications. She was born on <u>2nd October 1996</u> in <u>Princeton, NJ</u> and was brought up in the same city. She later moved to <u>Menlo Park in California</u> to be a part of Facebook’s team. She is an avid reader and loves traveling.</p></blockquote>
<ul>
<li><strong>QA dataset:</strong> 为了进一步评估模型的抽取 (而不仅仅是记忆) 知识的能力, 作者设计了 QA dataset: 对于每条人物传记, 可生成如下的六条 QA:</li>
</ul>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402102747.png" alt="20250402102747"></p>
<h4 id="训练策略">训练策略</h4>
<ul>
<li>
<p>主要采用 GPT2/Llama 进行训练, 最后也会讨论一下 BERT. 记 <strong>baseline</strong> 为按照数据集各个属性的多数进行猜测的方式.</p>
</li>
<li>
<p><strong>Pretrain + Instruction finetune:</strong> 在 bioS/bioR 上从头预训练 LM, 然后用 QA data 的一半进行指令微调, 然后再用 QA 的剩下一半进行测试.</p>
</li>
<li>
<p><strong>Mixed Training:</strong> 在 bioS/bioR + 一半的 QA data 上从头预训练 LM, 按照一定比例采样 BIO data 和 QA data (比如 2:8). 用剩下的 QA data 进行测试.</p>
</li>
</ul>
<h3 id="mixed-training-enables-knowledge-extraction">Mixed Training Enables Knowledge Extraction</h3>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402104406.png" alt="20250402104406"></p>
<p><strong>注</strong>: <strong>first-token accuracy</strong> 指的是对应 answer 的第一个 token 的预测正确率, <strong>generation accuracy</strong> 指的是完全回答出整个属性的正确率.</p>
<ul>
<li>
<p>如上图所示, 采用 mixed training 可以很容易取得很高的正确率, 不论是 BIO 数据的 in/out-distribution accuracy, 还是 QA in/out-distribution accuracy.</p>
</li>
<li>
<p>特别地, 可以注意到, 通过 QA data 的学习能够很快地帮助 BIO data 的记忆, 渐渐地这种优势能够逐步泛化到 out-distribution data.</p>
</li>
<li>
<p><strong>结论:</strong> <strong>Mixed Training</strong> 能够有效提高模型的知识提取能力.</p>
</li>
</ul>
<h3 id="model-fails-to-extract-knowledge-after-bio-pretrain">Model Fails to Extract Knowledge After BIO Pretrain</h3>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402105156.png" alt="20250402105156"></p>
<ul>
<li>
<p>如上图所示, <strong>Pretrain + Instruction finetune</strong> 难以获得有效的 out-distribution QA generation accuracy, 无论是 LoRA finetune 还是 full finetune.</p>
</li>
<li>
<p>实际上, full finetune 是能够保证训练数据能够被记忆的, 但由于 pretraining 过程中没有接触过 QA 的数据类型, 后面 finetune 无法改变模型的推理逻辑. 这有种学生一个劲地死记硬背, 但是没有做过任何真题, 考试的时候就直接蒙圈了.</p>
</li>
<li>
<p><strong>结论:</strong> 仅在知识库中训练可以记忆知识但是缺乏提取能力, 而且这种能力没法<strong>后天补足</strong>.</p>
</li>
</ul>
<h3 id="knowledge-augmentation">Knowledge Augmentation</h3>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402105947.png" alt="20250402105947"></p>
<ul>
<li>
<p>有一个问题是, QA 这种数据类型对于知识提取是否是必须的? 能不能通过其它方式学习到呢? 上图就是探索, 在 bioS 上应用三种数据增强后的效果:</p>
<ol>
<li><strong>multiM:</strong> 即用 $M$ 种模板为每个个体生成多样的人物传记;</li>
<li><strong>fullname:</strong> 将 he/she/they 等代词替换为个体的 fullname;</li>
<li><strong>permute:</strong> 上述传记有 6 个句子, 这个数据增强就是将 6 个句子进行一个随机打乱.</li>
</ol>
<p>可以发现, multi5 + permute 就能够取得和 mixed training 相当的效果了!</p>
</li>
<li>
<p><strong>结论:</strong> 数据增强尤其是多样的模板 + 属性位置打乱能够完全替代 QA 在知识提取能力增强方面的作用.</p>
</li>
</ul>
<h3 id="position-based-probing">Position-Based Probing</h3>
<ul>
<li>其实看完上面的实验, 大概率会有这样一种感觉, 就是纯的 bioS/bioR 的预训练可能会导致模型将知识绑定到一些奇奇怪怪的东西上面. 而加入了数据增强之后, 大概率这些数据就能够被绑定到 full name 之上了. 所以作者做了一些实验进行验证.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402114406.png" alt="20250402114406"></p>
<ul>
<li>
<p>如上图所示, 作者定义了 6 个 <em>special token positions</em>, 每个 token position 都可以定义 6 个任务, 分别是 <em>c_name/univ/major/b_data/b_city/c_city</em>, 所以实际上总共有 $6 \times 6$ 个任务.</p>
</li>
<li>
<p>对于每个任务, 固定预训练的网络, 然后对于 embedding layer 加一个 rank-2 的 LoRA 然后在网络最后加一个额外的可训练的分类线性层.</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402123345.png" alt="20250402123345"></p>
<ul>
<li>
<p>如上图所示, 在纯的 bioS 上训练, 基本上 special token 对于预测所对应的属性比较准 (比如根据上面的例子, &lsquo;at&rsquo; 容易预测对公司, &lsquo;on&rsquo; 容易预测对生日). 这实际上说明了, 在纯的 bioS 上训练, 模型会以一种奇怪的方式记忆知识. 而不是将这些信息和 full name 绑定在一起.</p>
</li>
<li>
<p>而加入了数据增强之后, 基本上每个 special token 都能预测对后面的属性.</p>
</li>
<li>
<p><strong>结论:</strong> 越多的数据增强能够大大提高 $P$-probing 的正确率, 衍生出正确关联关系.</p>
</li>
</ul>
<h3 id="query-based-probing">Query-Based Probing</h3>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402124222.png" alt="20250402124222"></p>
<ul>
<li>
<p>实际上, 我们可以对 full name 也进行类似上面的过程, 可以得到一样的结论, 即数据增强能够促使模型将知识绑定到 full name 上而不是一些奇怪的联系. 如上图所示, 当我们的输入仅包括 full name 的时候, 只有经过数据增强的预训练能够取得较高的分类精度 (即此时 full name 绑定了很多对应的属性).</p>
</li>
<li>
<p><strong>结论:</strong> 越多的数据增强能够大大提高 $Q$-probing 的正确率, 将属性正确地绑定到 full name 之上.</p>
</li>
</ul>
<h3 id="celebrity-can-help-minority">Celebrity Can Help Minority</h3>
<ul>
<li>现在有一个问题, 因为实际的数据, 我们很难对每条数据都进行合理的数据增强. 那么仅对部分数据数据增强是否能够帮助对未进行过数据增强的数据的知识抽取呢?</li>
</ul>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402124804.png" alt="20250402124804"></p>
<ul>
<li><strong>结论:</strong> 应用部分数据上的数据增强也能够帮助其它数据的知识抽取.</li>
</ul>
<h3 id="knowledge-storage-for-bidirectional-models">Knowledge Storage for Bidirectional Models</h3>
<p><img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250402134531.png" alt="20250402134531"></p>
<ul>
<li><strong>结论:</strong> BERT 在知识抽取方面能力堪忧, 大概率的原因是 BERT 是双向的, 导致其错误的关联更甚, 比如 城市 &lsquo;Bellevue&rsquo; 和州 &lsquo;WA&rsquo; 可能产生相互关联, 而不是把这个信息引导向 full name. 而 birth, major 这类较为独立的词, 反而能够较好地和 full name 关联上.</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<ol class="reference">
  <li>
    Allen-Zhu Z., and Li Y.
    <u>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction.</u>
    <i>ICML</i>, 2024.
    <a href="http://arxiv.org/abs/2407.05441" style="color: #007acc; font-weight: bold; text-decoration: none;">[PDF]</a>
    <a href="https://physics.allen-zhu.com/part-3-knowledge/part-3-1" style="color: #007acc; font-weight: bold; text-decoration: none;">[Code]</a>
  </li>
  <!-- 添加更多文献条目 -->
</ol>

</div>
<div class="mt-5 mb-3 px-5">
    
<ul class="flex items-center gap-2">
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/note/">Note</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/llm/">LLM</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/knowledge/">Knowledge</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/seminal/">Seminal</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/empirical/">Empirical</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/icml/">ICML</a>
  </li>
  <li><a class="px-2 py-1 rounded shadow bg-stone-800 text-white dark:bg-white dark:text-slate-800  "
      href="https://www.mtandhj.com/tags/2024/">2024</a>
  </li>
</ul>
</div>

<div class="post-comment">
    
    <link
    rel="stylesheet"
    href="https://unpkg.com/@waline/client@v3/dist/waline.css"
/>
<div id="waline"></div>
<script type="module">
import { init } from 'https://unpkg.com/@waline/client@v3/dist/waline.js';

window.onload = () => {
    init({
        el: '#waline',
        reaction: false,
        serverURL: 'https://comment.mtandhj.com',
    });
}
</script>
</div>

</main>

<script src="/js/main.js"></script>

<footer class="bg-stone-100 dark:bg-stone-900 dark:text-white border-t border-stone-400 w-full p-4 text-center font-bold">
  <div class="container mx-auto flex items-center justify-between">
    <p> MTandHJ &copy; 2025 </p>
  </div>
</footer>

</body>

</html>
