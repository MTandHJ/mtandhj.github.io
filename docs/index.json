[{"content":"\rEsser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 $$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"\rvan den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() ","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"}]