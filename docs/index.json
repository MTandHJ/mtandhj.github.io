[{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 $$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 $$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$\r$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 $$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$\r$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 $$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 -ddd\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 ddd\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 ddd\n通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 $$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$ 来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ $$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ $$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ $$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$ 这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ $$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$ 这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ $$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$ 这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布: $$\rq(z|x; \\phi).\r$$ 比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可; Decoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间: $$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练. $$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$; 给定一个输入 $x$, 其对应的离散值为 $$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$ 其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行. 接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了. $$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n容易发现, 这其实相当于我们的后验分布为:\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. $$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n容易发现, 这其实相当于我们的后验分布为:\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. 对于第二点, 作者建议采取 straight-through estimator, 另外设计了另外两个损失用于训练 $\\phi$ 以及 codebook $E$:\n$$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$ 这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n容易发现, 这其实相当于我们的后验分布为:\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. 对于第二点, 作者建议采取 straight-through estimator, 另外设计了另外两个损失用于训练 $\\phi$ 以及 codebook $E$:\n$$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r# 第一页\r这是第一页的内容看看看看看阿卡看看看看看看看看看看看看看看看看看看看看看看看\n### dier\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ### 333\r\u003cul\u003e \u003cli\u003e \u003cp\u003e为什么没有 bullet\n\u003cul\u003e \u003cli\u003eddd $$\rx + 1\r$$\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e ","permalink":"http://localhost:1313/slides/test/","title":"Slide-test"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"}]