[{"content":"\r","permalink":"http://localhost:1313/trends/spectral-gnn/","title":"Spectral Graph Neural Networks"},{"content":"预备知识 请先了解 NLP 中的 Scaling Law. 核心思想 作者团队希望验证一下在广告场景下是否也有类似于 NLP 中的 scaling law, 即探究是否随着广告预测模型地增大, 是否能够有规律地提升一些线上指标. (因为关注的是实际的线上指标, 这也衍生出了一些独特的问题, 这里就不讲了). 注意到, 实际的推荐系统通常是包含多个阶段, 每个阶段可能还包含不同指标导向的模型, 因此相当复杂. 为了探究 scaling law, 作者团队主要针对 Pre-ranking 阶段探究一个排序模型 (MLPs): 特征: 同时包括 sparse 和 dense features, 对于 dense features 应用 log1p transformation. 模型: 5-layer 的 MLPs, 每一层包括一个 batch normalization, linear mapping 和 PReLU. 通过 He initialization 初始化权重. 如上图所示, FLOPs 和作者设定的指标 R/R* 随着 MLPs 变大所产生的变化情况, 可以通过 Broken Neural Scaling Law (BNSL) 的公式很好的拟合. 参考文献 Wang Y., Yang Z., Zhang Z., Wang Z., Yang J.,\rWen S., Jiang P., and Gai K.\rScaling Laws for Online Advertisement Retrieval.\rarXiv, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/advertisement-scaling-law/","title":"Scaling Laws for Online Advertisement Retrieval"},{"content":"预备知识 了解基本 pretrain-finetune 范式.\n预训练模型权重: $W_{pre} \\in \\mathbb{R}^d$;\n微调后的模型权重: $W_i, i=1,2,\\ldots, N$.\n核心思想 现阶段, pretrain-finetune 最为主流的范式: 即为了推广到一个新的任务上, 我们通常是在一个预训练好的模型上进行微调 (可以是全量微调, 也可以是通过如 分类头, pre-tuning, LORA 等方式进行微调).\n现在的问题是, 比如我们可能有许许多多的子任务, 然后我们必须为每个子任务都存一份权重, 即使权重相较于预训练模型的权重会小很多, 这依旧是一个较为麻烦的事情 (其实个人感觉还好, 但是故事是这般讲的).\n假设我们有 $N$ 个任务, 分别微调得到了微调后的权重 $W_i, i=1,2,\\ldots, N$. 一种很自然的方式是通过平均的方式进行权重融合:\n$$\rW_M = \\mathcal{M}([W_1,\\ldots, W_N]).\r$$ 但是如上图所示, 这个结果其实非常糟糕, 当然了, 作者也比较其它的融合方法, 大抵上和单独微调的模型结果相去甚远, 离多任务训练的模型也有不小的差距.\nEMR-MERGING 另辟蹊径, 融合之后返回\n$$\rW_{uni}, [E_1, \\ldots, E_N] = \\mathcal{M}'([W_1, \\ldots, W_N]).\r$$这里 $W_{uni}$ 是所有任务共享的模型权重, $[E_1, \\ldots, E_N]$ 则是每个任务单独的一组参数, 它包含一个非常轻量化的 mask 矩阵以及一个 scale factor. 示意图如下:\nEMR-MERGING 的具体流程非常简单: 计算每个微调任务的权重 $W_i$ 与预训练权重 $W_{pre}$ 间的差值:\n$$\r\\tau_i = W_i - W_{pre} \\in \\mathbb{R}^d.\r$$ 然后计算:\n$$\r\\tau_{uni} = \\gamma_{uni} \\odot \\epsilon_{uni}, \\\\\r\\gamma_{uni} = \\text{sgn}(\\sum_{i=1}^N \\tau_i), \\\\\r[\\epsilon_{uni}]_k = \\max |[\\tau_i]_k| \\cdot \\mathbb{I}[\\text{sgn}([\\tau_i]_k) = \\text{sgn}([\\tau_{uni}]_k)].\r$$即 $\\tau_{uni}$ 是每个 entry 中和平均结果符号一致的最大值 (in absolute value).\n得到了共享的部分之后, 接下来为每个任务 $i$ 计算它所独有的 mask 以及 scale factor:\n$$\rM_i = (\\tau_i \\odot \\tau_{uni} \u003e 0), \\\\\r\\lambda_i = \\frac{\r\\text{sum}(\\text{abs}( \\tau_i))\r}{\r\\text{sum}(\\text{abs}(M_i \\odot \\tau_F))\r},\r$$即, 我们只保留和原有权重 $W_i$ 方向一致的, 然后通过一个 scale factor $\\lambda_i$ 来使得调整后的权重在 L1 范数上保持不变.\n注: 个人其实不太喜欢这个方法, 但是这个问题有一点意思.\n参考文献 Huang C., Ye P., Chen T., He T., Yue X., and Ouyang W.\rEMR-MERGING: Tuning-Free High-Performance Model Merging.\rNeurIPS, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/emr-merging/","title":"EMR-MERGING: Tuning-Free High-Performance Model Merging"},{"content":"预备知识 请了解 QARM. $\\mathcal{H}_u = \\{v_1^h, v_2^h, \\ldots, v_n^h\\}$, user historical behavior sequence, 在快手的场景下, $v$ 表示一个视频. $\\mathcal{S} = \\{v_1, v_2, \\ldots, v_m\\}$, 推荐的一串视频流. session watch time (swt), view probability (vtr), follow probability (wtr), like probability (ltr). 核心思想 我们知道, 在工业界推荐系统有着一套复杂的流程: 粗排-精排. 这一套流程被广泛应用有着不同的原因:\n庞大的商品数量: 由于精排通常是 pair-wise 的比较 (因为需要利用交叉特征), 所以必须通过一步步粗排来减少候选商品的数量以保证有限的开销; 多样化的推荐策略: 在工业场景中, 推荐的目标远非\u0026quot;精度\u0026quot;, 实际上还要考虑比如多样性等指标以保证用户的留存以及各种品类的商品具有最低限度的曝光度, 此外, 还需要考虑比如广告的因素. 然而, 由于生成式推荐的发展, 第一个问题其实已经可以迎刃而解了, 这促使我们探索端到端推荐的可能性.\nOneRec 的基本流程如下:\n对每个 video $v_i$, 通过 QARM 中的操作得到其所对应的多模态 embedding $\\bm{e} \\in \\mathbb{R}^d$;\n$\\mathbf{e}_i$ 通过类似 QARM 的 fixed RQ-VAE 进行残差量化, 得到其离散化表示\n$$\r(s_i^1, s_i^2, \\ldots, s_i^L).\r$$ OneRec 的主体结构为 Encoder-Decoder:\nEncoder 以 $\\mathcal{H}_u$ 为输入得到 user 的编码, 其作为 Decoder (full visible cross-attention) 的 Key/Value; Decoder 负责生成视频流推荐, 注意与一般的 next-token/item 推荐不同, OneRec 的目标是生成一个 session 的视频, 它的特殊之处主要体现在后面会讲到的训练过程. Balanced K-means Clustering 与 QARM 的 codebook 的确定方式的不同之处在于, 本文还强调每个类别的均分性. Next-Token Prediction OneRec 的训练目标和一般的有不同之处, 首先, 需要在用户的交互记录中选择 high-quality sessions (文中并未提及是要同时满足还是满足任一即可, 我感觉后者会比较合理):\n该 session 内用户所观看的视频数 $\\ge 5$; 该 session 内用户累积观看时长 $\\ge$ 某个阈值; 该用户对该 session 内的视频发生了如点赞, 收藏, 分享等行为. 于是, Decoder 的输入为:\n$$\r\\mathcal{\\bar{S}} = \\{\r\\bm{s}_{\\text{[BOS]}}, \\mathbf{s}_1^1, \\mathbf{s}_1^2, \\cdots, \\mathbf{s}_1^L,\r\\bm{s}_{\\text{[BOS]}}, \\mathbf{s}_2^1, \\mathbf{s}_2^2, \\cdots, \\mathbf{s}_2^L,\r\\cdots,\r\\bm{s}_{\\text{[BOS]}}, \\mathbf{s}_m^1, \\mathbf{s}_m^2, \\cdots, \\mathbf{s}_m^L\r\\},\r$$其中 $\\mathbf{s}_{\\text{[BOS]}}$ 为不同视频的分隔符.\n于是, OneRec 的主要损失为:\n$$\r\\mathcal{L}_{\\text{NTP}}\r= - \\sum_{i=1}^m \\sum_{j=1}^L\r\\log P(\\mathbf{s}_i^{j+1}| [\r\\bm{s}_{\\text{[BOS]}}, \\mathbf{s}_1^1, \\mathbf{s}_1^2, \\cdots, \\mathbf{s}_1^L,\r\\cdots,\r\\bm{s}_{\\text{[BOS]}}, \\mathbf{s}_i^1, \\cdots, \\mathbf{s}_i^j\r]; \\Theta).\r$$ Iterative Preference Alignment with RM 特别地, 本文还采用 Direct Preference Optimization (DPO) 来进行偏好对齐. 由于推荐场景数据反馈的稀疏性, OneRec 借助一个 reward model (RM) 首先拟合反馈, 然后再用于提升 OneRec.\n对于 RM, 我们首先得到 user 的 target-aware 的表示 $\\bm{h}_f \\in \\mathbb{R}^{m \\times d}$ 序列, 然后分别计算不同的指标:\n$$\r\\hat{r}^{swt} = \\text{Tower}^{swt}(\\text{Sum}(\\mathbf{h}_f)), \\\\\r\\hat{r}^{vtr} = \\text{Tower}^{vtr}(\\text{Sum}(\\mathbf{h}_f)), \\\\\r\\hat{r}^{wtr} = \\text{Tower}^{wtr}(\\text{Sum}(\\mathbf{h}_f)), \\\\\r\\hat{r}^{ltr} = \\text{Tower}^{ltr}(\\text{Sum}(\\mathbf{h}_f)),\r$$其中 $\\text{Tower}(\\cdot) = \\text{Sigmoid}(\\text{MLP}(\\cdot))$. 然后通过 BCE 进行训练:\n$$\r\\mathcal{L}_{\\text{RM}} = -\\sum_{xtr \\in \\{swt, vtr, wtr, ltr\\}}\r\\big(\ry^{xtr} \\log (\\hat{r}^{xtr})\r+ (1 - y^{xtr}) \\log (1 - \\hat{r}^{xtr})\r\\big).\r$$ 接下来通过 RM 来迭代地提升 OneRec:\n参考文献 Deng J., Wang S., Cai K.,\rRen L., Hu Q., Ding W.,\rLuo Q., and Zhou G.\rOneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment.\rarXiv, 2025.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/onerec/","title":"OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment"},{"content":"预备知识 请了解 RQ-VAE. 核心思想 我们知道, 多模态推荐主要涉及:\n$$\r\\text{text/image} \\longrightarrow \\text{Encoder} \\longrightarrow \\text{Embedding} \\longrightarrow \\text{Recommender}\r$$的过程, 且通常 $\\text{Encoder}$ 是通过 CV, NLP 任务预训练好后固定下来的.\n作者认为这种方式缺少了推荐任务的约束, 且通常用作 ID embedding 的补充而不是替代. 于是本文就希望:\n通过推荐任务微调 Encoder; 通过向量量化来替代 ID. Item Alignment of QARM QARM 首先通过 Item-Item 的匹配任务来约束 Encoder, 即 Item 和它在推荐上\u0026rsquo;相似\u0026rsquo;的 Item 靠近, \u0026lsquo;不相似\u0026rsquo;的远离. 这里的相似 Item 通过如下的两种方式得到:\n通过过往的 User2Item 检索模型: 为每个用户所 positive clicked target item 选择最相似的 Item; 通过过往的 Item2Item 检索模型: 为每个 item 选择高相似度的 Item. 得到高相似度的 Item pairs 之后, 通过对比学习进行训练:\n$$\r\\textbf{M}_{\\text{trigger}} = \\text{MLLM}(\r\\mathbf{T}_{\\text{trigger}}^{\\text{text}}, \\mathbf{T}_{\\text{trigger}}^{\\text{audio}}, \\mathbf{T}_{\\text{trigger}}^{\\text{image}}\r), \\\\\r\\textbf{M}_{\\text{target}} = \\text{MLLM}(\r\\mathbf{T}_{\\text{target}}^{\\text{text}}, \\mathbf{T}_{\\text{target}}^{\\text{audio}}, \\mathbf{T}_{\\text{target}}^{\\text{image}}\r), \\\\\r\\mathcal{L}_{\\text{align}} = \\text{Batch-Contrastive}(\r\\mathbf{M}_{\\text{trigger}},\r\\mathbf{M}_{\\text{target}},\r\\mathcal{B}\r).\r$$这里 $\\text{MLLM}$ 是需要微调的 Encoder.\nQuantitative Code of QARM QARM 不采取之前常用的 RQ-VAE.\n作者认为既然先前已经对 Encoder 进行微调了, 我们只需要从得到的 embeddings 中估计一些聚类中心, 即可作为 codebook. 当然了, 如果我们还希望使用 RQ-VAE 的格式, 就需要为每一层筛选 codebook $\\mathbf{R}^l \\in \\mathbb{R}^{K \\times d}, \\: l=1,2,\\ldots, L$:\n将所有 item 喂入 Encoder 得到 embedding table $\\mathbf{M}^{(0)} \\in \\mathbb{R}^{N \\times d}$.\n通过 Kmeans 得到 $K$ 个聚类中心, 作为 $\\mathbf{R}^1$.\n为 $\\mathbf{M}^{(0)}$ 中的每个 embedding 通过最近邻匹配上述的聚类中心, 得到 $\\mathbf{\\hat{M}}^{(0)}$.\n得到下一步的 embedding table $\\mathbf{M}^{(1)} = \\mathbf{M}^{(0)} - \\mathbf{\\hat{M}}^{(0)}$.\n重复上述过程, 得到\n$$\r\\mathbf{R}^1,\r\\mathbf{R}^2,\r\\ldots,\r\\mathbf{R}^L.\r$$ 接下来, 对于一个 embedding $\\mathbf{m} \\in \\mathbf{M}^{(0)}$, 我们可以通过如下方式得到它的离散编码 $(r_1, r_2, \\ldots, r_L)$:\n$$\rr_1 = \\text{NearestCode}(\\mathbf{R}^1, m, 1), \\quad \\mathbf{m}^1 = \\mathbf{m} - \\mathbf{R}_{r_1}^1, \\\\\rr_2 = \\text{NearestCode}(\\mathbf{R}^2, m^1, 1), \\quad \\mathbf{m}^2 = \\mathbf{m}^1 - \\mathbf{R}_{r_2}^2, \\\\\r\\cdots, r_{L} = \\text{NearestCode}(\\mathbf{R}^L, \\mathbf{m}^{L-1}, 1).\r$$ 此即可用于后续的训练.\n参考文献 Luo X., Cao J., Sun T., Yu J., Huang R.,\rYuan W., Lin H., Zheng Y., Wang S., Hu Q.,\rQiu C., Zhang J., Zhang X., Yan Z., Zhang J.,\rZhang S., Wen M., Liu Z., Gai K., and Zhou G.\rQARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou.\rarXiv, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/qarm/","title":"QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"},{"content":"核心思想 Linear 本文探索 LLM embeddings 的潜力, 方法极为简单:\n将 title 的 embeddings 作为对应 item 的表示 (记为 $\\bm{z}_i$), 其过程如下 (注意到, 实际上, 是将 decoder-only 的 next-token embedding 作为 item 的表示, 而不是平均之类的方式): # model_path = \u0026#34;meta-llama/Meta-Llama-3-8B\u0026#34; model_path = \u0026#34;meta-llama/Llama-2-7b-hf\u0026#34; # model_path = \u0026#34;meta-llama/Llama-2-13b-hf\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_path, device_map = \u0026#39;auto\u0026#39;) model = AutoModelForCausalLM.from_pretrained(model_path, device_map = \u0026#39;auto\u0026#39;) tokenizer.padding_side = \u0026#34;left\u0026#34; tokenizer.pad_token = tokenizer.eos_token item_df = pd.read_csv(\u0026#39;raw_data/items_filtered.csv\u0026#39;, index_col=0) item_df.rename(columns={\u0026#39;title\u0026#39;: \u0026#39;item_name\u0026#39;}, inplace=True) batch_size = 64 for i in tqdm(range(0, len(item_df), batch_size)): # print(i) item_names = item_df[\u0026#39;item_name\u0026#39;][i:i+batch_size] # 生成输出 inputs = tokenizer(item_names.tolist(), return_tensors=\u0026#34;pt\u0026#34;, padding=True, truncation=True, max_length=128).to(model.device) with torch.no_grad(): output = model(**inputs, output_hidden_states=True) seq_embeds = output.hidden_states[-1][:, -1, :].detach().cpu().numpy() # break if i == 0: item_llama_embeds = seq_embeds else: item_llama_embeds = np.concatenate((item_llama_embeds, seq_embeds), axis=0) 然后 user 表示为其所交互过的商品的平均:\n$$\r\\bm{z}_u = \\frac{1}{|\\mathcal{N}_u|} \\sum_{i \\in \\mathcal{N}_u} \\bm{z}_i.\r$$ 然后通过共享的 projector 得到:\n$$\r\\bm{e}_u = \\mathbf{W} \\bm{z}_u, \\quad \\bm{e}_i = \\mathbf{W} \\bm{z}_i.\r$$ score 以 cosine similarity 来计算:\n$$\rs_{ui} = \\frac{\\bm{e}_u^T \\bm{e}_i}{\\|\\bm{e}_u\\| \\| \\bm{e}_i\\|}.\r$$ 通过 InfoNCE 进行训练 (温度参数 $\\tau \\approx 0.15$), 注意, $\\bm{z}$ 是固定的.\n下面是 linear projector 下的结果:\n有一些很有趣的点: BERT, RoBERTa 等传统的 encoder 模型反而会取得很差的效果, 这个和之前的一些经验是相符的; 随着 LLM 的能力的增强, 效果也越来越好了, 很容易就能够超过 ID-based 的方法. AlphaRec AlphaRec 的做法:\nlinear projector 进行了一点点修改 $$\r\\bm{e} = \\mathbf{W}_2 \\text{LeakyReLU}\r\\big(\r\\mathbf{W}_1 \\bm{z}\r\\big).\r$$ 加上 LightGCN: $$\r\\mathbf{F} = \\sum_{l=0}^{L+1} \\mathbf{\\tilde{A}}^l \\mathbf{E},\r$$这里 $\\mathbf{E}$ 是所有的 $\\bm{e}_u, \\bm{e}_i$, $\\mathbf{\\tilde{A}}$ 是对应的 (对称) normalized 邻接矩阵.\n如上图所示, AlphaRec 如此简单的方法就能够取得非常惊人的效果 (而且效率很高). 其它潜力 可以作为初始化而加速和提高传统模型\n非常强的 zero-shot 能力, 甚至能够媲美传统模型 full-training 的效果\nintention-aware, 用户可以输入自己的意图, 同样通过 LLM 得到 embedding, 加权平均得到 user 的表示:\n$$\r\\bm{\\tilde{e}}_u = (1 - \\alpha) \\bm{e}_u + \\alpha \\bm{e}_u^{Intention}.\r$$ 个人测试 参考作者的代码, 自己实现了 AlphaRec, 在 Movies 进行了一下测试. Movies # AlphaRec root: ../../data dataset: AmazonMovies_Alpha tasktag: Matching embedding_dim: 64 num_layers: 2 epochs: 500 batch_size: 4096 optimizer: adam lr: 5.e-4 weight_decay: 1.e-6 tau: 0.15 num_negs: 256 projector: mlp monitors: [LOSS, Recall@1, Recall@10, Recall@20, HitRate@10, HitRate@20, NDCG@10, NDCG@20] which4best: Recall@20 # LightGCN root: ../../data dataset: AmazonMovies_Alpha tasktag: Matching embedding_dim: 64 num_layers: 2 epochs: 1000 batch_size: 2048 optimizer: adam lr: 1.e-3 weight_decay: 1.e-3 monitors: [LOSS, Recall@1, Recall@10, Recall@20, HitRate@10, HitRate@20, NDCG@10, NDCG@20] which4best: NDCG@20 Beauty # LightGCN root: ../../data dataset: Amazon2014Beauty_550811_ROU tasktag: Matching embedding_dim: 64 num_layers: 3 epochs: 1000 batch_size: 2048 optimizer: adam lr: 1.e-3 weight_decay: 1.e-3 monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 # LightGCN + InfoNCE root: ../../data dataset: Amazon2014Beauty_550811_ROU tasktag: Matching embedding_dim: 64 num_layers: 3 num_negs: 256 tau: 0.15 epochs: 500 batch_size: 2048 optimizer: adam lr: 5.e-4 weight_decay: 1.e-2 monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 root: ../../data dataset: Amazon2014Beauty_550811_ROU tasktag: Matching embedding_dim: 64 num_layers: 3 tfile: llama2_7b_title.pkl # llama2_13b_title.pkl epochs: 500 batch_size: 2048 optimizer: adam lr: 5.e-4 weight_decay: 0. tau: 0.15 num_negs: 256 projector: mlp monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 Method R@1 R@10 R@20 N@10 N@20 LightGCN 0.0079 0.0538 0.0836 0.0282 0.0361 LightGCN+InfoNCE 0.0098 0.0544 0.0829 0.0296 0.0371 AlphaRec (Llama2-7B) 0.0104 0.0618 0.0925 0.0330 0.0412 AlphaRec (Llama2-13B) 0.0107 0.0608 0.0921 0.0329 0.0412 AlphaRec (MiniLM-L12-v2) 0.0100 0.0608 0.0930 0.0322 0.0407 Baby # LightGCN root: ../../data dataset: Amazon2014Baby_550811_RAU tasktag: Matching embedding_dim: 64 # num_layers: 3 # num_negs: 256 # tau: 0.25 epochs: 100 batch_size: 2048 optimizer: adam lr: 1.e-3 weight_decay: 5.e-3 monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 # LightGCN + InfoNCE root: ../../data dataset: Amazon2014Baby_550811_RAU tasktag: Matching embedding_dim: 64 num_layers: 3 num_negs: 256 tau: 0.25 epochs: 500 batch_size: 2048 optimizer: adam lr: 1.e-3 weight_decay: 1.e-3 monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 # AlphaRec root: ../../data dataset: Amazon2014Baby_550811_RAU tasktag: Matching embedding_dim: 64 num_layers: 3 tfile: llama2_7b_title.pkl epochs: 500 batch_size: 2048 optimizer: adam lr: 5.e-4 weight_decay: 0. tau: 0.25 num_negs: 256 projector: mlp monitors: [LOSS, Recall@1, Recall@10, Recall@20, NDCG@10, NDCG@20] which4best: NDCG@20 Method R@1 R@10 R@20 N@10 N@20 LightGCN 0.0037 0.0212 0.0357 0.0113 0.0151 LightGCN+InfoNCE 0.0036 0.0206 0.0344 0.0111 0.0147 AlphaRec (Llama2-7B) 0.0039 0.0243 0.0399 0.0128 0.0169 AlphaRec (Llama2-13B) 0.0037 0.0242 0.0399 0.0126 0.0167 AlphaRec (MiniLM-L12-v2) 0.0031 0.0229 0.0385 0.0117 0.0158 从自己处理的数据集来看, LLM 并没有比传统的 BERT 好上太多.\n不过 Next-token embedding 的能力还是比较令人惊讶的, 这方面的原因可能可以通过 这篇文章 解释.\n参考文献 Sheng L., Zhang A., Zhang Y., Chen Y., Wang X., and Chua T.\rLanguage Representations Can be What Recommenders Need: Findings and Potentials\rICLR, 2025.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/alpharec/","title":"Language Representations Can be What Recommenders Need: Findings and Potentials"},{"content":"预备知识 $\\mathcal{U}$, user set, $|\\mathcal{U}| = M$; $\\mathcal{V}$, item set, $|\\mathcal{V}| = N$; $\\mathcal{S}^u = [v_1^u, v_2^u, \\ldots, v_{n_u}^u]$, 用户 $u$ 的交互序列. 核心思想 一般的序列推荐模型 (e.g., SASRec), 对应位置的输入 embedding 用于预测 next-item (如上图 (a) 所示).\n在 LLM 领域, CoT (chain-of-thought) 已经被证明在各领域上性能提高的优势了. 实际上, CoT 实际上是促使模型进行多次推理以获得更为准确可靠的结果. 那么类似的思想能不能推广到推荐呢? 这衍生了本文的 ReaRec.\n假设第 $i$ 个位置的输入是 $\\mathbf{h}_i^0$, 故整个序列的输入为:\n$$\r\\mathbf{H}^{0} = [\\mathbf{h}_1^0, \\mathbf{h}_2^0, \\ldots, \\mathbf{h}_n^0] \\in \\mathbb{R}^{n \\times d}.\r$$ 经过 $L$ 个 transformer blocks 之后, 我们可以得到输出特征:\n$$\r\\mathbf{H}^{L} = [\\mathbf{h}_1^L, \\mathbf{h}_2^L, \\ldots, \\mathbf{h}_n^L].\r$$ 接下来, 令 $\\mathbf{h}_{n+1}^0 = \\mathbf{h}_n^L + \\mathbf{p}_{n+1}^R$, 这里 $\\mathbf{p}_{n+1}^R$ 是特别的 reasoning position embedding.\n如此以往, 我们可以得到\n$$\r\\mathbf{H}^{L} = [\\mathbf{h}_1^L, \\mathbf{h}_2^L, \\ldots, \\mathbf{h}_n^L, \\underbrace{\\mathbf{h}_{n+1}^L, \\ldots, \\mathbf{h}_{n+k}^L}_{=: \\mathbf{R} \\in \\mathbb{R}^{k \\times d}}].\r$$为了符号简便, 重新记 $\\mathbf{r}_i = \\mathbf{h}_{n+i}$.\n接下来就是这么利用 $\\mathbf{R}$ 以及如何训练使得其有意义.\nEnsemble Reasoning Learning (ERL) ERL 将 $k$ 的平均作为 user 的表示:\n$$\r\\mathbf{h}_u = \\frac{1}{k} \\sum_{i=1}^k \\mathbf{r}_i^{L}.\r$$然后通过内积可以用于预测.\n训练稍有不同, 不是直接拿 $\\mathbf{h}_u$ 进行训练, 它要求每个阶段的表示 $\\mathbf{r}_i^L$ 都和 target 匹配:\n$$\r\\mathcal{L}_{\\text{Rec}} = -\\sum_{i=1}^k \\log \\hat{y}_{v_+}^{i}, \\\\\r\\hat{y}_{v_+}^i = [\\text{softmax}( \\mathbf{r}_k \\cdot \\mathbf{E}^T)]_{v^+},\r$$这里 $\\mathbf{E}$ 是所有 item 的初始 embeddings.\n同时要求不同的阶段尽可能表示不同含义, 以满足多样性 (就像 ensemble learning 里面通常也要求不同的模型具有不同的特点):\n$$\r\\mathcal{L}_{\\text{KL}} = -\\sum_{i=1}^k \\sum_{j=i+1}^k \\text{KL}(\r\\hat{y}^i \\| \\hat{y}^j\r).\r$$最小化 $\\mathcal{L}_{\\text{KL}}$ 等价于最大化两两的 KL 散度, 以获取多样性 (原文应该漏了一个 \u0026lsquo;-\u0026rsquo;).\nERL 的损失即为\n$$\r\\mathcal{L}_{\\text{ERL}} = \\mathcal{L}_{\\text{Rec}} + \\lambda\r\\mathcal{L}_{\\text{KL}}.\r$$ Progressive Reasoning Learning (PRL) PRL 用最后一个 $\\mathbf{h}_u = \\mathbf{r}_k^L$ 作为用户的表示.\nPRL 是希望在一步一步推理过程中, 模型对自己的预测变得越发自信, 所以首先 在原本的 $\\mathcal{L}_{\\text{Rec}}$ 中引入一个 Progressive Temperature Annealing:\n$$\r\\hat{y}^i = \\text{softmax}(\\mathbf{r}_i^L \\cdot \\mathbf{E}^T / \\tau_i), \\\\\r\\tau_i = \\tau \\cdot \\alpha^{k - i}.\r$$即越往后训练难度越大, 这就要求模型越往后变得越发自信.\n除此之外, 作者额外引入一个对比学习损失来增强模型的鲁棒性:\n$$\r\\mathcal{L}_{\\text{RCL}}\r= -\\sum_{i=1}^k\r\\log \\frac{\r\\exp(\\text{sim}(\\mathbf{\\tilde{r}}_i^L, \\mathbf{r}_i^+ / \\tau))\r}{\r\\exp(\\text{sim}(\\mathbf{\\tilde{r}}_i^L, \\mathbf{r}_i^+ / \\tau))\r+ \\sum_{\\mathbf{r}_i^- \\in \\mathbf{R}_i^-}\\exp(\\text{sim}(\\mathbf{\\tilde{r}}_i^L, \\mathbf{r}_i^- / \\tau))\r},\r$$其中 $\\mathbf{\\tilde{r}}$ 以 $\\mathbf{\\tilde{r}}_i^0 = \\mathbf{r}_i + \\bm{\\epsilon}$ 为输入得到的特征 ($\\bm{\\epsilon}$ 采样自一个正态分布 $\\mathcal{N}(\\bm{0}, \\gamma \\bm{I})$). $\\mathbf{r}_i^+, \\mathbf{r}_i^-$ 则分别表示正负样本.\n于是训练损失为\n$$\r\\mathcal{L}_{\\mathbf{PRL}} = \\mathcal{L}_{\\text{Rec}} + \\mathcal{L}_{\\text{RCL}}.\r$$ 参考文献 Tang J., Dai S., Shi T., Xu J., Chen X., Chen W., Jian W., and Jiang Y.\rThink Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation.\rarXiv, 2025.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/rearec/","title":"Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation"},{"content":"核心思想 LLM 已经惊艳了所有人, 尤其是它广博的知识面, 几乎可以说是博古通今 (当然了, 有幻觉问题). 所以, 一个很自然的问题是, LLM 存储和提取知识的机制是怎么样的呢? 虽然已经有一些工作在现有的 LLM 的基础上进行探索, 但是并没有严格控制变量, 导致其得出的结论并不那么严谨. 比如询问 \u0026ldquo;高斯的出生日期?\u0026rdquo;, LLM 得到的答案可能来自两种: 1. 记忆了 wikipedia 等知识库并从中抽取; 2. 训练语料里中恰好有这个问题, 从而能够很好地回答.\n为了避免上述第二种情况引发的一个干扰, 作者人为构造一些数据集, 并从头训练以严格控制变量.\nSetting 数据集 bioS: 从 $N=100,000$ 个体中随机生成 profiles: 每个个体的出生日期, 出生的城市, 毕业院校, 就职公司, 工作城市等独立随机生成. 每个个体的 full name 是独一无二的. 如下是一个例子, Anya Briar Forger was born on October 2, 1996. She spent her early years in Princeton, NJ. She received mentorship and guidance from faculty members at Massachusetts Institute of Technology. She completed her education with a focus on Communications. She had a professional role at Meta Platforms. She was employed in Menlo Park, CA.\n对于 bioS, 在后续的实验中可能会涉及 3 种不同的数据增强方法:\nmultiM: 即用 $M$ 种模板为每个个体生成多样的人物传记; fullname: 将 he/she/they 等代词替换为个体的 fullname; permute: 上述传记有 6 个句子, 这个数据增强就是将 6 个句子进行一个随机打乱. bioR: 这个数据集借助 Llama 生成更为接近现实的任务传记 (风格上更为符合), 如下是一个例子:\nAnya Briar Forger is a renowned social media strategist and community manager. She is currently working as a Marketing Manager at Meta Platforms. She completed her graduation from MIT with a degree in Communications. She was born on 2nd October 1996 in Princeton, NJ and was brought up in the same city. She later moved to Menlo Park in California to be a part of Facebook’s team. She is an avid reader and loves traveling.\nQA dataset: 为了进一步评估模型的抽取 (而不仅仅是记忆) 知识的能力, 作者设计了 QA dataset: 对于每条人物传记, 可生成如下的六条 QA: 训练策略 主要采用 GPT2/Llama 进行训练, 最后也会讨论一下 BERT. 记 baseline 为按照数据集各个属性的多数进行猜测的方式.\nPretrain + Instruction finetune: 在 bioS/bioR 上从头预训练 LM, 然后用 QA data 的一半进行指令微调, 然后再用 QA 的剩下一半进行测试.\nMixed Training: 在 bioS/bioR + 一半的 QA data 上从头预训练 LM, 按照一定比例采样 BIO data 和 QA data (比如 2:8). 用剩下的 QA data 进行测试.\nMixed Training Enables Knowledge Extraction 注: first-token accuracy 指的是对应 answer 的第一个 token 的预测正确率, generation accuracy 指的是完全回答出整个属性的正确率.\n如上图所示, 采用 mixed training 可以很容易取得很高的正确率, 不论是 BIO 数据的 in/out-distribution accuracy, 还是 QA in/out-distribution accuracy.\n特别地, 可以注意到, 通过 QA data 的学习能够很快地帮助 BIO data 的记忆, 渐渐地这种优势能够逐步泛化到 out-distribution data.\n结论: Mixed Training 能够有效提高模型的知识提取能力.\nModel Fails to Extract Knowledge After BIO Pretrain 如上图所示, Pretrain + Instruction finetune 难以获得有效的 out-distribution QA generation accuracy, 无论是 LoRA finetune 还是 full finetune.\n实际上, full finetune 是能够保证训练数据能够被记忆的, 但由于 pretraining 过程中没有接触过 QA 的数据类型, 后面 finetune 无法改变模型的推理逻辑. 这有种学生一个劲地死记硬背, 但是没有做过任何真题, 考试的时候就直接蒙圈了.\n结论: 仅在知识库中训练可以记忆知识但是缺乏提取能力, 而且这种能力没法后天补足.\nKnowledge Augmentation 有一个问题是, QA 这种数据类型对于知识提取是否是必须的? 能不能通过其它方式学习到呢? 上图就是探索, 在 bioS 上应用三种数据增强后的效果:\nmultiM: 即用 $M$ 种模板为每个个体生成多样的人物传记; fullname: 将 he/she/they 等代词替换为个体的 fullname; permute: 上述传记有 6 个句子, 这个数据增强就是将 6 个句子进行一个随机打乱. 可以发现, multi5 + permute 就能够取得和 mixed training 相当的效果了!\n结论: 数据增强尤其是多样的模板 + 属性位置打乱能够完全替代 QA 在知识提取能力增强方面的作用.\nPosition-Based Probing 其实看完上面的实验, 大概率会有这样一种感觉, 就是纯的 bioS/bioR 的预训练可能会导致模型将知识绑定到一些奇奇怪怪的东西上面. 而加入了数据增强之后, 大概率这些数据就能够被绑定到 full name 之上了. 所以作者做了一些实验进行验证. 如上图所示, 作者定义了 6 个 special token positions, 每个 token position 都可以定义 6 个任务, 分别是 c_name/univ/major/b_data/b_city/c_city, 所以实际上总共有 $6 \\times 6$ 个任务.\n对于每个任务, 固定预训练的网络, 然后对于 embedding layer 加一个 rank-2 的 LoRA 然后在网络最后加一个额外的可训练的分类线性层.\n如上图所示, 在纯的 bioS 上训练, 基本上 special token 对于预测所对应的属性比较准 (比如根据上面的例子, \u0026lsquo;at\u0026rsquo; 容易预测对公司, \u0026lsquo;on\u0026rsquo; 容易预测对生日). 这实际上说明了, 在纯的 bioS 上训练, 模型会以一种奇怪的方式记忆知识. 而不是将这些信息和 full name 绑定在一起.\n而加入了数据增强之后, 基本上每个 special token 都能预测对后面的属性.\n结论: 越多的数据增强能够大大提高 $P$-probing 的正确率, 衍生出正确关联关系.\nQuery-Based Probing 实际上, 我们可以对 full name 也进行类似上面的过程, 可以得到一样的结论, 即数据增强能够促使模型将知识绑定到 full name 上而不是一些奇怪的联系. 如上图所示, 当我们的输入仅包括 full name 的时候, 只有经过数据增强的预训练能够取得较高的分类精度 (即此时 full name 绑定了很多对应的属性).\n结论: 越多的数据增强能够大大提高 $Q$-probing 的正确率, 将属性正确地绑定到 full name 之上.\nCelebrity Can Help Minority 现在有一个问题, 因为实际的数据, 我们很难对每条数据都进行合理的数据增强. 那么仅对部分数据数据增强是否能够帮助对未进行过数据增强的数据的知识抽取呢? 结论: 应用部分数据上的数据增强也能够帮助其它数据的知识抽取. Knowledge Storage for Bidirectional Models 结论: BERT 在知识抽取方面能力堪忧, 大概率的原因是 BERT 是双向的, 导致其错误的关联更甚, 比如 城市 \u0026lsquo;Bellevue\u0026rsquo; 和州 \u0026lsquo;WA\u0026rsquo; 可能产生相互关联, 而不是把这个信息引导向 full name. 而 birth, major 这类较为独立的词, 反而能够较好地和 full name 关联上. 参考文献 Allen-Zhu Z., and Li Y.\rPhysics of Language Models: Part 3.1, Knowledge Storage and Extraction.\rICML, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/knowledge-storage-and-extraction/","title":"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction"},{"content":"预备知识 请先了解 DKT.\n$e \\in [E] = \\{0, 1, 2, \\ldots, E - 1\\}$, 习题的序号;\n$r \\in \\{0, 1\\}$, 习题 $e$ 某个学生做对与否.\n核心思想 本文几乎就是 GPT (decoder-only) 的 transformer 在 DKT 上的应用: SAKT 希望根据学生的交互序列 $\\{(e_1, r_1), \\ldots, (e_t, r_t)\\}$ 预测下一题 $e_{t+1}$ 的做对做错的情况.\n由上述描述可知, 与一般的 Transformer 稍有不同, SAKT 在推理过程中需要同时 \u0026ldquo;见到\u0026rdquo; 交互序列 以及需要被预测的新的题目.\n对于学生的交互序列 $\\{(e_1, r_1), \\ldots, (e_t, r_t)\\}$, 首先得到输入的 token:\n$$\ry_i = e_i + r_i \\cdot E =\r\\left \\{\r\\begin{array}{ll}\re_i, \u0026 \\text{if } r_i = 0, \\\\\re_i + E, \u0026 \\text{otherwise}.\r\\end{array}\r\\right .\r$$即, 对于每个题, 我们需要训练两个独立的 token 以分别对应做对做错. 然后通过 embedding table $\\mathbf{M} \\in \\mathbb{R}^{2E \\times d}$ 得到交互序列的向量表示:\n$$\r\\mathbf{\\hat{M}} \\in \\mathbb{R}^{n \\times d}.\r$$这里 $n$ 是序列长度, 短补零长截断.\n对于学生的做题序列 $\\{e_2, \\ldots, e_{t+1}\\}$, 通过 embedding table $\\mathbf{E} \\in \\mathbb{R}^{E \\times d}$ 得到\n$$\r\\mathbf{\\hat{E}} \\in \\mathbb{R}^{n \\times d}.\r$$ 两个序列同时喂入 Transformer, 前者用于生成 Key, Value, 后者用于生成 Query:\n$$\r\\mathbf{Q} = \\mathbf{\\hat{E}} \\mathbf{W}^Q,\r\\mathbf{K} = \\mathbf{\\hat{M}} \\mathbf{W}^K,\r\\mathbf{V} = \\mathbf{\\hat{M}} \\mathbf{W}^V,\r$$其余的和传统的 causal transformer 类似. 注意到, 因为 causal attention 的缘故, 我们能够保证, 在预测 $e_{t+1}$ 的对错时仅依赖 $e_{\u003c t+1}, r_{\u003c t + 1}$ 的信息.\n通过 transformer 得到的第 $i$-th 位置的特征 $\\bm{f}_i$ 用于预测 $i+1$ 位置:\n$$\rp_i = \\text{Sigmoid}(\\mathbf{F}_i \\mathbf{w} + \\mathbf{b}).\r$$并通过 BCE 进行训练.\n参考文献 Pandey S., and Karypis G.\rA Self-Attentive Model for Knowledge Tracing.\rEDM, 2019.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/sakt/","title":"A Self-Attentive Model for Knowledge Tracing"},{"content":"报销流程 会议 目前仅有注册费的报销经验\n为了报销, 必须走 出国/回国流程:\n出国流程: \u0026ldquo;研究生出国(境)申请\u0026rdquo; -\u0026gt; 发起新流程, 涉及到:\n外语水平证书; 会议邀请函; 知情同意书扫描件.pdf; (必须这个名字); 会议资助申请表; (申请学院经费需要学院审核盖章) 国际会议资助申请人基本信息表; 论文中英文摘要. 回国流程: \u0026ldquo;研究生回国(境)申请\u0026rdquo; -\u0026gt; 发起新流程, 涉及到:\n会议总结报告; 会议论文; 会议照片; 注册费票据; poster/ppt; 会议日程安排; 机票订单、行程单/发票、登机牌; (如果实际上没出国不需要) 住宿费收据; (如果实际上没出国不需要) 预约报销单: 这个正常来说是导师完成的, 倘若导师下放这个权利, 需要经过: 公共数据库 -\u0026gt; 预约报销 -\u0026gt; 网上预约报账 -\u0026gt; 申请报销单, 报销单有几个点需要特别注意 (如果有发票, 需要通过 \u0026ldquo;增值税发票查验\u0026rdquo; 录入发票):\n业务大类: 因公出境; 单项目报销: 需要填导师的项目, 同时导师给你相应的授权 项目负责人: 填了上面的项目后会自动生成; 打印:\n报销单; 出国/回国申请流程; 会议邀请信; 会议日程; 相关发票和支付凭证、订单. 打印完, 请将材料分门别类, 除报销单外写上对应的类别.\n报销单: 审批人, 项目负责人, 经办人签字, 然后盖\u0026quot;报销章\u0026quot;.\n去找财务老师报销 (2025-03-31 时间节点是 7 楼的丁老师).\n","permalink":"http://localhost:1313/posts/ecnu/","title":"ECNU 生存指北"},{"content":"预备知识 请先了解 MANN 以及 DKT.\n$q \\in \\mathcal{Q}$, 题目集合, $|\\mathcal{Q}| = Q$;\n$r \\in \\{0, 1\\}$, 表示题目做对与否;\n核心思想 之前的 DKT, 比如利用 RNN 的方式, 还是一种 序列特征提取 -\u0026gt; 二分类 这样的传统的方式, 这种方式没法给我们一些更多的信息: 学生在不同 concepts 上的掌握程度.\nDKVMN (Dynamic Key-Value Memory Networks) 主要是通过改进一个记忆网络 MANN 来实现对习题的概念和学生的掌握情况的一个动态建模.\n我们构建 $\\mathbf{M}^k \\in \\mathbb{R}^{N \\times d_k}$ 来建模 $N$ latent concepts $\\{c^1, c^2, \\ldots, c^N\\}$, 以及 $\\mathbf{M}^v \\in \\mathbb{R}^{N \\times d_v}$ 来建模某个学生的对于不同 concept 的掌握情况 $\\{\\mathbf{s}_t^1, \\mathbf{s}_t^2, \\ldots, \\mathbf{s}_t^2 \\}$. 并用 $\\mathbf{M}_t^v$ 表示在了解 $q_t$ 做题情况后的状态.\nRead: 通过 $q_t$ 检索出其对应的 embedding $\\mathbf{k}_t$:\n计算 read 的权重分布:\n$$\rw_t(i) = \\text{Softmax}(\\mathbf{k}_t^T \\mathbf{M}^k (i)).\r$$ 通过 value matrix 进行初步加权 read:\n$$\r\\mathbf{r}_t = \\sum_{i=1}^N w_t(i) \\mathbf{M}_{t - 1}^v (i).\r$$ 由于每个题目都有其特别之处, 所以最终的用于预测题目错对的向量为:\n$$\r\\mathbf{f}_t = \\text{Tanh}\\big(\r\\mathbf{W}_1^T [\\mathbf{r}_t, \\mathbf{k}_t] + \\mathbf{b}_1\r\\big).\r$$ 通过如下方式进行预测:\n$$\rp_t = \\text{Sigmoid}(\r\\mathbf{W}_2^T \\mathbf{f}_t + \\mathbf{b}_2\r).\r$$ Write: 更新 $\\mathbf{M}_t^v$ 分为: 遗忘和加强:\n遗忘 (erasing): $$\r\\mathbf{\\tilde{M}}_{t}^v (i) = \\mathbf{M}_{t-1}^v (i)\r\\odot [\\mathbf{1} - w_t (i) \\mathbf{e}_t], \\\\\r\\mathbf{e}_t = \\text{Sigmoid} (\\mathbf{W}_3^T \\mathbf{v}_t + \\mathbf{b}_e).\r$$ 增强: $$\r\\mathbf{M}_t^v (i) = \\mathbf{\\tilde{M}}_{t-1}^v (i) + w_t (i) \\mathbf{a}_t, \\\\\r\\mathbf{a}_t = \\text{Tanh}(\\mathbf{D}^T \\mathbf{v}_t + \\mathbf{b}_a).\r$$ 通过 BCE 进行训练.\n参考文献 Zhang J., Shi X., King I., and Yeung D.\rDynamic Key-Value Memory Networks for Knowledge Tracing.\rWWW, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/dkvmn/","title":"Dynamic Key-Value Memory Networks for Knowledge Tracing"},{"content":"预备知识 $\\mathbf{x}_t \\in \\mathbb{R}^d$, 输入; $\\mathbf{k}_t \\in \\mathbb{R}^d$, 根据输入得到的用于更新的向量; $\\mathbf{M}_t \\in \\mathbb{R}^{N \\times d}$, memory matrix; 核心思想 我们希望维护一个 memory matrix $\\mathbf{M}_t$ 以保存最新最有用的信息 (least recently used access (LRUA)).\nRead: 负责从 $\\mathbf{M}_t$ 中读取信息, 给定 key $\\mathbf{k}_t$, 通过 cosine similarity 来计算两两相似度:\n$$\rK(\\mathbf{k}_t, \\mathbf{M}_t(i)) =\r\\frac{\r\\mathbf{k}_t \\cdot \\mathbf{M}_t (i)\r}{\r\\|\\mathbf{k}_t\\| \\| \\mathbf{M}_t (i) \\|\r}, \\quad \\forall i.\r$$这里 $\\mathbf{M}_t(i)$ 表示矩阵的第 $i$ 行. 接着, 通过重加权计算所读取的向量:\n$$\r\\mathbf{r}_t \\leftarrow \\sum_i w_t^r (i) \\mathbf{M}_t (i), \\\\\rw_t^r (i) \\leftarrow\r\\frac{\r\\exp (K (\\mathbf{k}_t, \\mathbf{M}_t (i)))\r}{\r\\sum_{j} \\exp (K (\\mathbf{k}_t, \\mathbf{M}_t (j)))\r}.\r$$记 $w (i)$ 所构成的向量为 $\\mathbf{w}$.\nWrite: 负责更新 $\\mathbf{M}_t$, 既然我们希望维护最新最有用的信息, 那么: 1. 对于那些经常不被 Read 的行, 应当更多地更新替换; 2. 对于刚刚被 Read 的, 应当予以更新以保证它的最新性.\n于是作者设计了这么一个机制:\n$$\r\\mathbf{M}_t (i) \\leftarrow \\mathbf{M}_{t-1}(i) + w_t^w (i) \\mathbf{k}_t, \\\\\r\\mathbf{w}_t^w \\leftarrow \\sigma (\\alpha) \\mathbf{w}_{t-1}^r + (1 - \\sigma (\\alpha)) \\mathbf{w}_{t-1}^{lu}, \\\\\rw_t^{lu} (i) = \\left \\{\r\\begin{array}{ll}\r0 \u0026 \\text{if } w_t^u (i) \u003e m(\\mathbf{w}_t^u, n), \\\\\r1 \u0026 \\text{if } w_t^u (i) \\le m(\\mathbf{w}_t^u, n),\r\\end{array}\r\\right . \\\\\r\\mathbf{w}_t^u \\leftarrow \\gamma \\mathbf{w}_{t-1}^u + \\mathbf{w}_t^r + \\mathbf{w}_t^w.\r$$这里, $\\alpha$ 是一个可学西的 sigmoid gate parameter, $m(\\mathbf{v}, n)$ 返回的是 $\\mathbf{v}$ 中 $n$-th 最小的元素. 即 $w_t^{lu}$ 指示了当前 memory matrix 中哪些是 least-used (lu) 的. 最后写入的权重由 $\\mathbf{w}_t^w$ 决定, 它是当前 Read 的权重以及 least-used 权重的加权. 即 $\\mathbf{M}_t (i)$ 的更新程度比较大, 当且仅当它在这一轮中被充分读取了, 或它在过去许多轮都没有被充分读取.\n参考文献 Santoro A., Bartunov S., Botvinick M., Wierstra D., and Lillicrap T.\rMeta-Learning with Memory-Augmented Neural Networks.\rICML, 2016.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/mann/","title":"Meta-Learning with Memory-Augmented Neural Networks"},{"content":"预备知识 RNN, LSTM: 循环神经网络依赖历史状态和当前信号估计下一时刻的状态, 可用于预测, 回归等. 核心思想 知识追踪的目的是根据学生的历史作答情况 $\\mathbf{x}_0, \\ldots, \\mathbf{x}_t = \\{q_t, a_t\\}$ (其中 $q_t$ 表示 exercise, $a_t$ 表示做对做错的情况), 来估计当前学生的状态 (通过一个向量 $\\mathbf{y} \\in [0, 1]^{M}$, 第 $i$ 个元素表示该学生当前状态下做对第 $i$ 题的概率). 需要特别注意的是, DKT 设定一个题目对应两个独立表示 (做对, 做错).\nDKT 有很多用处:\n根据 $\\mathbf{y}_t$ 我们可以判断学生对不同题目的掌握情况, 据此可以个性化出题;\n判断题目间的关系, 作者给出一种 influence 指标:\n$$\rJ_{ij} = \\frac{y(j|i)}{\\sum_{k} y(j | k)},\r$$这里 $y(j|i)$ 表示仅观察到做对题目 $i$ 情况下做对题目 $j$ 的概率. 显然 $J_{ij}$ 越大两个题目越趋同.\n参考文献 Piech C., Bassen J., Huang J., Ganguli S.,\rSahami M., Guibas L., Sohl-Dickstein J.\rDeep Knowledge Tracing.\rNeurIPS, 2015.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/dkt/","title":"Deep Knowledge Tracing"},{"content":"预备知识 请了解 TIGER 和 UniSRec. 核心思想 本文投稿 ICLR 2025 被惨拒, 我看主要问题集中在方法层面过于简单. 个人认为确实如此, 不过有一些实验观察我感觉还是很有趣的, 至少我之前不清楚生成式推荐在冷启动上居然存在问题 (我一直认为这方面应该是其优势才对). 这里需要先声明一下实验设置:\nDense: 采用的 UnisRec 的训练方式, 同时依赖 text embedding 和 id embedding 的版本, Dense 是指其推荐的方式最终是通过一个编码得到的 user embedding 和所有的 item embedding 进行一一匹配计算相似度然后排名得到的. 其输入 item embedding 为: $$\r\\tag{1}\r\\mathbf{e}_i = \\underbrace{\\mathbf{x}_i}_{ID} + \\underbrace{\\mathbf{x}_i^{\\text{text}}}_{text} + \\underbrace{\\mathbf{x}_i^{\\text{pos}}}_{positional}.\r$$ Generative: 采用的是 TIGER, 其首先通过 RQ-VAE 将 item 的文本信息编码成离散的 token 表示, 称之为 semantic ID, 然后基于 semantic ID 进行推荐. 相较于 Dense, 它的词表会小很多, 因此某种程度上会更加高效一些. 如上图所示, 相比于 Dense, TIGER 在 In-set 和 Cold-start 场景下的效果都不尽如人意. 特别是后者, 会让人感觉特别奇怪.\n首先一个值得怀疑的点是不是通过向量量化得到的 semantic ID 不能像独立的 ID embedding 那样抓住 item-item 间的关系, 为此作者将 (1) 中的 ID $i$ 替换为 semantic ID $(s_i^1, s_i^2, \\cdots, s_i^m)$:\n$$\r\\tag{2}\r\\mathbf{e}_{s_i^j} = \\underbrace{\\mathbf{x}_{s_i^j}}_{ID} + \\underbrace{\\mathbf{x}_i^{\\text{text}}}_{text} + \\underbrace{\\mathbf{x}_i^{\\text{pos}} + \\mathbf{x}_j^{\\text{pos}}}_{positional}.\r$$于是, 此时每个 item 通过一组 embeddings 表示:\n$$\r\\mathbf{E}_{i} = [\\mathbf{e}_{s_i^1}, \\mathbf{e}_{s_i^2}, \\cdots, \\mathbf{e}_{s_i^m}].\r$$ 比较的结果如下: Dense (SID) 即 (2) 能够和普通的 UniSRec (Dense) 结果相当了, 且在冷启动上也有不错的效果, 这说明 semantic ID 本文对于 item-item 的关系的学习其实并不差. 如果将 TIGER 中的 semantic ID 替换为一般的 ID (TIGER (T)), 效果也并没有什么增长, 这进一步验证了这一点.\nOverfitting 作者认为, 导致 TIGER 等生成式推荐在冷启动商品上的推荐效果不太理想的主要原因是 TIGER 过拟合到了那些见过的 semantic ID. 如下图所示, 那些冷启动商品的得分 (概率) 不例外的远远低于很多 in-set 的 semantic ID. 作者提的改进有两点 (LIGER): 采用 (2) 的方式并同时用 UniSREC 的 cosine similarity loss 和 TIGER 的 next-token prediction loss 来训练. 推理的时候对冷启动商品进行一个特殊处理: 相当于将冷启动商品单独摘出来, 然后和通过生成式检索出来的商品一起再通过传统的相似度计算来排名, 从而规避上述的问题 (算是取巧吧, 没有真正解决这个问题). 参考文献 Yang L., Paischer F., Hassani K., Li J., Shao S.,\rLi Z. G., He Y., Feng X., Noorshams N., Park S., Long B.,\rNowak R. D., Gao X., and Eghbalzadeh H.\rUnifying Generative and Dense Retrieval for Sequential Recommendation.\rarXiv, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/liger/","title":"Unifying Generative and Dense Retrieval for Sequential Recommendation"},{"content":"预备知识 $\\mathcal{U} = \\{u_1, u_2, \\ldots, u_{|\\mathcal{U}|}\\}$, users; $\\mathcal{I} = \\{i_1, i_2, \\ldots, i_{|\\mathcal{I}|}\\}$, items; $\\mathbf{R} \\in \\{0, 1\\}^{|\\mathcal{U}| \\times |\\mathcal{I}|}$, interaction matrix; $\\mathcal{G}(\\mathcal{U}, \\mathcal{I}, \\mathcal{E})$, 对应的图. 核心思想 以前的方法大多为:\n纯的 ID-based 的方法, 即 user/item 均用可训练的 embeddings 表示; 纯的语义表示, 即 user/item 均用文本或者其它模态信息编码得到的编码; 二者混合. 本文属于第三种, 特别之处在于 user embeddings 随机初始化 $\\mathbf{h}_u^{(0)}$, item 的 embeddings 用语义表示 $\\mathbf{h}_i^{(0)}$ (通过训练好的语言模型得到).\nSemantic Aligning Phase: 这一阶段是为了训练 user embedddings, 使其和 item 的编码得到的语义特征对齐:\n首先经过 LightGCN:\n$$\r\\mathbf{h}_u, \\mathbf{h}_i = \\text{Aggregator}\r\\big(\r\\mathcal{G}(\\mathcal{U}, \\mathcal{I}, \\mathcal{E}),\r\\mathbf{h}_u^{(0)}, \\mathbf{h}_i^{(0)}\r\\big).\r$$ 通过类似 DirectAU 的方式对齐 $\\mathbf{h}_u \\rightarrow \\mathbf{h}_i$:\n$$\rl_{align}^{\\mathcal{U}} = \\frac{1}{|\\mathcal{E}|} \\sum_{(u, i) \\in \\mathcal{E}} \\| \\mathbf{h}_u - \\text{freeze}(\\mathbf{h}_i) \\|^2, \\\\\rl_{uniform}^{\\mathcal{U}}\r\\log \\frac{1}{|\\mathcal{U}|^2}\r\\sum_{u \\in \\mathcal{U}} \\sum_{u^* \\in \\mathcal{U}} e^{-2 \\| \\mathbf{h}_u - \\mathbf{h}_{u^*} \\|}.\r$$ 第二个损失是促进均匀分布的.\nCollaborative Refining Phase: 在上一阶段训练完毕之后, 在进行一个协同微调 (作者是这么认为的) 的过程:\n通过一个可训练的 MLP 得到\n$$\r\\mathbf{\\tilde{h}}_i^{(0)} = \\text{MLP}(\\mathbf{h}_i^{(0)}).\r$$后续的过程是一样的.\n然后通过如下的 alignment loss 来令 $\\mathbf{\\tilde{h}}_i \\rightarrow \\mathbf{h}_u$:\n$$\rl_{align}^I = \\frac{1}{|\\mathcal{E}|}\r\\sum_{(u, i) \\in \\mathcal{E}} \\| \\text{freeze}(\\mathbf{h}_u) - \\mathbf{\\tilde{h}}_i \\|^2, \\\\\rl_{uniform}^{\\mathcal{I}}\r\\log \\frac{1}{|\\mathcal{I}|^2}\r\\sum_{i \\in \\mathcal{I}} \\sum_{i^* \\in \\mathcal{I}} e^{-2 \\| \\mathbf{\\tilde{h}}_i - \\mathbf{\\tilde{h}}_{i^*}\\|}.\r$$ 注: 容易发现, 这个阶段新引入的 MLP 是不会改变 $\\mathbf{\\tilde{h}}_i$ 的 embedding size 的, 所以在实际中, embedding size 必须和 text encoder 的 embedding size 保持一致. 这不是一个好的设计.\nInference 的时候, 利用内积即可, 特别的是, 作者认为冷启动场景下不需要经过 MLP, 直接用 textual embedding 就可以了. 参考文献 Wang C., Yang L., Liu Z., Liu X., Liang Y., and Yu P. S.\rCollaborative Alignment for Recommendation.\rICDE, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/carec/","title":"Collaborative Alignment for Recommendation"},{"content":"预备知识 $\\mathcal{S} = \\{i_1, i_2, \\ldots, i_n\\}$, user behavior sequence; $\\mathcal{T}_i = \\{t_1^i, t_2^i, \\ldots, t_{|\\mathcal{T}_i|}^i\\}$, item $i$ 的文本描述 ($t_j^i$ 可以理解为其中的一个句子); $\\mathcal{V}_i = \\{v_1^i, v_2^i, \\ldots, v_{|\\mathcal{V}_i|}^i\\}$, item $i$ 所对应的图片特征. 核心思想 Multimodal Feature Extraction MP4SR 首先将 item 的文本和图片转换为特征:\n对于文本, 直接通过 Sentence-BERT 对每个句子进行编码得到: $$\r\\mathbf{x}_i^t = stack\r\\bigg[\r\\text{BERT}(t_1^i), \\text{BERT}(t_2^i), \\ldots,\r\\text{BERT}(t_{|\\mathcal{T}_i|}^i),\r\\bigg].\r$$ 对于图片的转换则较为特殊: 图片 $\\overset{\\text{CLIP}}{\\rightarrow}$ 特征 $\\overset{\\text{匹配文本Token}}{\\rightarrow}$ Top-N 文本 token: $$\rf(w) = \\text{sim} (\\text{CLIP}(v_{\\ell}^i), \\text{CLIP}(w)) \\quad \\forall w \\in \\mathcal{D}, \\\\\r\\mathbf{v}_{\\ell}^i = \\text{BERT}\\bigg(\rconcat \\big(\r\\text{TopN}(\r\\{f(w_1), \\ldots, f(w_{|\\mathcal{D}|})\\},\rN\r)\r\\big)\r\\bigg), \\\\\r\\mathbf{x}_i^v = stack\r\\bigg [\r\\mathbf{v}_1^i, \\mathbf{v}_2^i, \\ldots, \\mathbf{v}_{|\\mathcal{V}_i|}^i\r\\bigg ] \\in \\mathbb{R}^{|\\mathcal{V}_i| \\times d},\r$$其中 $\\mathcal{D}$ 表示整个词表. 所以其实是相当于给图片匹配它所对应的文本描述, 如此一来就省去了图片和文本模态对齐的问题.\nMultimodal Mixup Sequence Encoder Sequence Random Dropout: 随机的 Dropout, 提取出 $\\mathcal{S}$ 的部分子序列 $\\tilde{\\mathcal{S}}$. Text and Image Encoders: 这部分主要是将之前提的特征进行一个融合 (注意每个 item 有多个文本和图片表征). 以文本为例: $$\r\\alpha^t = \\text{softmax}\\big(\r(\\mathbf{x}_i^t \\mathbf{W}_1^t + \\mathbf{b}_1^t) \\mathbf{W}_2^t + b_2^t\r\\big), \\\\\r\\mathbf{e}_i^t = \\sum_{j=1}^{|\\mathcal{T}_i|} \\alpha_j^t \\mathbf{x}_i^t [j, :].\r$$然后通过 MoE 进行进一步特征变换, 最终得到整个文本/图片序列表征:\n$$\r\\mathbf{Z}^t = stack[\r\\mathbf{z}_1^t, \\mathbf{z}_2^t, \\ldots, \\mathbf{z}_{|\\mathcal{\\tilde{S}}|}^t\r], \\\\\r\\mathbf{Z}^v = stack[\r\\mathbf{z}_1^v, \\mathbf{z}_2^v, \\ldots, \\mathbf{z}_{|\\mathcal{\\tilde{S}}|}^v\r].\r$$ Complementary Sequence Mixup: 为了进一步抹除两个模态的差异, 以一个 $p \\in [0, 0.5]$ 的概率进行 Mixup (两个序列混合), 得到: $$\r\\mathbf{M}^t, \\mathbf{M}^v.\r$$ Transformer Layers: 对 $\\mathbf{M}^t, \\mathbf{M}^v$ 进行特征变化得到最终的表示: $$\r\\mathbf{h}^t, \\mathbf{h}^v.\r$$Pre-training Objectives 这部分主要涉及两个目标, 其实主要就是对比学习里面正负样本的构建问题: Modality-specific Next Item Prediction: 其主要是以 Next-item 的 embeddding 作为正样本, in-batch 内的其它作为负样本; Cross-Modality Contrastive Learning: 这部分就是要求文本和图片两部分模态互相对齐, 其余的模态特征为负样本. 参考文献 Zhang L., Zhou X., Zeng Z., and Shen Z.\rMultimodal Pre-training for Sequential Recommendation via Contrastive Learning\rTORS, 2024.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/mp4sr/","title":"Multimodal Pre-training for Sequential Recommendation via Contrastive Learning"},{"content":"\r## Vector Quantization\r\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e\u003cstrong\u003e表征学习\u003c/strong\u003e一直是深度学习的重点\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://miro.medium.com/v2/resize:fit:4416/format:webp/1*bvMhd_xpVxfJYoKXYp5hug.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"background-1\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eEncoder $\\phi: X \\rightarrow \\bm{z} \\in \\textcolor{red}{\\mathbb{R}^{d}}$ (连续空间)\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e向量量化: $X \\rightarrow \\bm{c} \\in \\mathcal{C} = \\{\\bm{c}_k\\}_{k=1}^K$ (离散空间)\n\u003c/li\u003e \u003c/ul\u003e \u003cp\u003e\u003cspan style=\"color: blue\"\u003e✓\u003c/span\u003e 离散化表示更符合人类语言和符号特性, 或许更利于生成任务\n\u003cp\u003e\u003cspan style=\"color: blue\"\u003e✓\u003c/span\u003e 更强的可解释性和控制性\n\u003cp\u003e\u003cspan style=\"color: blue\"\u003e✓\r\u003ch3 id=\"vae\"\u003eVAE\u003c/h3\u003e \u003col\u003e \u003cli\u003e \u003cp\u003eEncoder $\\phi$: 它将输入 $X \\in \\mathbb{R}^{H \\times W \\times 3}$ 映射到一个分布:\n$$\r\\bm{z} \\sim q(\\bm{z}|X; \\phi).\r$$\u003cp\u003ee.g., 高斯分布: $\\phi(\\bm{x}) \\rightarrow (\\bm{\\mu}, \\Sigma) \\rightarrow \\mathcal{N}(\\bm{\\mu}, \\Sigma)$.\n\u003c/li\u003e \u003cli\u003e \u003cp\u003eDecoder $\\Phi$: 它将隐变量 $\\bm{z}$ 映射回 (通常来说) $X$ 的空间:\n$$\rp(X|\\bm{z}; \\Phi);\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e还有一个先验分布 $p(\\bm{z})$ 用于辅助训练.\n\u003c/li\u003e \u003c/ol\u003e \u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003evan den Oord A., et al., Neural Discrete Representation Learning. NeurIPS, 2017.\n\u003ch3 id=\"vae-1\"\u003eVAE\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e训练目标:\n$$\r\\begin{align*}\r-\\log p(X) \\le \\underbrace{\\mathbf{KL}(q_{\\phi}\\| p(\\bm{z})) +\r\\mathbb{E}_{\\bm{z} \\sim q_{\\phi}} -\\log p(X|\\bm{z}; \\Phi)}_{\\text{negative ELBO}}.\r\\end{align*}\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e最小化 KL 散度促进 $q_{\\phi}$ 的散度\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e$-\\log p(X|\\bm{z}; \\Phi)$ 在高斯分布的假设下退化为重构损失:\n$$\r\\mathcal{L}_{rec} = \\| \\Phi(\\bm{z}) - X\\|_2^2\r$$\u003c/li\u003e \u003ch3 id=\"vq-vae\"\u003eVQ-VAE\u003c/h3\u003e \u003cul\u003e \u003cli\u003eVQ-VAE $\\bm{z}$ 通过可训练的 Codebook 来解决实现离散化:\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250310215306.png\" alt=\"Image\" style=\"max-width: 100%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003e$X \\in \\mathbb{R}^{H \\times W \\times 3} \\overset{\\phi}{\\rightarrow} Z \\in \\mathbb{R}^{H' \\times W' \\times d} \\overset{\\varphi}{\\rightarrow} \\hat{Z} \\in \\mathcal{C}^{H' \\times W' \\times d} \\rightarrow$\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e$\\varphi(\\bm{z}) = \\text{argmin}_{\\bm{c} \\in \\mathcal{C}} \\|\\bm{c} - \\bm{z}\\|$.\n\u003c/li\u003e \u003ch3 id=\"vq-vae-1\"\u003eVQ-VAE\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e$Z \\rightarrow \\hat{Z}$ 是离散的, 无法传递梯度.\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e\u003cstrong\u003eSTE\u003c/strong\u003e (straight-through estimator):\n$$\r\\tilde{Z} \\leftarrow Z + \\text{sg}\\big((\\hat{Z} - Z)\\big), \\\\\r\\mathrm{d}\\tilde{Z} = \\mathrm{d} Z + 0.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e训练目标:\n$$\r\\mathcal{L} = \\mathcal{L}_{rec} + \\underbrace{\r\\| \\text{sg} (Z) - \\hat{Z}\\|_2^2 +\r\\beta \\cdot \\| Z - \\text{sg} (\\hat{Z})\\|_2^2.\r}_{\\mathcal{L}_{commit}}\r$$\u003c/li\u003e \u003ch3 id=\"vq-gan\"\u003eVQ-GAN\u003c/h3\u003e \u003cul\u003e \u003cli\u003e图片 Token 化 + Next-token prediction $p(s_i | s_{\u003c i}, \\textcolor{red}{condition})$\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250311144000.png\" alt=\"Image\" style=\"max-width: 100%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eEsser P., et al. Taming Transformers for High-Resolution Image Synthesis. CVPR, 2021.\n\u003ch3 id=\"codebook-collapse\"\u003eCodebook Collapse\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e离散化操作终究是带来了训练困难:\n\u003col\u003e \u003cli\u003eCodebook 中部分向量过于接近而造成的冗余\u003c/li\u003e \u003cli\u003eCodebook 中部分向量由于训练始终匹配不到 $Z$ 导致的冗余\u003c/li\u003e \u003c/ol\u003e \u003c/li\u003e \u003cli\u003e \u003cp\u003e一些方案:\n\u003col\u003e \u003cli\u003e对于 codebook 采用 K-means ++ 初始化 [1];\u003c/li\u003e \u003cli\u003e对于训练不充分的向量重新初始化 [2];\u003c/li\u003e \u003cli\u003e用 Gumbel-softmax 替代 STE [3]\u003c/li\u003e \u003c/ol\u003e \u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003e[1] Lancucki A., et al. Robust Training of Vector Quantized Bottleneck Models. 2020.\n\u003cp style=\"margin: 2px 0;\"\u003e[2] Dhariwai P., et al. Jukebox: A Generative Model for Music. 2020.\n\u003cp style=\"margin: 2px 0;\"\u003e[3] Takida Y., et al. SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization. ICML, 2022.\n\u003ch3 id=\"residual-quantization-rq-vae\"\u003eResidual Quantization (RQ-VAE)\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eCollapse 问题通常是当 codebook size 增大的时候发生\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e减小 size $\\rightarrow$ 更差的表达能力 \u003cstrong\u003evs.\u003c/strong\u003e 增大 size $\\rightarrow$ Collpase\n\u003c/li\u003e \u003cli\u003e \u003cp\u003eRQ-VAE:\n$$\rZ \\overset{\\varphi}{\\rightarrow} \\textcolor{red}{\\hat{Z}_1}\r\\overset{Z - \\hat{Z}_1}{\\rightarrow} R_1\r\\overset{\\varphi}{\\rightarrow} \\textcolor{red}{\\hat{Z}_2}\r\\overset{R_1 - \\hat{Z}_2}{\\rightarrow} R_2\r\\rightarrow \\cdots\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e$\\hat{Z} = \\sum_{n}^N \\hat{Z}_n$, 离散化表示 $(k_1, k_2, \\ldots, k_N)$\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eLee D., et al. Autoregressive Image Generation using Residual Quantization. CVPR, 2022.\n\u003ch3 id=\"tiger\"\u003eTIGER\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e传统推荐 (matching):\n$$\r\\bm{e}_u^T \\bm{e}_v, \\quad v \\in \\mathcal{V}.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e检索式推荐:\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316175859.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eRajput S., et al. Recommender Systems with Generative Retrieval. NeurIPS, 2023.\n\u003ch3 id=\"tiger-1\"\u003eTIGER\u003c/h3\u003e \u003cul\u003e \u003cli\u003e检索式推荐:\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316180725.png\" alt=\"Image\" style=\"max-width: 100%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"fixed-codebook\"\u003eFixed Codebook\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e固定 Codebook 为 (size: $|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d$):\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\ldots \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d}.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e比如 $L = 3, d=3$:\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003e量化:\n$$\r\\bm{\\hat{z}} = \\varphi \\big(\\tanh(\\bm{z}) \\big) = \\textcolor{red}{\\text{round}} \\big(\r\\textcolor{blue}{\\tanh} (\\bm{z})\r\\big).\r$$\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eMentzer F., et al. Finite Scalar Quantization: VQ-VAE Made Simple. 2023.\n\u003ch3 id=\"总结\"\u003e总结\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e向量量化提供了一种 token 化的方式\n\u003c/li\u003e \u003cli\u003e \u003cp\u003eCodebook 的设定和学习仍存在问题\n\u003c/li\u003e ","permalink":"http://localhost:1313/slides/vq/","title":"Vector Quantization"},{"content":"\r## Pushing the Limits of Low-Bit Optimizers with a Focus on EMA Dynamics\r\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e模型大小飞速增加 vs. 硬件价格居高不下\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312203012.png\" alt=\"Image\" style=\"max-width: 65%; height: auto; margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cul\u003e \u003cli\u003e解决方案: \u003cul\u003e \u003cli\u003eMoE, LoRA; ZeRO, FSDP;\u003c/li\u003e \u003cli\u003eNetwork Quantization; \u003cspan style=\"color: red;\"\u003eLightweight Optimizers\u003c/span\u003e\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003ch3 id=\"background-1\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eOptimizer States (2x model size):\n$$\rm_{t+1} \\leftarrow \\beta_1 \\cdot m_t + (1 - \\beta_1) \\cdot g, \\\\\rv_{t+1} \\leftarrow \\beta_2 \\cdot v_t + (1 - \\beta_2) \\cdot g^2.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003eDeepSeek-v3 训练框架: $g \\overset{\\text{BF16}}{\\rightarrow} m, v \\overset{\\text{FP32}}{\\rightarrow} \\theta$\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312204230.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"quantization-and-dequantization\"\u003eQuantization and Dequantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eQuantization:\n$$\rq = Q(x) := \\mathop{\\text{argmin}} \\limits_{k=0}^{2^b - 1} \\big|\\frac{x}{\\textcolor{red}{\\Delta}} - \\textcolor{red}{\\iota_k} \\big|.\r$$\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312205652.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003eDequantization:\n$$\r\\tilde{x} = Q^{\\dagger}(q) := \\iota_{q} \\cdot \\Delta.\r$$\n\u003ch3 id=\"stateful-optimizers-in-ultra-low-bits\"\u003e\u003cu\u003eS\u003c/u\u003etateful \u003cu\u003eO\u003c/u\u003eptimizers in Ultra \u003cu\u003eLO\u003c/u\u003ew Bits\u003c/h3\u003e \u003cul\u003e \u003cli\u003eLow-Bitwidth EMA update:\u003c/li\u003e \u003c/ul\u003e $$\r\\begin{array}{rl}\r\\text{Dequantization: } \u0026 \\tilde{x}_t = Q^{\\dagger}(q_t) = \\iota_{q_t} \\cdot \\Delta_t, \\\\\r\\text{EMA update: } \u0026 \\hat{x}_{t+1} \\leftarrow \\beta \\cdot \\tilde{x}_t + (1 - \\beta) \\cdot z_{t + 1}, \\\\\r\\text{Quantization: } \u0026 q_{t+1} = Q(\\hat{x}_{t+1}).\r\\end{array}\r$$\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eDettmers T., et al. 8-bit Optimizers via Block-wise Quantization. ICLR, 2022.\n\u003cp style=\"margin: 2px 0;\"\u003eLi B., et al. Memory Efficient Optimizers with 4-bit States. NeurIPS, 2023.\n\u003ch3 id=\"quantization-for-unsigned-ema-update\"\u003eQuantization for Unsigned EMA Update\u003c/h3\u003e \u003cul\u003e \u003cli\u003e\u003cem\u003eSignal Swamping\u003c/em\u003e (\u003cu\u003elarge-to-small number addition\u003c/u\u003e)\u003c/li\u003e \u003c/ul\u003e $$\r\\text{EMA update: } \\hat{x}_{t+1} \\leftarrow \\beta \\cdot \\tilde{x}_t + \\underbrace{\\textcolor{red}{(1 - \\beta) \\cdot z_{t + 1}}}_{\\text{very small for } \\beta \\rightarrow 1}.\r$$\u003cdiv style=\"text-align: center; margin-top: 50px; margin-bottom: -80px; padding: 0\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312211840.png\" alt=\"Image\" style=\"max-width: 70%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eHigham N. J. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing. 1993.\n\u003ch3 id=\"signal-swamping\"\u003eSignal Swamping\u003c/h3\u003e \u003cul\u003e \u003cli\u003e总结\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312212039.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"case-study\"\u003eCase Study\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312212821.png\" alt=\"Image\" style=\"max-width: 90%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003e一定\u003cspan style=\"color: red\"\u003e条件\u003c/span\u003e下:\n\u003cul\u003e \u003cli\u003eLinear 下全部不更新\u003c/li\u003e \u003cli\u003eDE 下部分更新\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e \u003cp\u003e实际上 $\\beta \\ge 0.9$ 为\u003cspan style=\"color: red\"\u003e相当常见的 setting\u003c/span\u003e\n\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e\r\u003ch3 id=\"case-study-1\"\u003eCase Study\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cul\u003e \u003cli\u003e$X \\in \\mathbb{R}^{1000}$\u003c/li\u003e \u003cli\u003e$Z \\sim \\mathcal{U}[0, 1]$\u003c/li\u003e \u003c/ul\u003e \u003cp\u003e\u003cspan style=\"color: red;\"\u003e\u003cem\u003eX\u003c/em\u003e\u003c/span\u003e Fixed $\\Delta$\n\u003cp\u003e\u003cspan style=\"color: red;\"\u003e\u003cem\u003eX\u003c/em\u003e\u003c/span\u003e $z \\le \\Delta$\n\u003cul\u003e \u003cli\u003e理论收敛至: 0.5\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312213810.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003ch3 id=\"stochastic-rounding\"\u003eStochastic Rounding\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e假设 $\\iota_{k-1} \\le x / \\Delta \\le \\iota_k$:\n$$\rQ_{sr}(x) :=\r\\left \\{\r\\begin{array}{ll}\rk-1 \u0026 w.p. \\quad \\frac{\\iota_k - x / \\Delta}{ \\iota_k - \\iota_{k-1}}, \\\\\rk \u0026 w.p. \\quad \\frac{x / \\Delta - \\iota_{k-1}}{ \\iota_k - \\iota_{k-1}}.\r\\end{array}\r\\right .\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003eHigh variance:\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313112908.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization\"\u003eLogarithmic Quantization\u003c/h3\u003e $$\r\\begin{array}{ll}\rQ(x) \u0026=\\text{Clip}(\\lfloor \\log_{\\alpha} \\frac{x}{\\Delta} + \\xi \\rceil; 0, 2^b - 1) \\\\\r\u0026\\approx \\mathop{\\text{argmin}} \\limits_{k=0}^{2^b - 1} \\big|\\frac{x}{\\Delta} \\cdot \\alpha^\\xi - \\iota_k \\big|,\r\\end{array}\r$$\u003cul\u003e \u003cli\u003e3-bit quantization anchors:\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313113440.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization-1\"\u003eLogarithmic Quantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e2-bit quantization illustration\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313113535.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization-2\"\u003eLogarithmic Quantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eEasy to implement\n\u003c/li\u003e \u003cli\u003e \u003cp\u003eState Decay Alignment\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313115306.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"quantization-for-signed-ema-update\"\u003eQuantization for Signed EMA Update\u003c/h3\u003e \u003cp\u003e\u003cstrong\u003eX\u003c/strong\u003e \u003cspan style=\"color: gray\"\u003eSingal Swamping\u003c/span\u003e\n\u003cp\u003e\u003cstrong\u003e✓\u003c/strong\u003e \u003cstrong\u003eSign representation\u003c/strong\u003e\n\u003cp\u003e\u003cstrong\u003e✓\u003c/strong\u003e \u003cstrong\u003eDescent direction\u003c/strong\u003e\n\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314115701.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"theoretical-analysis\"\u003eTheoretical Analysis\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314115959.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cp\u003e$\\rightarrow$ \u003cspan style=\"color: red\"\u003eBits $\\downarrow$\u003c/span\u003e or \u003cspan style=\"color: red\"\u003e$\\beta \\uparrow$ \u003c/span\u003e\n\u003cp\u003e$\\rightarrow$ Quantization errors \u003cspan style=\"color: red\"\u003e$\\uparrow$\u003c/span\u003e\n\u003cp\u003e$\\rightarrow$ gradient variance \u003cspan style=\"color: red\"\u003e $\\uparrow$ \u003c/span\u003e\n\u003cp\u003e$\\rightarrow$ \u003cspan style=\"color: red\"\u003e bad \u003c/span\u003e convergence\n\u003c/div\u003e\r\u003ch3 id=\"momentum-adjustment\"\u003eMomentum Adjustment\u003c/h3\u003e \u003cul\u003e \u003cli\u003e\u003cstrong\u003e方差控制:\u003c/strong\u003e 选择 $\\beta'$ 满足:\u003c/li\u003e \u003c/ul\u003e $$\r\\underbrace{\\frac{\\textcolor{gray}{\\beta'}}{1 - \\textcolor{gray}{\\beta'}} r_{\\text{median}}(b')}_{\\textcolor{gray}{\\text{undetermined}}}\r\\le \\underbrace{\\frac{\\beta}{1 - \\beta} r_{\\text{median}}(b)}_{\\textcolor{green}{\\text{valid setup}}}.\r$$\u003cul\u003e \u003cli\u003e\u003cstrong\u003e查表:\u003c/strong\u003e (\u003cu\u003e灰色区域代表了经验可行的参数推荐\u003c/u\u003e)\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314121510.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"experiments\"\u003eExperiments\u003c/h3\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250417145811.png\" alt=\"Image\" style=\"max-width: 85%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"loss\"\u003eLoss\u003c/h3\u003e \u003cul\u003e \u003cli\u003e损失正常收敛\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250319170139.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"quantile\"\u003eQuantile $x_p$\u003c/h3\u003e \u003cul\u003e \u003cli\u003e基本上 $p \\in [0.05, 0.3]$ 都有不错的性能\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250319170604.png\" alt=\"Image\" style=\"max-width: 55%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"beta-block-size\"\u003eBeta, Block size\u003c/h3\u003e \u003cul\u003e \u003cli\u003e损失正常收敛\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250407200935.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"2nd-state-distribution\"\u003e2nd State Distribution\u003c/h3\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250407201114.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"adabelief\"\u003eAdaBelief\u003c/h3\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250407202155.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\rThanks!\r","permalink":"http://localhost:1313/slides/solo/","title":"SOLO"},{"content":"预备知识 请务必了解 VQ-VAE. 核心思想 RQ-VAE 自称也是为了解决所谓的 codebook collapse 问题, 即当 codebook size 逐渐增加的时候, 或有越来越多的向量变得\u0026quot;冗余\u0026quot;.\n另一方面, 如果我们减少 codebook size, 很容易相当在向量量化的过程会造成非常大的信息损耗. 于是, 本文提出了 RQ-VAE, 本质上是一个向量逐步地匹配 $D$ 个向量, 而非 one-to-one 的模式.\nRQ-VAE 的过程可以如此形式化:\n给定图片输入 $\\mathbf{X} \\in \\mathbb{R}^{H_o \\times W_o \\times 3}$;\n经过 Encoder $E$ 得到\n$$\r\\mathbf{Z} = E(\\mathbf{X}) \\in \\mathbb{R}^{\r\\underbrace{H_o / f}_{=: H} \\times \\underbrace{W_o / f}_{=: W} \\times n_z\r};\r$$ 给定 codebook $\\mathcal{C} = \\{\\mathbf{e}_k\\}_{k \\in [K]}$, 进行向量量化:\n$$\rQ(\\mathbf{z} \\in \\mathbb{R}^{n_z}; \\mathcal{C})\r= \\text{argmin}_{k \\in [K]} \\|\\mathbf{z} - \\mathbf{e}_k \\|_2^2,\r$$对于 $\\mathbf{Z}$ 来说, 可以得到如下的 codes:\n$$\r\\mathbf{M}^{(1)} \\in [K]^{H \\times W}, \\\\\r\\mathbf{M}_{hw}^{(1)} = Q(\\mathbf{Z}_{hw}; \\mathcal{C});\r$$ 计算量化之后的残差\n$$\r\\mathbf{R}^{(d)} = \\mathbf{R}^{(d-1)} - \\mathbf{E}_{\\mathbf{M}^{(d)}}, \\quad d \\ge 1, \\\\\r\\mathbf{R}^{(0)} = \\mathbf{Z}.\r$$这里 $\\mathbf{E}_{\\mathbf{M}^{(0)}} \\in \\mathbb{R}^{H \\times W \\times n_z}$ 表示对应 codes 的向量表示. 将 $\\mathbf{R}^{(d)}$ 重新上述的向量量化过程.\n假设我们总归进行了 $D$ 步残差量化, 我们可以得到\n$$\r\\mathbf{M} \\in \\mathbb{R}^{H \\times W \\times D}\r$$的 codes.\n通过 Decoder $G$ 恢复图像:\n$$\r\\mathbf{\\hat{X}} = G(\\mathbf{\\hat{Z}}), \\quad \\mathbf{\\hat{Z}} = \\sum_{d=1}^D \\mathbf{E}_{\\mathbf{M}^{(d)}}.\r$$ 容易发现, 残差量化实际上就是希望一步一步地用 codebook 来表示自己 (有点 PCA 降维的感觉). 所以它的训练目标也是类似的:\n$$\r\\mathcal{L}_{\\text{recon}} = \\|\\mathbf{X} - \\mathbf{\\hat{X}} \\|_2^2, \\\\\r\\mathcal{L}_{\\text{commit}} =\r\\sum_{d=1}^D \\bigg \\| \\mathbf{Z} - \\text{sg}\r\\big[\r\\mathbf{\\hat{Z}}^{(d)}\r\\big]\r\\bigg \\|_2^2, \\\\\r\\mathbf{\\hat{Z}}^{(d)} = \\sum_{d'=1}^d \\mathbf{E}_{\\mathbf{M}^{(d')}}.\r$$注意到, 这里的 commit loss 部分, 要求 $\\mathbf{Z}$ 和每一个累积的近似部分近似, 以鼓励每个量化阶段都能抓住足够的信息, 此外这里吧 VQ-VAE 中的 $\\|\\text{sg}(\\mathbf{Z}) - \\mathbf{\\hat{Z}}\\|$ 给删掉了, 这部分主要用于 codebook 的监督和更新. 这里作者说:\nThe codebook $\\mathcal{C}$ is updated by the exponential moving average.\n此外, RQ-VAE 也用了 VQGAN 里建议的 adversarial training 用来提高生成图片的质量.\n通过上面的部分我们就能够进行有效的量化了, 至于怎么使用中间的离散表示就仁者见仁智者见智了. 本文给的建议是 (如上图所示) 按照 depth 相加得到一个个 token (相当于就是用 $\\mathbf{\\hat{Z}}$ 预测后续的 token).\n参考文献 Lee D., Kim C., Kim S., Cho M. and Han W.\rAutoregressive Image Generation using Residual Quantization.\rCVPR, 2022.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/rqvae/","title":"Autoregressive Image Generation using Residual Quantization"},{"content":"预备知识 请了解 RQ-VAE. 核心思想 目前主流的推荐系统的一个痛点是将 item 表示为一个 embedding, 这就导致对于冷启动的场景并不友好 (既然我们没法再立即获得高效的新来的 item 的表示). 此外, 现阶段的推荐系统大多采用 matching 的架构 (在 item 数量较多的时候可能会慢一些), 本文探索一种生成式的检索方式.\n本文所提出的 Tiger 依然 RQ-VAE, 对 item 的文本 embedding 首先进行编码, 得到的编码作为 item 的 \u0026lsquo;ID\u0026rsquo;, 后面的模型只需要在此基础上进行预测即可.\n如上图所示, 文本的 embedding 经过 RQ-VAE (codebook 不共享) 得到 semantic codes. 比如 codebook 的size 为 8, 则理论上可以表示 $8^K$ 个 items (这里 $K$ 表示残差量化的次数).\n在第一阶段训练完毕之后, 我们就可以用得到的编码作为每个 item 的 \u0026lsquo;ID\u0026rsquo;, 然后就可以训练一个模型来进行生成式推荐, 这里文中给了一个例子:\n对于一个新来的 item, 只需要 -\u0026gt; Tiger 编码 -\u0026gt; 就可以用于预测了. 注: 这里省略了避免 ID 碰撞的细节.\n参考文献 Rajput S., Mehta N., Singh A., Keshavan R., Vu T.,\rHeldt L., Hong L., Tay Y., Tran V. Q., Samost J., Kula M.,\rChi E. H. and Sathiamoorthy M.\rRecommender Systems with Generative Retrieval.\rNeurIPS, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/tiger/","title":"Recommender Systems with Generative Retrieval"},{"content":"招聘布局 此次招聘位于交大学术活动中心二楼的一角, 并不显眼, 外头也缺少足够的引导标志. 约莫着有三四十家高校和事业单位分列五排, 蓝白的布告栏竖在一张张桌子之后, 颇有严正以待的架势. 进门开始是东北大学的场地, 紧接着是荆楚学院、江西铜业技术研究院、某某集团. 较为心仪的嘉兴大学坐落在第三排的中部, 围满了一批批人. 其对面是第四排的丽水学院, 而第四排的尾部是南京理工大学. 转过去, 赫然便是湖南大学四个大字, 穿行而过是成都大学、中国警察学院、扬州大学以及坐落在角落的之江实验室.\n聊天过程 称之为聊天过程而非是面试是因为一来本次主题是双选会, 二来本身也不指望能够凭借这场招聘会找到心仪的工作, 更多的是摸摸清楚自己到底几斤几两.\n江西铜业技术研究院 我上来盯着这家单位的布告栏看, 想看看这类研究院具体有个什么要求. 不过面前的小哥倒是很热情地邀请我坐下来聊一聊, 不过得知我对此并无兴趣之后我们就分道扬镳了.\n某医科单位 很抱歉忘记具体的名字了, 负责招聘的人也是非常热情. 但是我说我的方向和医学可能八竿子打不着. 虽然上面也明确需要人工智能方面的人才, 但是大抵是需要医学图像的? 招聘人员拿着我的简历, 跳过了前面部分, 竟然直接跳到了\u0026quot;技能和语言\u0026quot;一栏, 说着 Python, PyTorch 和我们的需求挺符合的. 我感到奇怪, 我觉得这些是无关紧要的东西, 不过想来也能理解, 来招聘的多是为整个单位招人.\n南阳理工学院 这个主要是跟在师妹边上旁听的, 只能说是大开眼界: 据招聘人来说, 他们学院可以非常痛快地给安家费, 然后每个月到手能到 13K (副教授待遇), 而且科研横向不收取管理费! 另外, 需要说明的是, 这 13K 如果后期考核不通过会降到 11K (但是招聘说这句话的时候明显咯噔了一些, 大概率存在猫腻).\n优点: 没有非升即走, 博士毕业点击就送, 安家费给的痛快, 考核要求很低 缺点: 这类学院我个人都对他们是否能够长期维持存在疑问, 另外过于痛快总让我感觉其中有猫腻 丽水学院 丽水学院的招聘老师人挺好, 也没啥架子. 比较有趣的是, 丽水学院里面设立了一个 \u0026ldquo;数学与计算机学院\u0026rdquo;, 我当时的第一个感觉就是这玩意儿可真有意思. 由于对地域不太感兴趣, 仅探听到了大约年薪十几万这个普遍的待遇条件.\n中国刑事警察学院 (沈阳) 两位穿着警服的大佬端坐在我面前. 但是双方好像都比较拘束, 没一会儿我就又站起来了, 得到了如下的信息:\n没有非升即走 虽然招聘人员说教课, 科研都有, 但是大概率是教课为主 嘉兴大学 兜兜转转终于是坐下了. 我实在是没有想到嘉兴大学会受到这般追捧, 桌上的招聘简章早早就被拿光了.\n工资年薪十几万, 每个月到手 7000 左右, 说实话感觉嘉兴大学的野心挺大的, 招聘里面我大抵只能够到最后一档的优秀博士里面. 不过看着安家费给的不少 (80 万), 但是但是, 我如果去嘉兴大学我还买个锤子房子 (但是不买房这笔钱就木得了).\n人工智能学院: 计算机科学与技术、智能科学与技术、网络空间安全 (陈院长 13819330328, chenbin@zjxu.edu.cn; 王老师, 18258330972, rgznxy@zjxu.edu.cn) 数据科学学院: 统计学、数学 (宁院长, 13516735266, nzj76@163.com; 艾老师, 0573-83640102, skxy@zjxu.edu.cn) 荆楚理工学院 我本来想去门口的东北大学碰碰的, 但是由于那边人太多了, 我只能在一旁干看着. 结果, 荆楚学院的招聘人员就直接热情地招呼我了, 即使我说不太感兴趣也无用. 荆楚学院的招聘人员是人工智能学院的院长, 我当时听到这个 title 的时候简直虎躯一震, 咋现在人工智能都火到这地步了吗, 计算机学院没有就直接开设人工智能学院了. 进一步了解后, 她居然说周院长是她师弟, 只能说无巧不成书了.\n优点: 说是能给到为 A 类博士之上的待遇 (75-85 万安家费 (含荆楚市人才津贴), 15-20 万科研启动费); 可以安排配偶工作, 如果配偶是硕士可以安排当讲师 缺点: 没有硕士点, 只能说发展潜力巨大啊 注: 荆楚理工学院人工智能学院的招聘公告上: 安家费 75 -\u0026gt; 45, 科研启动费: 20 -\u0026gt; 5, 这啥意思?\n成都大学 成都大学的老师过于傲慢了, 感觉压根没有把人放在眼里. 上来我递了简历, 他说需要把发表成果的分区写上, 我老老实实地写上. 看到我有些成果后态度才有所改观.\n特聘研究员: 35-50 万年薪, 80-100 万元 科研启动费, 80 万购房补贴 + 10 万安家费 特聘副研究员: 25-40 万年薪, 40-60 万元 科研启动费, 50 万购房补贴 + 8 万安家费 优秀青年博士: 基本工资, 10 万元 科研启动费, 15 万购房补贴 + 5 万安家费 我问了 \u0026ldquo;特聘副研究员\u0026rdquo; 的考核要求: 青基 + 5 年 10 篇 (一区?) + \u0026hellip; 完全不感兴趣了, 只能说打扰了.\n计算机学院, 洪老师, 028-84616938, computer_yb@cdu.edu.cn\n注: 最离谱的一件事, 成都大学居然没有数学学院, 全校的数学课程是由计算机学院的老师负责的. 我当时好感就去了大半, 我不认为高数现代能够满足需求.\n扬州大学 最后去扬州大学走了一遭, 感觉这个整体上还挺中规中矩的.\n有三篇一区的文章 (要求其中一篇是顶刊) 就能评青年百人, 年薪 35 万 (不记得是不是啥安家费算在里头了), 然后考核是三年三篇 (一区?). 数学科学学院: 书记, zhling@yzu.edu.cn, 0514-87975259; 主任, liyyoung@yzu.edu.cn, 0514-87975509\n信息工程学院: 院长, 0514-87978037, xbsun@yzu.edu.cn; 书记, 0514-87978309, lvhm@yzu.edu.cn\n总结 虽然网上说现在江浙沪的双非都得非升即走了, 但是面试的这几个除了成都大学外都没有非升即走的说法 (可能层次还是低了).\n\u0026lsquo;学院\u0026rsquo;的待遇普遍会比\u0026rsquo;大学\u0026rsquo;好很多, 后者能拿到 10K 应该可以烧高香了.\n虽然大部分院校没有给出具体界定 \u0026lsquo;人才\u0026rsquo; 的标准, 大体上有个 5 篇顶会顶刊应该就都有的谈了.\n以后简历的成果部分得把分区加上, 因为招聘的人并不一定熟悉.\n","permalink":"http://localhost:1313/posts/%E4%B8%AD%E7%A0%94%E6%98%A525%E6%8B%9B%E8%81%98/","title":"中研春招聘小记"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\ldots \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它将输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n容易发现, 这其实相当于我们的后验分布为:\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. 对于第二点, 作者建议采取 straight-through estimator, 另外设计了另外两个损失用于训练 $\\phi$ 以及 codebook $E$:\n$$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"}]