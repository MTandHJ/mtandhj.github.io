[{"content":" Navon A., Achituve I., Maron H., Chechik G. and Fetaya E. Auxiliary learning by implicit differentiation. ICLR, 2021.\n概 通过 implicit differentiation 优化一些敏感的参数.\n$$\r1 + 2f(x)\r\\phi \\ell_{main}\r$$\nAuxiLearn 在实际的训练中, 我们常常会通过一些额外的任务来帮助更好的训练. $$\r\\ell_{main}, \\bm{x}\r$$ 其中 $\\phi_k \\ge 0$ 是第 $k$ 个额外任务 $\\ell_k$ 的系数.\n$$\rw_{t+1} \\leftarrow \\arg \\min_{W} \\mathcal{L}_T(W; \\phi_t) \\phi_{t+1}\r$$ 但是很显然, 如果利用梯度下降学习 $\\phi_k$ 并通过 clip 保证 $\\phi_k \\ge 0$, 一定会导致 $\\phi_k \\equiv 0$ 这一平凡解.\n问题设定 现在让我们来设定一个更加一般的问题: $$\r\\ell_{main}(W; \\mathcal{D}_{train})\r$$ 其中 $W \\in \\mathbb{R}^n$ 是模型中的基本参数, $\\phi \\in \\mathbb{R}^m$ 是一些其它的超参数, 然后 $D_{train}, D_{aux}$ 表示训练集和额外的集合 (比如验证集).\n不考虑 mini-batch, 合理的训练流程应该是: 如此重复. 就能够避免 $\\phi$ 的平凡解.\n当然, 如果每一次都严格按照两阶段计算, 计算量是相当庞大的 (比 grid search 也是不遑多让). 本文所提出来的 AuxiLearn 的改进就是提出了一种近似方法. 它的理论基础是 Implicit Function Theorem (IFT).\n$$\r\\nabla_{\\phi} \\mathcal{L}_A = \\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{\\nabla_{\\phi} W^*}_{n \\times m}.\r$$ 显然, 其中 $\\nabla_W \\mathcal{L}_A$ 是好计算的, 问题在于 $\\nabla_{\\phi} W^*$ 的估计.\n$$\rF(p, q) = 0,\r$$$$\rF(x, \\Phi(x)) = 0\r$$ 在某个集合上均成立.\n$$\r\\nabla_W \\mathcal{L}_T(W^*, \\phi) = 0,\r$$$$\r\\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0,\r$$$$\r\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0 \\\\\r\\Rightarrow\r\\nabla_W^2 \\mathcal{L}_T \\cdot \\nabla_{\\phi} W^* + \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T = 0 \\\\\r\\Rightarrow\r\\nabla_{\\phi} W^* = - (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T.\r$$ $$\r\\nabla_{\\phi} \\mathcal{L}_A = -\\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{(\\nabla_W^2 \\mathcal{L}_T)^{-1}}_{n \\times n}) \\cdot \\underbrace{\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T}_{n \\times m}.\r$$ $$\r(I - X)^{-1} = \\sum_{t} X^t \\Rightarrow X^{-1} = (I - (I - X))^{-1} = \\sum_{t} (I - X)^t.\r$$ 于是便得到了本文 AuxiLearn 算法 (算法 2 其实就是 Neumann series 的前 $J$ 项):\n理解两阶段的训练 $$\r\\mathcal{L}_T(W, \\phi) = \\ell_{main}(W; \\mathcal{D}_{train}) + \\phi \\cdot \\ell_{aux}(W; \\mathcal{D}_{train}).\r$$ $$\r\\begin{array}{ll}\r\\frac{d \\mathcal{L}_A}{d \\phi}\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} (\\nabla_W \\mathcal{\\ell}_{main}(\\mathcal{D}_{train}) + \\phi \\nabla_W \\ell_{aux}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}). \\\\\r\\end{array}\r$$ $$\r\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \u003e 0,\r$$ 即当主任务在 aux 集合上的更新方向和辅任务在训练集上在 $\\nabla_W^2 \\mathcal{L}_T^{-1}$ 意义上方向一致.\n代码 [official-code]\n","permalink":"http://localhost:1313/posts/test1/","title":"Test1"},{"content":"[TOC]\nChoi J., Wang Z., Venkataramani S., Chuang P. I., Srinivasan V. and Gopalakrishnan K. PACT: Parameterized clipping activation for quantized neural networks. 2018.\n概 本文提出对网络中的激活值进行裁剪以实现更低量化.\n主要内容 $$\ry = PACT(x) = 0.5 (|x| - |x - \\alpha| + \\alpha)\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, 0), \\\\\rx, \u0026 x \\in [0, \\alpha), \\\\\r\\alpha, \u0026 x \\in [\\alpha, +\\infty).\r\\end{array}\r\\right .\r$$ $$\r\\frac{\\partial y_q}{\\partial \\alpha}\r=\\frac{\\partial y_q}{\\partial y}\r\\frac{\\partial y}{\\partial \\alpha}\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, \\alpha), \\\\\r1, \u0026 x \\in [\\alpha, +\\infty].\r\\end{array}\r\\right.,\r$$ 其中 $y_q = round(y \\cdot \\frac{2^k - 1}{\\alpha}) \\cdot \\frac{\\alpha}{2^k - 1}$.\n","permalink":"http://localhost:1313/posts/test2/","title":"Test2"},{"content":" Navon A., Achituve I., Maron H., Chechik G. and Fetaya E. Auxiliary learning by implicit differentiation. ICLR, 2021.\n概 通过 implicit differentiation 优化一些敏感的参数.\n$$\r1 + 2f(x)\r\\phi \\ell_{main}\r$$\nAuxiLearn 在实际的训练中, 我们常常会通过一些额外的任务来帮助更好的训练. $$\r\\ell_{main}, \\bm{x}\r$$ 其中 $\\phi_k \\ge 0$ 是第 $k$ 个额外任务 $\\ell_k$ 的系数.\n$$\rw_{t+1} \\leftarrow \\arg \\min_{W} \\mathcal{L}_T(W; \\phi_t) \\phi_{t+1}\r$$ 但是很显然, 如果利用梯度下降学习 $\\phi_k$ 并通过 clip 保证 $\\phi_k \\ge 0$, 一定会导致 $\\phi_k \\equiv 0$ 这一平凡解.\n问题设定 现在让我们来设定一个更加一般的问题: $$\r\\ell_{main}(W; \\mathcal{D}_{train})\r$$ 其中 $W \\in \\mathbb{R}^n$ 是模型中的基本参数, $\\phi \\in \\mathbb{R}^m$ 是一些其它的超参数, 然后 $D_{train}, D_{aux}$ 表示训练集和额外的集合 (比如验证集).\n不考虑 mini-batch, 合理的训练流程应该是: 如此重复. 就能够避免 $\\phi$ 的平凡解.\n当然, 如果每一次都严格按照两阶段计算, 计算量是相当庞大的 (比 grid search 也是不遑多让). 本文所提出来的 AuxiLearn 的改进就是提出了一种近似方法. 它的理论基础是 Implicit Function Theorem (IFT).\n$$\r\\nabla_{\\phi} \\mathcal{L}_A = \\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{\\nabla_{\\phi} W^*}_{n \\times m}.\r$$ 显然, 其中 $\\nabla_W \\mathcal{L}_A$ 是好计算的, 问题在于 $\\nabla_{\\phi} W^*$ 的估计.\n$$\rF(p, q) = 0,\r$$$$\rF(x, \\Phi(x)) = 0\r$$ 在某个集合上均成立.\n$$\r\\nabla_W \\mathcal{L}_T(W^*, \\phi) = 0,\r$$$$\r\\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0,\r$$$$\r\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0 \\\\\r\\Rightarrow\r\\nabla_W^2 \\mathcal{L}_T \\cdot \\nabla_{\\phi} W^* + \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T = 0 \\\\\r\\Rightarrow\r\\nabla_{\\phi} W^* = - (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T.\r$$ $$\r\\nabla_{\\phi} \\mathcal{L}_A = -\\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{(\\nabla_W^2 \\mathcal{L}_T)^{-1}}_{n \\times n}) \\cdot \\underbrace{\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T}_{n \\times m}.\r$$ $$\r(I - X)^{-1} = \\sum_{t} X^t \\Rightarrow X^{-1} = (I - (I - X))^{-1} = \\sum_{t} (I - X)^t.\r$$ 于是便得到了本文 AuxiLearn 算法 (算法 2 其实就是 Neumann series 的前 $J$ 项):\n理解两阶段的训练 $$\r\\mathcal{L}_T(W, \\phi) = \\ell_{main}(W; \\mathcal{D}_{train}) + \\phi \\cdot \\ell_{aux}(W; \\mathcal{D}_{train}).\r$$ $$\r\\begin{array}{ll}\r\\frac{d \\mathcal{L}_A}{d \\phi}\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} (\\nabla_W \\mathcal{\\ell}_{main}(\\mathcal{D}_{train}) + \\phi \\nabla_W \\ell_{aux}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}). \\\\\r\\end{array}\r$$ $$\r\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \u003e 0,\r$$ 即当主任务在 aux 集合上的更新方向和辅任务在训练集上在 $\\nabla_W^2 \\mathcal{L}_T^{-1}$ 意义上方向一致.\n代码 [official-code]\n","permalink":"http://localhost:1313/posts/test1/","title":"Test1"},{"content":"[TOC]\nChoi J., Wang Z., Venkataramani S., Chuang P. I., Srinivasan V. and Gopalakrishnan K. PACT: Parameterized clipping activation for quantized neural networks. 2018.\n概 本文提出对网络中的激活值进行裁剪以实现更低量化.\n主要内容 $$\ry = PACT(x) = 0.5 (|x| - |x - \\alpha| + \\alpha)\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, 0), \\\\\rx, \u0026 x \\in [0, \\alpha), \\\\\r\\alpha, \u0026 x \\in [\\alpha, +\\infty).\r\\end{array}\r\\right .\r$$ $$\r\\frac{\\partial y_q}{\\partial \\alpha}\r=\\frac{\\partial y_q}{\\partial y}\r\\frac{\\partial y}{\\partial \\alpha}\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, \\alpha), \\\\\r1, \u0026 x \\in [\\alpha, +\\infty].\r\\end{array}\r\\right.,\r$$ 其中 $y_q = round(y \\cdot \\frac{2^k - 1}{\\alpha}) \\cdot \\frac{\\alpha}{2^k - 1}$.\n","permalink":"http://localhost:1313/posts/test2/","title":"Test2"},{"content":" Navon A., Achituve I., Maron H., Chechik G. and Fetaya E. Auxiliary learning by implicit differentiation. ICLR, 2021.\n概 通过 implicit differentiation 优化一些敏感的参数.\n$$\r1 + 2f(x)\r\\phi \\ell_{main}\r$$\nAuxiLearn 在实际的训练中, 我们常常会通过一些额外的任务来帮助更好的训练. $$\r\\ell_{main}, \\bm{x}\r$$ 其中 $\\phi_k \\ge 0$ 是第 $k$ 个额外任务 $\\ell_k$ 的系数.\n$$\rw_{t+1} \\leftarrow \\arg \\min_{W} \\mathcal{L}_T(W; \\phi_t) \\phi_{t+1}\r$$ 但是很显然, 如果利用梯度下降学习 $\\phi_k$ 并通过 clip 保证 $\\phi_k \\ge 0$, 一定会导致 $\\phi_k \\equiv 0$ 这一平凡解.\n问题设定 现在让我们来设定一个更加一般的问题: $$\r\\ell_{main}(W; \\mathcal{D}_{train})\r$$ 其中 $W \\in \\mathbb{R}^n$ 是模型中的基本参数, $\\phi \\in \\mathbb{R}^m$ 是一些其它的超参数, 然后 $D_{train}, D_{aux}$ 表示训练集和额外的集合 (比如验证集).\n不考虑 mini-batch, 合理的训练流程应该是: 如此重复. 就能够避免 $\\phi$ 的平凡解.\n当然, 如果每一次都严格按照两阶段计算, 计算量是相当庞大的 (比 grid search 也是不遑多让). 本文所提出来的 AuxiLearn 的改进就是提出了一种近似方法. 它的理论基础是 Implicit Function Theorem (IFT).\n$$\r\\nabla_{\\phi} \\mathcal{L}_A = \\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{\\nabla_{\\phi} W^*}_{n \\times m}.\r$$ 显然, 其中 $\\nabla_W \\mathcal{L}_A$ 是好计算的, 问题在于 $\\nabla_{\\phi} W^*$ 的估计.\n$$\rF(p, q) = 0,\r$$$$\rF(x, \\Phi(x)) = 0\r$$ 在某个集合上均成立.\n$$\r\\nabla_W \\mathcal{L}_T(W^*, \\phi) = 0,\r$$$$\r\\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0,\r$$$$\r\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0 \\\\\r\\Rightarrow\r\\nabla_W^2 \\mathcal{L}_T \\cdot \\nabla_{\\phi} W^* + \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T = 0 \\\\\r\\Rightarrow\r\\nabla_{\\phi} W^* = - (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T.\r$$ $$\r\\nabla_{\\phi} \\mathcal{L}_A = -\\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{(\\nabla_W^2 \\mathcal{L}_T)^{-1}}_{n \\times n}) \\cdot \\underbrace{\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T}_{n \\times m}.\r$$ $$\r(I - X)^{-1} = \\sum_{t} X^t \\Rightarrow X^{-1} = (I - (I - X))^{-1} = \\sum_{t} (I - X)^t.\r$$ 于是便得到了本文 AuxiLearn 算法 (算法 2 其实就是 Neumann series 的前 $J$ 项):\n理解两阶段的训练 $$\r\\mathcal{L}_T(W, \\phi) = \\ell_{main}(W; \\mathcal{D}_{train}) + \\phi \\cdot \\ell_{aux}(W; \\mathcal{D}_{train}).\r$$ $$\r\\begin{array}{ll}\r\\frac{d \\mathcal{L}_A}{d \\phi}\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} (\\nabla_W \\mathcal{\\ell}_{main}(\\mathcal{D}_{train}) + \\phi \\nabla_W \\ell_{aux}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}). \\\\\r\\end{array}\r$$ $$\r\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \u003e 0,\r$$ 即当主任务在 aux 集合上的更新方向和辅任务在训练集上在 $\\nabla_W^2 \\mathcal{L}_T^{-1}$ 意义上方向一致.\n代码 [official-code]\n","permalink":"http://localhost:1313/posts/test1/","title":"Test1"},{"content":"[TOC]\nChoi J., Wang Z., Venkataramani S., Chuang P. I., Srinivasan V. and Gopalakrishnan K. PACT: Parameterized clipping activation for quantized neural networks. 2018.\n概 本文提出对网络中的激活值进行裁剪以实现更低量化.\n主要内容 $$\ry = PACT(x) = 0.5 (|x| - |x - \\alpha| + \\alpha)\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, 0), \\\\\rx, \u0026 x \\in [0, \\alpha), \\\\\r\\alpha, \u0026 x \\in [\\alpha, +\\infty).\r\\end{array}\r\\right .\r$$ $$\r\\frac{\\partial y_q}{\\partial \\alpha}\r=\\frac{\\partial y_q}{\\partial y}\r\\frac{\\partial y}{\\partial \\alpha}\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, \\alpha), \\\\\r1, \u0026 x \\in [\\alpha, +\\infty].\r\\end{array}\r\\right.,\r$$ 其中 $y_q = round(y \\cdot \\frac{2^k - 1}{\\alpha}) \\cdot \\frac{\\alpha}{2^k - 1}$.\n","permalink":"http://localhost:1313/posts/test2/","title":"Test2"},{"content":" Navon A., Achituve I., Maron H., Chechik G. and Fetaya E. Auxiliary learning by implicit differentiation. ICLR, 2021.\n概 通过 implicit differentiation 优化一些敏感的参数.\n$$\r1 + 2f(x)\r\\phi \\ell_{main}\r$$\nAuxiLearn 在实际的训练中, 我们常常会通过一些额外的任务来帮助更好的训练. $$\r\\ell_{main}, \\bm{x}\r$$ 其中 $\\phi_k \\ge 0$ 是第 $k$ 个额外任务 $\\ell_k$ 的系数.\n$$\rw_{t+1} \\leftarrow \\arg \\min_{W} \\mathcal{L}_T(W; \\phi_t) \\phi_{t+1}\r$$ 但是很显然, 如果利用梯度下降学习 $\\phi_k$ 并通过 clip 保证 $\\phi_k \\ge 0$, 一定会导致 $\\phi_k \\equiv 0$ 这一平凡解.\n问题设定 现在让我们来设定一个更加一般的问题: $$\r\\ell_{main}(W; \\mathcal{D}_{train})\r$$ 其中 $W \\in \\mathbb{R}^n$ 是模型中的基本参数, $\\phi \\in \\mathbb{R}^m$ 是一些其它的超参数, 然后 $D_{train}, D_{aux}$ 表示训练集和额外的集合 (比如验证集).\n不考虑 mini-batch, 合理的训练流程应该是: 如此重复. 就能够避免 $\\phi$ 的平凡解.\n当然, 如果每一次都严格按照两阶段计算, 计算量是相当庞大的 (比 grid search 也是不遑多让). 本文所提出来的 AuxiLearn 的改进就是提出了一种近似方法. 它的理论基础是 Implicit Function Theorem (IFT).\n$$\r\\nabla_{\\phi} \\mathcal{L}_A = \\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{\\nabla_{\\phi} W^*}_{n \\times m}.\r$$ 显然, 其中 $\\nabla_W \\mathcal{L}_A$ 是好计算的, 问题在于 $\\nabla_{\\phi} W^*$ 的估计.\n$$\rF(p, q) = 0,\r$$$$\rF(x, \\Phi(x)) = 0\r$$ 在某个集合上均成立.\n$$\r\\nabla_W \\mathcal{L}_T(W^*, \\phi) = 0,\r$$$$\r\\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0,\r$$$$\r\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T(W^*(\\phi), \\phi) = 0 \\\\\r\\Rightarrow\r\\nabla_W^2 \\mathcal{L}_T \\cdot \\nabla_{\\phi} W^* + \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T = 0 \\\\\r\\Rightarrow\r\\nabla_{\\phi} W^* = - (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T.\r$$ $$\r\\nabla_{\\phi} \\mathcal{L}_A = -\\underbrace{\\nabla_W \\mathcal{L}_A}_{1 \\times n} \\cdot \\underbrace{(\\nabla_W^2 \\mathcal{L}_T)^{-1}}_{n \\times n}) \\cdot \\underbrace{\\nabla_{\\phi} \\nabla_W \\mathcal{L}_T}_{n \\times m}.\r$$ $$\r(I - X)^{-1} = \\sum_{t} X^t \\Rightarrow X^{-1} = (I - (I - X))^{-1} = \\sum_{t} (I - X)^t.\r$$ 于是便得到了本文 AuxiLearn 算法 (算法 2 其实就是 Neumann series 的前 $J$ 项):\n理解两阶段的训练 $$\r\\mathcal{L}_T(W, \\phi) = \\ell_{main}(W; \\mathcal{D}_{train}) + \\phi \\cdot \\ell_{aux}(W; \\mathcal{D}_{train}).\r$$ $$\r\\begin{array}{ll}\r\\frac{d \\mathcal{L}_A}{d \\phi}\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} \\nabla_W \\mathcal{L}_T \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_{\\phi} (\\nabla_W \\mathcal{\\ell}_{main}(\\mathcal{D}_{train}) + \\phi \\nabla_W \\ell_{aux}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_A \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \\\\\r\u0026= -\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}). \\\\\r\\end{array}\r$$ $$\r\\nabla_W \\mathcal{L}_{main}(\\mathcal{D}_{aux}) \\cdot (\\nabla_W^2 \\mathcal{L}_T)^{-1} \\cdot \\nabla_W^T \\mathcal{\\ell}_{aux}(\\mathcal{D}_{train}) \u003e 0,\r$$ 即当主任务在 aux 集合上的更新方向和辅任务在训练集上在 $\\nabla_W^2 \\mathcal{L}_T^{-1}$ 意义上方向一致.\n代码 [official-code]\n","permalink":"http://localhost:1313/posts/test1/","title":"Test1"},{"content":"[TOC]\nChoi J., Wang Z., Venkataramani S., Chuang P. I., Srinivasan V. and Gopalakrishnan K. PACT: Parameterized clipping activation for quantized neural networks. 2018.\n概 本文提出对网络中的激活值进行裁剪以实现更低量化.\n主要内容 $$\ry = PACT(x) = 0.5 (|x| - |x - \\alpha| + \\alpha)\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, 0), \\\\\rx, \u0026 x \\in [0, \\alpha), \\\\\r\\alpha, \u0026 x \\in [\\alpha, +\\infty).\r\\end{array}\r\\right .\r$$ $$\r\\frac{\\partial y_q}{\\partial \\alpha}\r=\\frac{\\partial y_q}{\\partial y}\r\\frac{\\partial y}{\\partial \\alpha}\r=\\left \\{\r\\begin{array}{ll}\r0, \u0026 x \\in (-\\infty, \\alpha), \\\\\r1, \u0026 x \\in [\\alpha, +\\infty].\r\\end{array}\r\\right.,\r$$ 其中 $y_q = round(y \\cdot \\frac{2^k - 1}{\\alpha}) \\cdot \\frac{\\alpha}{2^k - 1}$.\n","permalink":"http://localhost:1313/posts/test2/","title":"Test2"}]