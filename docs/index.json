[{"content":"招聘布局 此次招聘位于交大学术活动中心二楼的一角, 并不显眼, 外头也缺少足够的引导标志. 约莫着有三四十家高校和事业单位分列五排, 蓝白的布告栏竖在一张张桌子之后, 颇有严正以待的架势. 进门开始是东北大学的场地, 紧接着是荆楚学院、江西铜业技术研究院、某某集团. 较为心仪的嘉兴大学坐落在第三排的中部, 围满了一批批人. 其对面是第四排的丽水学院, 而第四排的尾部是南京理工大学. 转过去, 赫然便是湖南大学四个大字, 穿行而过是成都大学、中国警察学院、扬州大学以及坐落在角落的之江实验室.\n聊天过程 称之为聊天过程而非是面试是因为一来本次主题是双选会, 二来本身也不指望能够凭借这场招聘会找到心仪的工作, 更多的是摸摸清楚自己到底几斤几两.\n江西铜业技术研究院 我上来盯着这家单位的布告栏看, 想看看这类研究院具体有个什么要求. 不过面前的小哥倒是很热情地邀请我坐下来聊一聊, 不过得知我对此并无兴趣之后我们就分道扬镳了.\n某医科单位 很抱歉忘记具体的名字了, 负责招聘的人也是非常热情. 但是我说我的方向和医学可能八竿子打不着. 虽然上面也明确需要人工智能方面的人才, 但是大抵是需要医学图像的? 招聘人员拿着我的简历, 跳过了前面部分, 竟然直接跳到了\u0026quot;技能和语言\u0026quot;一栏, 说着 Python, PyTorch 和我们的需求挺符合的. 我感到奇怪, 我觉得这些是无关紧要的东西, 不过想来也能理解, 来招聘的多是为整个单位招人.\n南阳理工学院 这个主要是跟在师妹边上旁听的, 只能说是大开眼界: 据招聘人来说, 他们学院可以非常痛快地给安家费, 然后每个月到手能到 13K (副教授待遇), 而且科研横向不收取管理费! 另外, 需要说明的是, 这 13K 如果后期考核不通过会降到 11K (但是招聘说这句话的时候明显咯噔了一些, 大概率存在猫腻).\n优点: 没有非升即走, 博士毕业点击就送, 安家费给的痛快, 考核要求很低 缺点: 这类学院我个人都对他们是否能够长期维持存在疑问, 另外过于痛快总让我感觉其中有猫腻 丽水学院 丽水学院的招聘老师人挺好, 也没啥架子. 比较有趣的是, 丽水学院里面设立了一个 \u0026ldquo;数学与计算机学院\u0026rdquo;, 我当时的第一个感觉就是这玩意儿可真有意思. 由于对地域不太感兴趣, 仅探听到了大约年薪十几万这个普遍的待遇条件.\n中国刑事警察学院 (沈阳) 两位穿着警服的大佬端坐在我面前. 但是双方好像都比较拘束, 没一会儿我就又站起来了, 得到了如下的信息:\n没有非升即走 虽然招聘人员说教课, 科研都有, 但是大概率是教课为主 嘉兴大学 兜兜转转终于是坐下了. 我实在是没有想到嘉兴大学会受到这般追捧, 桌上的招聘简章早早就被拿光了.\n工资年薪十几万, 每个月到手 7000 左右, 说实话感觉嘉兴大学的野心挺大的, 招聘里面我大抵只能够到最后一档的优秀博士里面. 不过看着安家费给的不少 (80 万), 但是但是, 我如果去嘉兴大学我还买个锤子房子 (但是不买房这笔钱就木得了).\n人工智能学院: 计算机科学与技术、智能科学与技术、网络空间安全 (陈院长 13819330328, chenbin@zjxu.edu.cn; 王老师, 18258330972, rgznxy@zjxu.edu.cn) 数据科学学院: 统计学、数学 (宁院长, 13516735266, nzj76@163.com; 艾老师, 0573-83640102, skxy@zjxu.edu.cn) 荆楚理工学院 我本来想去门口的东北大学碰碰的, 但是由于那边人太多了, 我只能在一旁干看着. 结果, 荆楚学院的招聘人员就直接热情地招呼我了, 即使我说不太感兴趣也无用. 荆楚学院的招聘人员是人工智能学院的院长, 我当时听到这个 title 的时候简直虎躯一震, 咋现在人工智能都火到这地步了吗, 计算机学院没有就直接开设人工智能学院了. 进一步了解后, 她居然说周院长是她师弟, 只能说无巧不成书了.\n优点: 说是能给到为 A 类博士之上的待遇 (75-85 万安家费 (含荆楚市人才津贴), 15-20 万科研启动费); 可以安排配偶工作, 如果配偶是硕士可以安排当讲师 缺点: 没有硕士点, 只能说发展潜力巨大啊 注: 荆楚理工学院人工智能学院的招聘公告上: 安家费 75 -\u0026gt; 45, 科研启动费: 20 -\u0026gt; 5, 这啥意思?\n成都大学 成都大学的老师过于傲慢了, 感觉压根没有把人放在眼里. 上来我递了简历, 他说需要把发表成果的分区写上, 我老老实实地写上. 看到我有些成果后态度才有所改观.\n特聘研究员: 35-50 万年薪, 80-100 万元 科研启动费, 80 万购房补贴 + 10 万安家费 特聘副研究员: 25-40 万年薪, 40-60 万元 科研启动费, 50 万购房补贴 + 8 万安家费 优秀青年博士: 基本工资, 10 万元 科研启动费, 15 万购房补贴 + 5 万安家费 我问了 \u0026ldquo;特聘副研究员\u0026rdquo; 的考核要求: 青基 + 5 年 10 篇 (一区?) + \u0026hellip; 完全不感兴趣了, 只能说打扰了.\n计算机学院, 洪老师, 028-84616938, computer_yb@cdu.edu.cn\n注: 最离谱的一件事, 成都大学居然没有数学学院, 全校的数学课程是由计算机学院的老师负责的. 我当时好感就去了大半, 我不认为高数现代能够满足需求.\n扬州大学 最后去扬州大学走了一遭, 感觉这个整体上还挺中规中矩的.\n有三篇一区的文章 (要求其中一篇是顶刊) 就能评青年百人, 年薪 35 万 (不记得是不是啥安家费算在里头了), 然后考核是三年三篇 (一区?). 数学科学学院: 书记, zhling@yzu.edu.cn, 0514-87975259; 主任, liyyoung@yzu.edu.cn, 0514-87975509\n信息工程学院: 院长, 0514-87978037, xbsun@yzu.edu.cn; 书记, 0514-87978309, lvhm@yzu.edu.cn\n总结 虽然网上说现在江浙沪的双非都得非升即走了, 但是面试的这几个除了成都大学外都没有非升即走的说法 (可能层次还是低了).\n\u0026lsquo;学院\u0026rsquo;的待遇普遍会比\u0026rsquo;大学\u0026rsquo;好很多, 后者能拿到 10K 应该可以烧高香了.\n虽然大部分院校没有给出具体界定 \u0026lsquo;人才\u0026rsquo; 的标准, 大体上有个 5 篇顶会顶刊应该就都有的谈了.\n以后简历的成果部分得把分区加上, 因为招聘的人并不一定熟悉.\n","permalink":"http://localhost:1313/posts/%E4%B8%AD%E7%A0%94%E6%98%A525%E6%8B%9B%E8%81%98/","title":"中研春招聘小记"},{"content":"预备知识 VQ-VAE 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\\text{sg}(\\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:\ncodebook 中的部分向量过于接近, 从而冗余; 很多向量在训练过程中完全不会匹配到任何向量. 习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.\n以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:\n[1] 中对会对那些长期不产生匹配的向量进行重新初始化; [2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性; [3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE. 核心思想 注意到, 一般的向量量化 (VQ) 需要一个显式的可训练的 codebook $\\mathcal{C} = \\{c_k\\}_{k=1}^K$, 然后给定一个隐变量 $z \\in \\mathbb{R}^d$, 通过\n$$\rz_q = \\text{argmin}_{c \\in \\mathcal{C}} \\|z - c\\|\r$$来进行一个量化.\n本文的不同之处在于, codebook 相当于是预设的好, 无需训练, 其形式为:\n$$\r\\mathcal{C} = \\{-\\lfloor L / 2 \\rfloor, -\\lfloor L / 2 \\rfloor + 1, \\ldots, 0, \\lfloor L / 2 \\rfloor - 1, \\lfloor L / 2 \\rfloor\\}^{d},\r$$这里 $L$ 是一个超参数, 他直接决定了 CodeBook 的大小:\n$$\r|\\mathcal{C}| = (2 \\lfloor L / 2 \\rfloor + 1)^d.\r$$ 例子: 当 $L=3, d=3$ 的时候, 我们有\n$$\r\\mathcal{C} = \\{\r(-1, -1, -1),\r(-1, -1, 0),\r\\ldots,\r(1, 1, 1)\r\\}.\r$$ 显然这种不需要训练的 codebook 至少不存在 collapse 中的第一个问题, 实际上它均匀地分布在超立方体之上:\n当然了, 第二个问题可能还是存在的, 因此 FSQ 还引入了一个 bounfding function $f$, 它将 $z$ 每个元素的值\u0026rsquo;压缩\u0026rsquo;到 $[-L/2, L/2]$ 之中去, 比如\n$$\rf: z \\rightarrow \\lfloor L / 2 \\rfloor \\tanh (z).\r$$ 特别地, FSQ 的量化可以以一种非常简便的方式实现, 无需一一计算距离:\n$$\rz_q = \\text{round}(f(z)).\r$$ 其它和普通的 VQ 并没有特别大的区别.\n注: $L$ 不一定需要每个维度相同, 可以每个维度单独设置.\n参考文献 Dhariwal P., Jun H., Payne C., Kim J. W., Radford A. and Sutskever I.\rJukebox: A Generative Model for Music.\rarXiv, 2020.\r[PDF]\r[Code]\rLancucki A., Chorowski J., Sanchez G., Marxer R., Chen N., Dolfing H. J.G.A., Khurana S., Alumae T. and Laurent A.\rRobust Training of Vector Quantized Bottleneck Models.\rarXiv, 2020.\r[PDF]\r[Code]\rTakida Y., Shibuya T., Liao W., Lai C., Ohmura J., Uesaka T., Murata N., Takahashi S., Kumakura T. and Mitsufuji Y.\rSQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization.\rICML, 2022.\r[PDF]\r[Code]\rMentzer F., Minnen D., Agustsson E. and Tschannen M.\rFinite Scalar Quantization: VQ-VAE Made Simple.\rarXiv, 2023.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/fsq/","title":"Finite Scalar Quantization: VQ-VAE Made Simple"},{"content":"预备知识 在学习 VQGAN 之前, 请务必先了解 VQ-VAE. 核心思想 Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力. Part1: 离散编码 既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.\n给定一个图片 $x \\in \\mathbb{R}^{H \\times W \\times 3}$, 首先通过一个 CNN encoder $E$ 来得到初步的编码:\n$$\r\\hat{z} = E(x) \\in \\mathbb{R}^{h \\times w \\times n_z}.\r$$ 接着, element-wise 地为每一个\u0026rsquo;像素点\u0026rsquo;匹配它的 token:\n$$\rz_{\\mathbf{q}} = \\mathbf{q}(\\hat{z}) := \\bigg(\\text{argmin}_{z_k \\in \\mathcal{Z}} \\|\\hat{z}_{ij} - z_k\\| \\bigg)_{ij},\r$$这里 $\\mathcal{Z} = \\{z_k\\}_{k=1}^K \\subset \\mathbb{R}^{n_z}$, 俗称 codebook.\n通过 CNN decoder, 我们可以还原出对应的图片:\n$$\r\\hat{x} = G(z_{\\mathbf{q}}).\r$$ 当然, 我们需要训练这个模型, 任务目标和 VQ-VAE 的略有不同:\n$$\r\\mathcal{L} = \\mathcal{L}_{VQ} + \\lambda \\mathcal{L}_{GAN}, \\\\\r\\mathcal{L}_{VQ} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\mathcal{L}_{rec}} + \\|\\text{sg}(E(x)) - z_{\\mathbf{q}} \\|^2 + \\|\\text{sg}(z_{\\mathbf{q}}) - E(x) \\|^2, \\\\\r\\mathcal{L}_{GAN} = \\log D(x) + \\log (1 - D(\\hat{x})).\r$$这里 $\\text{sg}(\\cdot)$ 表示梯度截断, $D(\\cdot)$ 则是 GAN 里面常用的判别器. $\\lambda$ 是一个自适应的超参数:\n$$\r\\lambda = \\frac{\\nabla_{G_L} [\\mathcal{L}_{rec}]}{\\nabla_{G_L} \\mathcal{L}_{GAN} + \\delta},\r$$这里 $\\delta = 1e-6$ 是一个小量防止数值不稳定.\n特别地, 文中有一句话:\nTo do so, we propose VQGAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good perceptual quality at increased compression rate.\n因此, 实际使用的时候, $\\mathcal{L}_{VQ}$ 中的 $\\mathcal{L}_{rec}$ 应当替换为 perceptual loss. Part2: Transformer 生成 通过上面我们就有了 $z_{\\mathbf{q}}$, 它实际上可以表示为离散的 token:\n$$\rs \\in \\{0, \\ldots, |\\mathcal{Z}| - 1\\}^{h \\times w}, \\quad s_{ij} = k \\text{ such that } (z_{\\mathbf{q}})_{ij} = z_k.\r$$ 因此, 我们可以把这些当成\u0026rsquo;文本\u0026rsquo;然后像一般的 NLP 那样进行 next-token predication:\n$$\rp(s|c) = \\prod_{i} p(s_i | s_{\u003c i}, c),\r$$这里 $c$ 是一些条件 (可以是文本, 也可以是图像).\n由 Transformer 预测出来的 tokens 收集起来经过 decoder $G$ 就可以得到\u0026rsquo;操作\u0026rsquo;过后的图像了. 当然了, Transformer 需要其它的方式训练. 现在这种方式已经被广泛应用于图像生成了 (如, Diffusion). 不过 Diffusion 里面采用 VQGAN 的流程主要是由于它的高效性.\n参考文献 Esser P., Rombach R. and Ommer B.\rTaming Transformers for High-Resolution Image Synthesis.\rCVPR, 2021.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqgan/","title":"Taming Transformers for High-Resolution Image Synthesis"},{"content":"预备知识 作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.\n既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:\nEncoder $\\phi$: 它讲输入 $x \\in \\mathbb{R}^D$ 映射到一个分布:\n$$\rq(z|x; \\phi).\r$$比如当服从的高斯分布, 实质上 $\\phi(x) \\rightarrow (\\mu, \\sigma) \\rightarrow \\mathcal{N}(\\mu, \\sigma^2)$, 然后 $z$ 从该分布中采样即可;\nDecoder $\\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:\n$$\rp(x|z; \\Phi);\r$$ 还有一个先验分布 $p(z)$ 用于辅助训练.\nVAE 的训练目标是极大似然的一个下界:\n$$\r\\begin{align*}\r\\log p(x) \u0026= \\log \\int p(x, z) \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x, z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\log \\int q(z|x; \\phi) \\cdot \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026\\ge \\int q(z|x; \\phi) \\log \\frac{p(x| z; \\Phi) p(z)}{q(z|x; \\phi)} \\mathrm{d}z \\\\\r\u0026= \\int q(z|x; \\phi) \\log \\frac{p(z)}{q(z|x; \\phi)} \\mathrm{d}z +\r\\int q(z|x; \\phi) \\log p(x|z; \\Phi) \\mathrm{d}z \\\\\r\u0026= \\underbrace{-\\mathbf{KL}(q_{\\phi}\\| p(z)) +\r\\mathbb{E}_{z \\sim q_{\\phi}} \\log p(x|z; \\Phi)}_{\\text{ELBO}}.\r\\end{align*}\r$$ ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\\Phi}$ 也是一个高斯, 则通常称之为重构损失).\n核心思想 VQ-VAE 的希望 $z$ 不再局限于连续的向量, 而是离散的值, 做法其实极为简单:\n预设一个 codebook $E \\in \\mathbb{R}^{K \\times d}$;\n给定一个输入 $x$, 其对应的离散值为\n$$\rx \\rightarrow \\phi(x) \\rightarrow \\text{argmin}_{k} \\|\\phi(x) - e_k\\|,\r$$其中 $e_k$ 表示 codebook $E$ 中 $k$-th 行.\n接下来, decoder 部分的输入将是 $e_{k^*}$ 而不再是 $z$ 了.\n容易发现, 这其实相当于我们的后验分布为:\n$$\rq(z = e_{k^*}|x; \\phi) =\r\\left \\{\r\\begin{array}{ll}\r1 \u0026 k^* = \\text{argmin}_{k} \\|\\phi(x) - e_k\\|, \\\\\r0 \u0026 otherwise.\r\\end{array}\r\\right .\r$$ 但是这里其实有一个大问题, $\\phi$ 的训练梯度来源:\nKL 散度, 但是上述的概率实际上的 \u0026lsquo;固定\u0026rsquo; 的, 没法提供额外的信息; 交叉熵, 由于我们用 $e_{k^*}$ 替代了, 导致梯度没法直接计算. 对于第二点, 作者建议采取 straight-through estimator, 另外设计了另外两个损失用于训练 $\\phi$ 以及 codebook $E$:\n$$\rL = \\log p(x|z_q; \\Phi) + \\| \\text{sg} (\\phi(x)) - e_{k^*}\\|_2^2 +\r\\beta \\cdot \\| \\phi(x) - \\text{sg} (e_{k^*})\\|_2^2.\r$$这里 $\\text{sg}(\\cdot)$ 表示 stop-gradient 操作, $\\beta$ 是超参数 (默认为 0.25).\n注: straight-through estimator (STE):\nz_q = z + (z_q - z).detach() 参考文献 van den Oord A., Vinyals O. and Kavukcuoglu K.\rNeural Discrete Representation Learning.\rNeurIPS, 2017.\r[PDF]\r[Code]\r","permalink":"http://localhost:1313/posts/vqvae/","title":"Neural Discrete Representation Learning"},{"content":"\r## Pushing the Limits of Low-Bit Optimizers with a Focus on EMA Dynamics\r\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e模型大小飞速增加 vs. 硬件价格居高不下\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312203012.png\" alt=\"Image\" style=\"max-width: 65%; height: auto; margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cul\u003e \u003cli\u003e解决方案: \u003cul\u003e \u003cli\u003eMoE, LoRA; ZeRO, FDSP;\u003c/li\u003e \u003cli\u003eNetwork Quantization; \u003cspan style=\"color: red;\"\u003eLightweight Optimizers\u003c/span\u003e\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003ch3 id=\"background-1\"\u003eBackground\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eOptimizer States (2x model size):\n$$\rm_{t+1} \\leftarrow \\beta_1 \\cdot m_t + (1 - \\beta_1) \\cdot g, \\\\\rv_{t+1} \\leftarrow \\beta_2 \\cdot v_t + (1 - \\beta_2) \\cdot g^2.\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003eDeepSeek-v3 训练框架: $g \\overset{\\text{BF16}}{\\rightarrow} m, v \\overset{\\text{FP32}}{\\rightarrow} \\theta$\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312204230.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"quantization-and-dequantization\"\u003eQuantization and Dequantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eQuantization:\n$$\rq = Q(x) := \\mathop{\\text{argmin}} \\limits_{k=0}^{2^b - 1} \\big|\\frac{x}{\\Delta} - \\iota_k \\big|.\r$$\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312205652.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003eDequantization:\n$$\r\\tilde{x} = Q^{\\dagger}(q) := \\iota_{q} \\cdot \\Delta.\r$$\n\u003ch3 id=\"stateful-optimizers-with-low-bit-states\"\u003e\u003cu\u003eS\u003c/u\u003etateful \u003cu\u003eO\u003c/u\u003eptimizers with \u003cu\u003eLO\u003c/u\u003ew-Bit States\u003c/h3\u003e \u003cul\u003e \u003cli\u003eLow-Bitwidth EMA update:\u003c/li\u003e \u003c/ul\u003e $$\r\\begin{array}{rl}\r\\text{Dequantization: } \u0026 \\tilde{x}_t = Q^{\\dagger}(q_t) = \\iota_{q_t} \\cdot \\Delta_t, \\\\\r\\text{EMA update: } \u0026 \\hat{x}_{t+1} \\leftarrow \\beta \\cdot \\tilde{x}_t + (1 - \\beta) \\cdot z_{t + 1}, \\\\\r\\text{Quantization: } \u0026 q_{t+1} = Q(\\hat{x}_{t+1}).\r\\end{array}\r$$\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eDettmers T., et al. 8-bit Optimizers via Block-wise Quantization. ICLR, 2022.\n\u003cp style=\"margin: 2px 0;\"\u003eLi B., et al. Memory Efficient Optimizers with 4-bit States. NeurIPS, 2023.\n\u003ch3 id=\"quantization-for-unsigned-ema-update\"\u003eQuantization for Unsigned EMA Update\u003c/h3\u003e \u003cul\u003e \u003cli\u003e\u003cem\u003eSignal Swamping\u003c/em\u003e\u003c/li\u003e \u003c/ul\u003e \u003cdiv style=\"text-align: center; margin-top: 50px; margin-bottom: -80px; padding: 0\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312211840.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-ref\"\u003e\r\u003cdiv style=\"width: 100px; height: 1px; background: black; margin-bottom: 5px;\"\u003e\u003c/div\u003e\r\u003cp style=\"margin: 2px 0;\"\u003eHigham N. J. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing, 1993.\n\u003ch3 id=\"signal-swamping\"\u003eSignal Swamping\u003c/h3\u003e \u003cul\u003e \u003cli\u003e总结\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312212039.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"case-study\"\u003eCase Study\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312212821.png\" alt=\"Image\" style=\"max-width: 90%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003e满足一定情况:\n\u003cul\u003e \u003cli\u003eLinear 下全部不更新\u003c/li\u003e \u003cli\u003eDE 下部分更新\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003e \u003cp\u003e$\\beta \\ge \\cdots$ 条件很容易满足\n\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e\r\u003ch3 id=\"case-study-1\"\u003eCase Study\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cul\u003e \u003cli\u003e \u003cp\u003e$X \\in \\mathbb{R}^{1000}$\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e$Z \\sim \\mathcal{U}[0, 1]$\n\u003c/li\u003e \u003cli\u003e \u003cp\u003e理想的值: 0.5\n\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312213810.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003ch3 id=\"stochastic-rounding\"\u003eStochastic Rounding\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003e假设 $\\iota_{k-1} \\le x / \\Delta \\le \\iota_k$:\n$$\rQ_{sr}(x) :=\r\\left \\{\r\\begin{array}{ll}\rk-1 \u0026 w.p. \\quad \\frac{\\iota_k - x / \\Delta}{ \\iota_k - \\iota_{k-1}}, \\\\\rk \u0026 w.p. \\quad \\frac{x / \\Delta - \\iota_{k-1}}{ \\iota_k - \\iota_{k-1}}.\r\\end{array}\r\\right .\r$$\u003c/li\u003e \u003cli\u003e \u003cp\u003eHigh variance:\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313112908.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization\"\u003eLogarithmic Quantization\u003c/h3\u003e $$\r\\begin{array}{ll}\rQ(x) \u0026=\\text{Clip}(\\lfloor \\log_{\\alpha} \\frac{x}{\\Delta} + \\xi \\rceil; 0, 2^b - 1) \\\\\r\u0026\\approx \\mathop{argmin} \\limits_{k=0}^{2^b - 1} \\big|\\frac{x}{\\Delta} \\cdot \\alpha^\\xi - \\iota_k \\big|,\r\\end{array}\r$$\u003cul\u003e \u003cli\u003e3bit quantization anchors:\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313113440.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization-1\"\u003eLogarithmic Quantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e2-bit quantization illustration\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313113535.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"logarithmic-quantization-2\"\u003eLogarithmic Quantization\u003c/h3\u003e \u003cul\u003e \u003cli\u003e \u003cp\u003eEasy to implement\n\u003c/li\u003e \u003cli\u003e \u003cp\u003eState Decay Alignment\n\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250313115306.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"quantization-for-signed-ema-update\"\u003eQuantization for Signed EMA Update\u003c/h3\u003e \u003cp\u003e\u003cstrong\u003eX\u003c/strong\u003e Singal Swamping\n\u003cp\u003e\u003cstrong\u003e✓\u003c/strong\u003e Sign representation\n\u003cp\u003e\u003cstrong\u003e✓\u003c/strong\u003e Descent direction\n\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314115701.png\" alt=\"Image\" style=\"max-width: 80%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"theoretical-analysis\"\u003eTheoretical Analysis\u003c/h3\u003e \u003cdiv class=\"slide-cols\"\u003e\r\u003cdiv class=\"slide-col-6\"\u003e\r\u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314115959.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003c/div\u003e\r\u003c/div\u003e\r\u003cdiv class=\"slide-col-4\"\u003e\r\u003cp\u003e-\u0026gt; \u003cspan style=\"color: red\"\u003eLow bitwidth\u003c/span\u003e or \u003cspan style=\"color: red\"\u003e$\\beta \\uparrow$ \u003c/span\u003e\n\u003cp\u003e-\u0026gt; Quantization errors \u003cspan style=\"color: red\"\u003e$\\uparrow$\u003c/span\u003e\n\u003cp\u003e-\u0026gt; gradient variance \u003cspan style=\"color: red\"\u003e $\\uparrow$ \u003c/span\u003e\n\u003cp\u003e-\u0026gt; \u003cspan style=\"color: red\"\u003e bad \u003c/span\u003e convergence\n\u003c/div\u003e\r\u003ch3 id=\"momentum-adjustment\"\u003eMomentum Adjustment\u003c/h3\u003e \u003cul\u003e \u003cli\u003e方差控制, 给定 $b$ bitwidth 要求选择 $\\beta'$ 满足:\u003c/li\u003e \u003c/ul\u003e $$\r\\frac{\\beta'}{1 - \\beta'} r_{\\text{median}}(b')\r\\le \\frac{\\beta}{1 - \\beta} r_{\\text{median}}(b).\r$$\u003cul\u003e \u003cli\u003e查表:\u003c/li\u003e \u003c/ul\u003e \u003cdiv class=\"slide-img\"\u003e\r\u003cimg src=\"https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250314121510.png\" alt=\"Image\" style=\"max-width: 95%; height: auto;margin: 0 auto;\"\u003e\r\u003ch3 id=\"experiments\"\u003eExperiments\r","permalink":"http://localhost:1313/slides/solo/","title":"SOLO"},{"content":" 廖雪峰Git教程\n初始化 在你想要git的文件夹内 git bash here\n接着注册\ngit config --global user.name \u0026#34;XXXXXX\u0026#34;\rgit config --global user.email \u0026#34;XXX@+++.com\u0026#34; 配置别名\ngit config --global alias.last \u0026#39;log -1\u0026#39;\rgit config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; 上面的步骤是第一次使用git, 若不是可省略\n将所在目录变成git可以管理的仓库\ngit init 在所在目录添加 .gitignore 文件, 一般可以直接在这儿选择所需要的就行, 特殊情况可以自己再加点定制\ngit add .gitignore\rgit commit -m \u0026#34;add .gitignore\u0026#34; 远程仓库 创建ssh key\nssh-keygen -t rsa -C \u0026#34;xxx@+++.com\u0026#34; 然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.\n在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key\n在GitHub上新建repo, 并复制其ssh\n执行\ngit remote add origin ssh 将本地的内容推送到远程库上\ngit push -u origin master 分支管理 创建分支\ngit branch dev 或者(下面都是创建并移动至)\ngit switch -c dev 或者\ngit checkout -b dev 通过\ngit branch 查看当前的分支情况\n通过\ngit switch master 切换至master主分支\n合并分支\ngit merge dev 删除分支\ngit branch -d dev 多人协作 联系之前远程仓库的内容, 通过\ngit remote\rgit remote -v 来查看当前的远程仓库的信息.\n推送\ngit push origin master\rgit push origin dev 拷贝clone 这部分算是第二步, 模拟另外一个地方从头开始工作的情形.\n在某个目录下抓取\ngit clone ssh 查看分支\ngit branch 此时只有 master\n获得dev分支\ngit checkout -b dev origin/dev 然后在dev上进行操作, 并提交修改\n解决冲突 这个即为第三步\n首先如果直接提交本地的修改会出错, 因为版本不一致, 需要先抓取最新的提交\ngit pull 但是此时也不行, 因为当前有俩个分支, 所以需要声名抓的是哪一个\ngit branch --set-upstream-to=origin/\u0026lt;branch\u0026gt; dev 我们这里就是\ngit branch --set-upstream-to=origin/dev dev 如果是在master上进行操作:\ngit branch --set-upstream-to=origin/master master 然后再\ngit pull 解决冲突, 会在文件中出现change, 得选择是否接受change\n提交修改\ngit push origin dev 标签 给某个commit打上标签\ngit tag v1.0 此时给最新的commit打上标签, 也可以\ngit tag v1.0 ef2a5d7 更具体的\ngit tag -a v1.0 -m \u0026#34;version 1.0\u0026#34; ef2a5d7 通过\ngit show v1.0 来查看对应的标签信息\n删除标签\ngit tag -d 另外:\n推送某个标签到远程\ngit push origin v1.0 一次性推送全部尚未推送到远程的本地标签\ngit push origin -tags 删除远程标签\n首先删除本地标签\ngit tag -d v1.0 然后从远程删除\ngit push origin :refs/tags/v1.0 版本回退 git reset git reset --hard HEAD^ 回退到上一版本, HEAD^^就是上上一版本, HEAD~100就是往上100个版本.\ngit reset --hard GPL GPL就是库的那一堆16位\ngit reset HEAD filename 把暂存区的修改撤销, 重新放回工作区, 或者用\ngit restore --staged filename git revert 类似于reset, 只是在\u0026quot;回退“版本的时候, 前面的版本信息不会丢失, 即\n​\tA -\u0026gt; B -\u0026gt; C\n现在想要回到B, reset后为\n​\tA -\u0026gt; B\nrevert后为\n​\tA -\u0026gt; B -\u0026gt; C -\u0026gt; B\n","permalink":"http://localhost:1313/posts/git/","title":"Git"}]