<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MTandHJ</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on MTandHJ</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scaling Laws for Online Advertisement Retrieval</title>
      <link>http://localhost:1313/posts/advertisement-scaling-law/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/advertisement-scaling-law/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请先了解 &lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34;&gt;NLP 中的 Scaling Law&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;作者团队希望验证一下在广告场景下是否也有类似于 NLP 中的 scaling law, 即探究是否随着广告预测模型地&lt;strong&gt;增大&lt;/strong&gt;, 是否能够&lt;strong&gt;有规律&lt;/strong&gt;地提升一些&lt;strong&gt;线上指标&lt;/strong&gt;. (因为关注的是实际的线上指标, 这也衍生出了一些独特的问题, 这里就不讲了).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250421171053.png&#34; alt=&#34;20250421171053&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;注意到, 实际的推荐系统通常是包含多个阶段, 每个阶段可能还包含不同指标导向的模型, 因此相当复杂. 为了探究 scaling law, 作者团队主要针对 Pre-ranking 阶段探究一个排序模型 (MLPs):&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征:&lt;/strong&gt; 同时包括 sparse 和 dense features, 对于 dense features 应用 &lt;a href=&#34;https://openreview.net/pdf?id=Ut1vF_q_vC&#34;&gt;log1p transformation&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型:&lt;/strong&gt; 5-layer 的 MLPs, 每一层包括一个 batch normalization, linear mapping 和 PReLU. 通过 He initialization 初始化权重.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250421171806.png&#34; alt=&#34;20250421171806&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如上图所示, FLOPs 和作者设定的指标 R/R* 随着 MLPs 变大所产生的变化情况, 可以通过 Broken Neural Scaling Law (BNSL) 的公式很好的拟合.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;&#xA;&lt;ol class=&#34;reference&#34;&gt;&#xD;&#xA;  &lt;li&gt;&#xD;&#xA;    Wang Y., Yang Z., Zhang Z., Wang Z., Yang J.,&#xD;&#xA;    Wen S., Jiang P., and Gai K.&#xD;&#xA;    &lt;u&gt;Scaling Laws for Online Advertisement Retrieval.&lt;/u&gt;&#xD;&#xA;    &lt;i&gt;arXiv&lt;/i&gt;, 2024.&#xD;&#xA;    &lt;a href=&#34;http://arxiv.org/abs/2411.13322&#34; style=&#34;color: #007acc; font-weight: bold; text-decoration: none;&#34;&gt;[PDF]&lt;/a&gt;&#xD;&#xA;    &lt;a href=&#34;&#34; style=&#34;color: #007acc; font-weight: bold; text-decoration: none;&#34;&gt;[Code]&lt;/a&gt;&#xD;&#xA;  &lt;/li&gt;&#xD;&#xA;  &lt;!-- 添加更多文献条目 --&gt;&#xD;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>EMR-MERGING: Tuning-Free High-Performance Model Merging</title>
      <link>http://localhost:1313/posts/emr-merging/</link>
      <pubDate>Fri, 18 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/emr-merging/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;了解基本 pretrain-finetune 范式.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;预训练模型权重: $W_{pre} \in \mathbb{R}^d$;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;微调后的模型权重: $W_i, i=1,2,\ldots, N$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;现阶段, pretrain-finetune 最为主流的范式: 即为了推广到一个新的任务上, 我们通常是在一个预训练好的模型上进行微调 (可以是全量微调, 也可以是通过如 分类头, pre-tuning, LORA 等方式进行微调).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;现在的问题是, 比如我们可能有许许多多的子任务, 然后我们必须为每个子任务都存一份权重, 即使权重相较于预训练模型的权重会小很多, 这依旧是一个较为麻烦的事情 (其实个人感觉还好, 但是故事是这般讲的).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;假设我们有 $N$ 个任务, 分别微调得到了微调后的权重 $W_i, i=1,2,\ldots, N$. 一种很自然的方式是通过&lt;strong&gt;平均&lt;/strong&gt;的方式进行&lt;strong&gt;权重融合&lt;/strong&gt;:&lt;/p&gt;&#xA;$$&#xD;&#xA;    W_M = \mathcal{M}([W_1,\ldots, W_N]).&#xD;&#xA;    $$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250418152012.png&#34; alt=&#34;20250418152012&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;但是如上图所示, 这个结果其实非常糟糕, 当然了, 作者也比较其它的融合方法, 大抵上和单独微调的模型结果相去甚远, 离多任务训练的模型也有不小的差距.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;EMR-MERGING 另辟蹊径, 融合之后返回&lt;/p&gt;&#xA;$$&#xD;&#xA;    W_{uni}, [E_1, \ldots, E_N] = \mathcal{M}&#39;([W_1, \ldots, W_N]).&#xD;&#xA;    $$&lt;p&gt;这里 $W_{uni}$ 是所有任务共享的模型权重, $[E_1, \ldots, E_N]$ 则是每个任务单独的一组参数, 它包含一个非常轻量化的 mask 矩阵以及一个 scale factor. 示意图如下:&lt;/p&gt;</description>
    </item>
    <item>
      <title>OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment</title>
      <link>http://localhost:1313/posts/onerec/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/onerec/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请了解 &lt;a href=&#34;https://www.mtandhj.com/posts/qarm/&#34;&gt;QARM&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{H}_u = \{v_1^h, v_2^h, \ldots, v_n^h\}$, user historical behavior sequence, 在快手的场景下, $v$ 表示一个视频.&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{S} = \{v_1, v_2, \ldots, v_m\}$, 推荐的一串视频流.&lt;/li&gt;&#xA;&lt;li&gt;session watch time (&lt;strong&gt;swt&lt;/strong&gt;), view probability (&lt;strong&gt;vtr&lt;/strong&gt;), follow probability (&lt;strong&gt;wtr&lt;/strong&gt;), like probability (&lt;strong&gt;ltr&lt;/strong&gt;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250415142401.png&#34; alt=&#34;20250415142401&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;我们知道, 在工业界推荐系统有着一套复杂的流程: 粗排-精排. 这一套流程被广泛应用有着不同的原因:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;庞大的商品数量: 由于精排通常是 pair-wise 的比较 (因为需要利用交叉特征), 所以必须通过一步步粗排来减少候选商品的数量以保证有限的开销;&lt;/li&gt;&#xA;&lt;li&gt;多样化的推荐策略: 在工业场景中, 推荐的目标远非&amp;quot;精度&amp;quot;, 实际上还要考虑比如多样性等指标以保证用户的留存以及各种品类的商品具有最低限度的曝光度, 此外, 还需要考虑比如广告的因素.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;然而, 由于生成式推荐的发展, 第一个问题其实已经可以迎刃而解了, 这促使我们探索端到端推荐的可能性.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250415143509.png&#34; alt=&#34;20250415143509&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OneRec 的基本流程如下:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对每个 video $v_i$, 通过 &lt;a href=&#34;https://www.mtandhj.com/posts/qarm/&#34;&gt;QARM&lt;/a&gt; 中的操作得到其所对应的多模态 embedding $\bm{e} \in \mathbb{R}^d$;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$\mathbf{e}_i$ 通过类似 &lt;a href=&#34;https://www.mtandhj.com/posts/qarm/&#34;&gt;QARM&lt;/a&gt; 的 fixed RQ-VAE 进行残差量化, 得到其离散化表示&lt;/p&gt;</description>
    </item>
    <item>
      <title>QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou</title>
      <link>http://localhost:1313/posts/qarm/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/qarm/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请了解 &lt;a href=&#34;https://www.mtandhj.com/posts/rqvae/&#34;&gt;RQ-VAE&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250415120028.png&#34; alt=&#34;20250415120028&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;我们知道, 多模态推荐主要涉及:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \text{text/image} \longrightarrow \text{Encoder} \longrightarrow \text{Embedding} \longrightarrow \text{Recommender}&#xD;&#xA;    $$&lt;p&gt;的过程, 且通常 $\text{Encoder}$ 是通过 CV, NLP 任务预训练好后固定下来的.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;作者认为这种方式缺少了推荐任务的约束, 且通常用作 ID embedding 的补充而不是替代. 于是本文就希望:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过推荐任务微调 Encoder;&lt;/li&gt;&#xA;&lt;li&gt;通过向量量化来替代 ID.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;item-alignment-of-qarm&#34;&gt;Item Alignment of QARM&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;QARM 首先通过 Item-Item 的匹配任务来约束 Encoder, 即 Item 和它在推荐上&amp;rsquo;相似&amp;rsquo;的 Item 靠近, &amp;lsquo;不相似&amp;rsquo;的远离. 这里的相似 Item 通过如下的两种方式得到:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过过往的 &lt;strong&gt;User2Item&lt;/strong&gt; 检索模型: 为每个用户所 positive clicked target item 选择最相似的 Item;&lt;/li&gt;&#xA;&lt;li&gt;通过过往的 &lt;strong&gt;Item2Item&lt;/strong&gt; 检索模型: 为每个 item 选择高相似度的 Item.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;得到高相似度的 Item pairs 之后, 通过对比学习进行训练:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Language Representations Can be What Recommenders Need: Findings and Potentials</title>
      <link>http://localhost:1313/posts/alpharec/</link>
      <pubDate>Sat, 05 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/alpharec/</guid>
      <description>&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250323131310.png&#34; alt=&#34;20250323131310&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;linear&#34;&gt;Linear&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;本文探索 LLM embeddings 的潜力, 方法极为简单:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;将 title 的 embeddings 作为对应 item 的表示 (记为 $\bm{z}_i$), 其过程如下 (注意到, 实际上, 是将 decoder-only 的 next-token embedding 作为 item 的表示, 而不是平均之类的方式):&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# model_path = &amp;#34;meta-llama/Meta-Llama-3-8B&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;meta-llama/Llama-2-7b-hf&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# model_path = &amp;#34;meta-llama/Llama-2-13b-hf&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoTokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(model_path, device_map &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoModelForCausalLM&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(model_path, device_map &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;padding_side &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;left&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pad_token &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;eos_token&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;item_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;raw_data/items_filtered.csv&amp;#39;&lt;/span&gt;, index_col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;item_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rename(columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;item_name&amp;#39;&lt;/span&gt;}, inplace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(item_df), batch_size)):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# print(i)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    item_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; item_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;item_name&amp;#39;&lt;/span&gt;][i:i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;batch_size]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 生成输出&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer(item_names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tolist(), return_tensors&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pt&amp;#34;&lt;/span&gt;, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, truncation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, max_length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;device)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;no_grad():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;inputs, output_hidden_states&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    seq_embeds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hidden_states[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][:, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, :]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;detach()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        item_llama_embeds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; seq_embeds&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        item_llama_embeds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concatenate((item_llama_embeds, seq_embeds), axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;然后 user 表示为其所交互过的商品的平均:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
      <link>http://localhost:1313/posts/rearec/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rearec/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\mathcal{U}$, user set, $|\mathcal{U}| = M$;&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{V}$, item set, $|\mathcal{V}| = N$;&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{S}^u = [v_1^u, v_2^u, \ldots, v_{n_u}^u]$, 用户 $u$ 的交互序列.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250403212901.png&#34; alt=&#34;20250403212901&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;一般的序列推荐模型 (e.g., SASRec), 对应位置的输入 embedding 用于预测 next-item (如上图 (a) 所示).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;在 LLM 领域, CoT (chain-of-thought) 已经被证明在各领域上性能提高的优势了. 实际上, CoT 实际上是促使模型进行多次推理以获得更为准确可靠的结果. 那么类似的思想能不能推广到推荐呢? 这衍生了本文的 ReaRec.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250403213425.png&#34; alt=&#34;20250403213425&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;假设第 $i$ 个位置的输入是 $\mathbf{h}_i^0$, 故整个序列的输入为:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \mathbf{H}^{0} = [\mathbf{h}_1^0, \mathbf{h}_2^0, \ldots, \mathbf{h}_n^0] \in \mathbb{R}^{n \times d}.&#xD;&#xA;    $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;经过 $L$ 个 transformer blocks 之后, 我们可以得到输出特征:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction</title>
      <link>http://localhost:1313/posts/knowledge-storage-and-extraction/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/knowledge-storage-and-extraction/</guid>
      <description>&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;LLM 已经惊艳了所有人, 尤其是它广博的知识面, 几乎可以说是博古通今 (当然了, 有幻觉问题). 所以, 一个很自然的问题是, LLM 存储和提取知识的机制是怎么样的呢? 虽然已经有一些工作在现有的 LLM 的基础上进行探索, 但是并没有严格控制变量, 导致其得出的结论并不那么严谨. 比如询问 &amp;ldquo;高斯的出生日期?&amp;rdquo;, LLM 得到的答案可能来自两种: 1. 记忆了 wikipedia 等知识库并从中抽取; 2. 训练语料里中恰好有这个问题, 从而能够很好地回答.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;为了避免上述第二种情况引发的一个干扰, 作者人为构造一些数据集, 并从头训练以严格控制变量.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;&#xA;&lt;h4 id=&#34;数据集&#34;&gt;数据集&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;bioS:&lt;/strong&gt; 从 $N=100,000$ 个体中随机生成 profiles: 每个个体的出生日期, 出生的城市, 毕业院校, 就职公司, 工作城市等独立随机生成. 每个个体的 full name 是独一无二的. 如下是一个例子,&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;u&gt;Anya Briar Forger&lt;/u&gt; was born on &lt;u&gt;October 2, 1996&lt;/u&gt;. She spent her early years in &lt;u&gt;Princeton, NJ&lt;/u&gt;. She received mentorship and guidance from faculty members at &lt;u&gt;Massachusetts Institute of Technology&lt;/u&gt;. She completed her education with a focus on Communications. She had a professional role at &lt;u&gt;Meta Platforms&lt;/u&gt;. She was employed in &lt;u&gt;Menlo Park, CA&lt;/u&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Self-Attentive Model for Knowledge Tracing</title>
      <link>http://localhost:1313/posts/sakt/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakt/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;请先了解 &lt;a href=&#34;https://www.mtandhj.com/posts/dkt/&#34;&gt;DKT&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$e \in [E] = \{0, 1, 2, \ldots, E - 1\}$, 习题的序号;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$r \in \{0, 1\}$, 习题 $e$ 某个学生做对与否.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250401140515.png&#34; alt=&#34;20250401140515&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;本文几乎就是 GPT (decoder-only) 的 transformer 在 DKT 上的应用: SAKT 希望根据学生的交互序列 $\{(e_1, r_1), \ldots, (e_t, r_t)\}$ 预测下一题 $e_{t+1}$ 的做对做错的情况.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;由上述描述可知, 与一般的 Transformer 稍有不同, SAKT 在推理过程中需要同时 &amp;ldquo;见到&amp;rdquo; &lt;strong&gt;交互序列&lt;/strong&gt; 以及需要被预测的&lt;strong&gt;新的题目&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于学生的交互序列 $\{(e_1, r_1), \ldots, (e_t, r_t)\}$, 首先得到输入的 token:&lt;/p&gt;&#xA;$$&#xD;&#xA;    y_i = e_i + r_i \cdot E =&#xD;&#xA;    \left \{&#xD;&#xA;        \begin{array}{ll}&#xD;&#xA;        e_i, &amp; \text{if } r_i = 0, \\&#xD;&#xA;        e_i + E, &amp; \text{otherwise}.&#xD;&#xA;        \end{array}&#xD;&#xA;    \right .&#xD;&#xA;    $$&lt;p&gt;即, 对于每个题, 我们需要训练两个独立的 token 以分别对应做对做错. 然后通过 embedding table $\mathbf{M} \in \mathbb{R}^{2E \times d}$ 得到交互序列的向量表示:&lt;/p&gt;</description>
    </item>
    <item>
      <title>ECNU 生存指北</title>
      <link>http://localhost:1313/posts/ecnu/</link>
      <pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ecnu/</guid>
      <description>&lt;h2 id=&#34;报销流程&#34;&gt;报销流程&lt;/h2&gt;&#xA;&lt;h3 id=&#34;会议&#34;&gt;会议&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;目前仅有注册费的报销经验&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;为了报销, 必须走 &lt;strong&gt;出国/回国流程&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;出国流程:&lt;/strong&gt; &amp;ldquo;研究生出国(境)申请&amp;rdquo; -&amp;gt; 发起新流程, 涉及到:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;外语水平证书;&lt;/li&gt;&#xA;&lt;li&gt;会议邀请函;&lt;/li&gt;&#xA;&lt;li&gt;知情同意书扫描件.pdf;  (必须这个名字);&lt;/li&gt;&#xA;&lt;li&gt;会议资助申请表; (申请学院经费需要学院审核盖章)&lt;/li&gt;&#xA;&lt;li&gt;国际会议资助申请人基本信息表;&lt;/li&gt;&#xA;&lt;li&gt;论文中英文摘要.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;回国流程:&lt;/strong&gt; &amp;ldquo;研究生回国(境)申请&amp;rdquo; -&amp;gt; 发起新流程, 涉及到:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;会议总结报告;&lt;/li&gt;&#xA;&lt;li&gt;会议论文;&lt;/li&gt;&#xA;&lt;li&gt;会议照片;&lt;/li&gt;&#xA;&lt;li&gt;注册费票据;&lt;/li&gt;&#xA;&lt;li&gt;poster/ppt;&lt;/li&gt;&#xA;&lt;li&gt;会议日程安排;&lt;/li&gt;&#xA;&lt;li&gt;机票订单、行程单/发票、登机牌; (如果实际上没出国不需要)&lt;/li&gt;&#xA;&lt;li&gt;住宿费收据; (如果实际上没出国不需要)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;预约报销单:&lt;/strong&gt; 这个正常来说是导师完成的, 倘若导师下放这个权利, 需要经过: 公共数据库 -&amp;gt; 预约报销 -&amp;gt; 网上预约报账 -&amp;gt; 申请报销单, 报销单有几个点需要特别注意 (如果有发票, 需要通过 &amp;ldquo;增值税发票查验&amp;rdquo; 录入发票):&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;业务大类: 因公出境;&lt;/li&gt;&#xA;&lt;li&gt;单项目报销: 需要填导师的项目, 同时导师给你相应的授权&lt;/li&gt;&#xA;&lt;li&gt;项目负责人: 填了上面的项目后会自动生成;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;打印:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;报销单;&lt;/li&gt;&#xA;&lt;li&gt;出国/回国申请流程;&lt;/li&gt;&#xA;&lt;li&gt;会议邀请信;&lt;/li&gt;&#xA;&lt;li&gt;会议日程;&lt;/li&gt;&#xA;&lt;li&gt;相关发票和支付凭证、订单.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;打印完, 请将材料分门别类, 除报销单外写上对应的类别.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;报销单: 审批人, 项目负责人, 经办人签字, 然后盖&amp;quot;报销章&amp;quot;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamic Key-Value Memory Networks for Knowledge Tracing</title>
      <link>http://localhost:1313/posts/dkvmn/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/dkvmn/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;请先了解 &lt;a href=&#34;https://www.mtandhj.com/posts/mann/&#34;&gt;MANN&lt;/a&gt; 以及 &lt;a href=&#34;https://www.mtandhj.com/posts/dkt/&#34;&gt;DKT&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$q \in \mathcal{Q}$, 题目集合, $|\mathcal{Q}| = Q$;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$r \in \{0, 1\}$,  表示题目做对与否;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250330174039.png&#34; alt=&#34;20250330174039&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;之前的 DKT, 比如利用 RNN 的方式, 还是一种 序列特征提取 -&amp;gt; 二分类 这样的传统的方式, 这种方式没法给我们一些更多的信息: 学生在不同 concepts 上的掌握程度.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;DKVMN (Dynamic Key-Value Memory Networks) 主要是通过改进一个记忆网络 MANN 来实现对习题的概念和学生的掌握情况的一个动态建模.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;我们构建 $\mathbf{M}^k \in \mathbb{R}^{N \times d_k}$ 来建模 $N$ latent concepts $\{c^1, c^2, \ldots, c^N\}$, 以及 $\mathbf{M}^v \in \mathbb{R}^{N \times d_v}$ 来建模某个学生的对于不同 concept 的掌握情况 $\{\mathbf{s}_t^1, \mathbf{s}_t^2, \ldots, \mathbf{s}_t^2 \}$. 并用 $\mathbf{M}_t^v$ 表示在了解 $q_t$ 做题情况后的状态.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Meta-Learning with Memory-Augmented Neural Networks</title>
      <link>http://localhost:1313/posts/mann/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/mann/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\mathbf{x}_t \in \mathbb{R}^d$, 输入;&lt;/li&gt;&#xA;&lt;li&gt;$\mathbf{k}_t \in \mathbb{R}^d$, 根据输入得到的用于更新的向量;&lt;/li&gt;&#xA;&lt;li&gt;$\mathbf{M}_t \in \mathbb{R}^{N \times d}$, memory matrix;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;我们希望维护一个 memory matrix $\mathbf{M}_t$ 以保存&lt;strong&gt;最新最有用&lt;/strong&gt;的信息 (least recently used access (LRUA)).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Read:&lt;/strong&gt; 负责从 $\mathbf{M}_t$ 中读取信息, 给定 key $\mathbf{k}_t$, 通过 cosine similarity 来计算两两相似度:&lt;/p&gt;&#xA;$$&#xD;&#xA;    K(\mathbf{k}_t, \mathbf{M}_t(i)) =&#xD;&#xA;    \frac{&#xD;&#xA;        \mathbf{k}_t \cdot \mathbf{M}_t (i)&#xD;&#xA;    }{&#xD;&#xA;        \|\mathbf{k}_t\|  \| \mathbf{M}_t (i) \|&#xD;&#xA;    }, \quad \forall i.&#xD;&#xA;    $$&lt;p&gt;这里 $\mathbf{M}_t(i)$ 表示矩阵的第 $i$ 行. 接着, 通过重加权计算所读取的向量:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \mathbf{r}_t \leftarrow \sum_i w_t^r (i) \mathbf{M}_t (i), \\&#xD;&#xA;    w_t^r (i) \leftarrow&#xD;&#xA;    \frac{&#xD;&#xA;        \exp (K (\mathbf{k}_t, \mathbf{M}_t (i)))&#xD;&#xA;    }{&#xD;&#xA;        \sum_{j} \exp (K (\mathbf{k}_t, \mathbf{M}_t (j)))&#xD;&#xA;    }.&#xD;&#xA;    $$&lt;p&gt;记 $w (i)$ 所构成的向量为 $\mathbf{w}$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Knowledge Tracing</title>
      <link>http://localhost:1313/posts/dkt/</link>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/dkt/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RNN, LSTM: 循环神经网络依赖历史状态和当前信号估计下一时刻的状态, 可用于预测, 回归等.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250327211112.png&#34; alt=&#34;20250327211112&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;知识追踪的目的是根据学生的历史作答情况 $\mathbf{x}_0, \ldots, \mathbf{x}_t = \{q_t, a_t\}$ (其中 $q_t$ 表示 exercise, $a_t$ 表示做对做错的情况), 来估计当前学生的状态 (通过一个向量 $\mathbf{y} \in [0, 1]^{M}$, 第 $i$ 个元素表示该学生当前状态下做对第 $i$ 题的概率).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250327212027.png&#34; alt=&#34;20250327212027&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;需要特别注意的是, DKT 设定一个题目对应两个独立表示 (做对, 做错).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;DKT 有很多用处:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;根据 $\mathbf{y}_t$ 我们可以判断学生对不同题目的掌握情况, 据此可以个性化出题;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;判断题目间的关系, 作者给出一种 influence 指标:&lt;/p&gt;&#xA;$$&#xD;&#xA;        J_{ij} = \frac{y(j|i)}{\sum_{k} y(j | k)},&#xD;&#xA;        $$&lt;p&gt;这里 $y(j|i)$ 表示仅观察到做对题目 $i$ 情况下做对题目 $j$ 的概率. 显然 $J_{ij}$ 越大两个题目越趋同.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;&#xA;&lt;ol class=&#34;reference&#34;&gt;&#xD;&#xA;  &lt;li&gt;&#xD;&#xA;    Piech C., Bassen J., Huang J., Ganguli S.,&#xD;&#xA;    Sahami M., Guibas L., Sohl-Dickstein J.&#xD;&#xA;    &lt;u&gt;Deep Knowledge Tracing.&lt;/u&gt;&#xD;&#xA;    &lt;i&gt;NeurIPS&lt;/i&gt;, 2015.&#xD;&#xA;    &lt;a href=&#34;https://proceedings.neurips.cc/paper/2015/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf&#34; style=&#34;color: #007acc; font-weight: bold; text-decoration: none;&#34;&gt;[PDF]&lt;/a&gt;&#xD;&#xA;    &lt;a href=&#34;https://github.com/chrispiech/DeepKnowledgeTracing&#34; style=&#34;color: #007acc; font-weight: bold; text-decoration: none;&#34;&gt;[Code]&lt;/a&gt;&#xD;&#xA;  &lt;/li&gt;&#xD;&#xA;  &lt;!-- 添加更多文献条目 --&gt;&#xD;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Unifying Generative and Dense Retrieval for Sequential Recommendation</title>
      <link>http://localhost:1313/posts/liger/</link>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/liger/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请了解 &lt;a href=&#34;https://www.mtandhj.com/posts/tiger/&#34;&gt;TIGER&lt;/a&gt; 和 UniSRec.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;本文投稿 ICLR 2025 被惨拒, 我看主要问题集中在方法层面过于简单. 个人认为确实如此, 不过有一些实验观察我感觉还是很有趣的, 至少我之前不清楚生成式推荐在冷启动上居然存在问题 (我一直认为这方面应该是其优势才对).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250327143851.png&#34; alt=&#34;20250327143851&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;这里需要先声明一下实验设置:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Dense:&lt;/strong&gt; 采用的 UnisRec 的训练方式, 同时依赖 text embedding 和 id embedding 的版本, Dense 是指其推荐的方式最终是通过一个编码得到的 user embedding 和所有的 item embedding 进行一一匹配计算相似度然后排名得到的. 其输入 item embedding 为:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;$$&#xD;&#xA;    \tag{1}&#xD;&#xA;    \mathbf{e}_i = \underbrace{\mathbf{x}_i}_{ID} + \underbrace{\mathbf{x}_i^{\text{text}}}_{text} + \underbrace{\mathbf{x}_i^{\text{pos}}}_{positional}.&#xD;&#xA;    $$&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Generative:&lt;/strong&gt; 采用的是 TIGER, 其首先通过 RQ-VAE 将 item 的文本信息编码成离散的 token 表示, 称之为 semantic ID, 然后基于 semantic ID 进行推荐. 相较于 Dense, 它的词表会小很多, 因此某种程度上会更加高效一些.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;如上图所示, 相比于 Dense, TIGER 在 In-set 和 Cold-start 场景下的效果都不尽如人意. 特别是后者, 会让人感觉特别奇怪.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Collaborative Alignment for Recommendation</title>
      <link>http://localhost:1313/posts/carec/</link>
      <pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/carec/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\mathcal{U} = \{u_1, u_2, \ldots, u_{|\mathcal{U}|}\}$, users;&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{I} = \{i_1, i_2, \ldots, i_{|\mathcal{I}|}\}$, items;&lt;/li&gt;&#xA;&lt;li&gt;$\mathbf{R} \in \{0, 1\}^{|\mathcal{U}| \times |\mathcal{I}|}$, interaction matrix;&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{G}(\mathcal{U}, \mathcal{I}, \mathcal{E})$, 对应的图.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250325154434.png&#34; alt=&#34;20250325154434&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;以前的方法大多为:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;纯的 ID-based 的方法, 即  user/item 均用可训练的 embeddings 表示;&lt;/li&gt;&#xA;&lt;li&gt;纯的语义表示, 即 user/item 均用文本或者其它模态信息编码得到的编码;&lt;/li&gt;&#xA;&lt;li&gt;二者混合.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;本文属于第三种, 特别之处在于 user embeddings 随机初始化 $\mathbf{h}_u^{(0)}$, item 的 embeddings 用语义表示 $\mathbf{h}_i^{(0)}$ (通过训练好的语言模型得到).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Semantic Aligning Phase:&lt;/strong&gt; 这一阶段是为了训练 user embedddings, 使其和 item 的编码得到的语义特征对齐:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;首先经过 LightGCN:&lt;/p&gt;&#xA;$$&#xD;&#xA;        \mathbf{h}_u, \mathbf{h}_i = \text{Aggregator}&#xD;&#xA;        \big(&#xD;&#xA;            \mathcal{G}(\mathcal{U}, \mathcal{I}, \mathcal{E}),&#xD;&#xA;            \mathbf{h}_u^{(0)}, \mathbf{h}_i^{(0)}&#xD;&#xA;        \big).&#xD;&#xA;        $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;通过类似 DirectAU 的方式对齐 $\mathbf{h}_u \rightarrow \mathbf{h}_i$:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multimodal Pre-training for Sequential Recommendation via Contrastive Learning</title>
      <link>http://localhost:1313/posts/mp4sr/</link>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/mp4sr/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\mathcal{S} = \{i_1, i_2, \ldots, i_n\}$, user behavior sequence;&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{T}_i = \{t_1^i, t_2^i, \ldots, t_{|\mathcal{T}_i|}^i\}$, item $i$ 的文本描述 ($t_j^i$ 可以理解为其中的一个句子);&lt;/li&gt;&#xA;&lt;li&gt;$\mathcal{V}_i = \{v_1^i, v_2^i, \ldots, v_{|\mathcal{V}_i|}^i\}$, item $i$ 所对应的图片特征.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250324111832.png&#34; alt=&#34;20250324111832&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;multimodal-feature-extraction&#34;&gt;Multimodal Feature Extraction&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MP4SR 首先将 item 的文本和图片转换为特征:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;对于文本, 直接通过 Sentence-BERT 对每个句子进行编码得到:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;$$&#xD;&#xA;    \mathbf{x}_i^t = stack&#xD;&#xA;    \bigg[&#xD;&#xA;      \text{BERT}(t_1^i), &#xD;&#xA;      \text{BERT}(t_2^i), &#xD;&#xA;      \ldots,&#xD;&#xA;      \text{BERT}(t_{|\mathcal{T}_i|}^i),&#xD;&#xA;    \bigg].&#xD;&#xA;    $$&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;对于图片的转换则较为特殊: 图片 $\overset{\text{CLIP}}{\rightarrow}$ 特征 $\overset{\text{匹配文本Token}}{\rightarrow}$ Top-N 文本 token:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;$$&#xD;&#xA;    f(w) = \text{sim} (\text{CLIP}(v_{\ell}^i), \text{CLIP}(w)) \quad \forall w \in \mathcal{D}, \\&#xD;&#xA;    \mathbf{v}_{\ell}^i = \text{BERT}\bigg(&#xD;&#xA;      concat \big(&#xD;&#xA;        \text{TopN}(&#xD;&#xA;          \{f(w_1), \ldots, f(w_{|\mathcal{D}|})\},&#xD;&#xA;          N&#xD;&#xA;        )&#xD;&#xA;      \big)&#xD;&#xA;    \bigg), \\&#xD;&#xA;    \mathbf{x}_i^v = stack&#xD;&#xA;    \bigg [&#xD;&#xA;      \mathbf{v}_1^i,  \mathbf{v}_2^i, \ldots, \mathbf{v}_{|\mathcal{V}_i|}^i&#xD;&#xA;    \bigg ] \in \mathbb{R}^{|\mathcal{V}_i| \times d},&#xD;&#xA;    $$&lt;p&gt;其中 $\mathcal{D}$ 表示整个词表. 所以其实是相当于给图片匹配它所对应的文本描述, 如此一来就省去了图片和文本模态对齐的问题.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vector Quantization</title>
      <link>http://localhost:1313/slides/vq/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/slides/vq/</guid>
      <description>&lt;section data-markdown&gt;&#xD;&#xA;## Vector Quantization&#xD;&#xA;&lt;/section&gt;&#xD;&#xA;&lt;!-- --------------------------------------------------------- --&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;&lt;textarea data-template&gt;&#xD;&#xA;&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;表征学习&lt;/strong&gt;一直是深度学习的重点&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;slide-img&#34;&gt;&#xD;&#xA;  &lt;img src=&#34;https://miro.medium.com/v2/resize:fit:4416/format:webp/1*bvMhd_xpVxfJYoKXYp5hug.png&#34; alt=&#34;Image&#34; style=&#34;max-width: 80%; height: auto;margin: 0 auto;&#34;&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/textarea&gt;&#xD;&#xA;&lt;/section&gt;&#xD;&#xA;&lt;!-- --------------------------------------------------------- --&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;&lt;textarea data-template&gt;&#xD;&#xA;&lt;h3 id=&#34;background-1&#34;&gt;Background&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Encoder $\phi: X \rightarrow \bm{z} \in \textcolor{red}{\mathbb{R}^{d}}$ (连续空间)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;向量量化: $X \rightarrow \bm{c} \in \mathcal{C} = \{\bm{c}_k\}_{k=1}^K$ (离散空间)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;span style=&#34;color: blue&#34;&gt;✓&lt;/span&gt; 离散化表示更符合人类语言和符号特性, 或许更利于生成任务&lt;/p&gt;&#xA;&lt;p&gt;&lt;span style=&#34;color: blue&#34;&gt;✓&lt;/span&gt; 更强的可解释性和控制性&lt;/p&gt;&#xA;&lt;p&gt;&lt;span style=&#34;color: blue&#34;&gt;✓&lt;/span&gt; 更好的可检索性&lt;/p&gt;&#xA;&lt;/textarea&gt;&#xD;&#xA;&lt;/section&gt;&#xD;&#xA;&lt;!-- --------------------------------------------------------- --&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;&lt;textarea data-template&gt;&#xD;&#xA;&lt;h3 id=&#34;vae&#34;&gt;VAE&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Encoder $\phi$: 它将输入 $X \in \mathbb{R}^{H \times W \times 3}$ 映射到一个分布:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \bm{z} \sim q(\bm{z}|X; \phi).&#xD;&#xA;    $$&lt;p&gt;e.g., 高斯分布: $\phi(\bm{x}) \rightarrow (\bm{\mu}, \Sigma) \rightarrow \mathcal{N}(\bm{\mu}, \Sigma)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SOLO</title>
      <link>http://localhost:1313/slides/solo/</link>
      <pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/slides/solo/</guid>
      <description>&lt;!-- --------------------------------------------------------- --&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;## Pushing the Limits of Low-Bit Optimizers with a Focus on EMA Dynamics&#xD;&#xA;&lt;/section&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;&lt;textarea data-template&gt;&#xD;&#xA;&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型大小飞速增加 vs. 硬件价格居高不下&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;slide-img&#34;&gt;&#xD;&#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312203012.png&#34; alt=&#34;Image&#34; style=&#34;max-width: 65%; height: auto; margin: 0 auto;&#34;&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;解决方案:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MoE, LoRA; ZeRO, FSDP;&lt;/li&gt;&#xA;&lt;li&gt;Network Quantization; &lt;span style=&#34;color: red;&#34;&gt;Lightweight Optimizers&lt;/span&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/textarea&gt;&#xD;&#xA;&lt;/section&gt;&#xD;&#xA;&lt;!-- --------------------------------------------------------- --&gt;&#xD;&#xA;&lt;section data-markdown&gt;&#xD;&#xA;&lt;textarea data-template&gt;&#xD;&#xA;&lt;h3 id=&#34;background-1&#34;&gt;Background&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Optimizer States (2x model size):&lt;/p&gt;&#xA;$$&#xD;&#xA;  m_{t+1} \leftarrow \beta_1 \cdot m_t + (1 - \beta_1) \cdot g, \\&#xD;&#xA;  v_{t+1} \leftarrow \beta_2 \cdot v_t + (1 - \beta_2) \cdot g^2.&#xD;&#xA;  $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;DeepSeek-v3 训练框架: $g \overset{\text{BF16}}{\rightarrow} m, v \overset{\text{FP32}}{\rightarrow} \theta$&lt;/p&gt;</description>
    </item>
    <item>
      <title>谱图神经网络</title>
      <link>http://localhost:1313/trends/gnn/</link>
      <pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/trends/gnn/</guid>
      <description>&lt;div id=&#34;timeline&#34;&gt;&#xD;&#xA;  &lt;!-- 时间线将由 JavaScript 自动生成 --&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;script&gt;&#xD;&#xA;// 时间线数据&#xD;&#xA;window.timelineData = [&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2013-12-01&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;DeepWalk&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;首个将深度学习应用于图结构的工作&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1403.6652&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;seminal&#34;&#xD;&#xA;  },&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2014-09-15&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;Spectral CNN&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;首个基于谱图理论的卷积神经网络&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1312.6203&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;novel&#34;&#xD;&#xA;  },&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2016-05-01&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;GCN&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;提出了简化的图卷积网络框架&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1609.02907&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;seminal&#34;&#xD;&#xA;  },&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2017-02-10&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;GraphSAGE&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;提出了基于采样的图神经网络方法&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1706.02216&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;novel&#34;&#xD;&#xA;  },&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2017-02-09&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;GraphSAGE2&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1706.02216&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;emmm&#34;&#xD;&#xA;  },&#xD;&#xA;  {&#xD;&#xA;    &#34;date&#34;: &#34;2018-07-01&#34;,&#xD;&#xA;    &#34;title&#34;: &#34;GAT&#34;,&#xD;&#xA;    &#34;description&#34;: &#34;引入注意力机制到图神经网络中&#34;,&#xD;&#xA;    &#34;paperUrl&#34;: &#34;https://arxiv.org/abs/1710.10903&#34;,&#xD;&#xA;    &#34;importance&#34;: &#34;seminal&#34;&#xD;&#xA;  }&#xD;&#xA;];&#xD;&#xA;&lt;/script&gt;&#xD;&#xA;&lt;script src=&#34;http://localhost:1313/js/timeline.js&#34;&gt;&lt;/script&gt;</description>
    </item>
    <item>
      <title>Autoregressive Image Generation using Residual Quantization</title>
      <link>http://localhost:1313/posts/rqvae/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rqvae/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请务必了解 &lt;a href=&#34;https://www.mtandhj.com/posts/vqvae/&#34;&gt;VQ-VAE&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316155423.png&#34; alt=&#34;20250316155423&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;RQ-VAE 自称也是为了解决所谓的 &lt;a href=&#34;https://www.mtandhj.com/posts/fsq/#%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86&#34;&gt;codebook collapse&lt;/a&gt; 问题, 即当 codebook size 逐渐增加的时候, 或有越来越多的向量变得&amp;quot;冗余&amp;quot;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;另一方面, 如果我们减少 codebook size, 很容易相当在向量量化的过程会造成非常大的信息损耗. 于是, 本文提出了 RQ-VAE, 本质上是一个向量逐步地匹配 $D$ 个向量, 而非 one-to-one 的模式.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;RQ-VAE 的过程可以如此形式化:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;给定图片输入 $\mathbf{X} \in \mathbb{R}^{H_o \times W_o \times 3}$;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;经过 Encoder $E$ 得到&lt;/p&gt;&#xA;$$&#xD;&#xA;        \mathbf{Z} = E(\mathbf{X}) \in \mathbb{R}^{&#xD;&#xA;            \underbrace{H_o / f}_{=: H} \times &#xD;&#xA;            \underbrace{W_o / f}_{=: W} \times &#xD;&#xA;            n_z&#xD;&#xA;        };&#xD;&#xA;        $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;给定 codebook $\mathcal{C} = \{\mathbf{e}_k\}_{k \in [K]}$, 进行向量量化:&lt;/p&gt;&#xA;$$&#xD;&#xA;        Q(\mathbf{z} \in \mathbb{R}^{n_z}; \mathcal{C})&#xD;&#xA;        = \text{argmin}_{k \in [K]} \|\mathbf{z} - \mathbf{e}_k \|_2^2,&#xD;&#xA;        $$&lt;p&gt;对于 $\mathbf{Z}$ 来说, 可以得到如下的 codes:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Recommender Systems with Generative Retrieval</title>
      <link>http://localhost:1313/posts/tiger/</link>
      <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/tiger/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;请了解 &lt;a href=&#34;https://www.mtandhj.com/posts/rqvae/&#34;&gt;RQ-VAE&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316175829.png&#34; alt=&#34;20250316175829&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;目前主流的推荐系统的一个痛点是将 item 表示为一个 embedding, 这就导致对于冷启动的场景并不友好 (既然我们没法再立即获得高效的新来的 item 的表示). 此外, 现阶段的推荐系统大多采用 matching 的架构 (在 item 数量较多的时候可能会慢一些), 本文探索一种生成式的检索方式.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;本文所提出的 Tiger 依然 RQ-VAE, 对 item 的文本 embedding 首先进行编码, 得到的编码作为 item 的 &amp;lsquo;ID&amp;rsquo;, 后面的模型只需要在此基础上进行预测即可.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316175859.png&#34; alt=&#34;20250316175859&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;如上图所示, 文本的 embedding 经过 RQ-VAE (codebook 不共享) 得到 semantic codes. 比如 codebook 的size 为 8, 则理论上可以表示 $8^K$ 个 items (这里 $K$ 表示残差量化的次数).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;在第一阶段训练完毕之后, 我们就可以用得到的编码作为每个 item 的 &amp;lsquo;ID&amp;rsquo;, 然后就可以训练一个模型来进行生成式推荐, 这里文中给了一个例子:&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250316180725.png&#34; alt=&#34;20250316180725&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于一个新来的 item, 只需要 -&amp;gt; Tiger 编码 -&amp;gt; 就可以用于预测了.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;注:&lt;/strong&gt; 这里省略了避免 ID 碰撞的细节.&lt;/p&gt;</description>
    </item>
    <item>
      <title>中研春招聘小记</title>
      <link>http://localhost:1313/posts/%E4%B8%AD%E7%A0%94%E6%98%A525%E6%8B%9B%E8%81%98/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/%E4%B8%AD%E7%A0%94%E6%98%A525%E6%8B%9B%E8%81%98/</guid>
      <description>&lt;h2 id=&#34;招聘布局&#34;&gt;招聘布局&lt;/h2&gt;&#xA;&lt;p&gt;此次招聘位于交大学术活动中心二楼的一角, 并不显眼, 外头也缺少足够的引导标志. 约莫着有三四十家高校和事业单位分列五排, 蓝白的布告栏竖在一张张桌子之后, 颇有严正以待的架势. 进门开始是东北大学的场地, 紧接着是荆楚学院、江西铜业技术研究院、某某集团. 较为心仪的嘉兴大学坐落在第三排的中部, 围满了一批批人. 其对面是第四排的丽水学院, 而第四排的尾部是南京理工大学. 转过去, 赫然便是湖南大学四个大字, 穿行而过是成都大学、中国警察学院、扬州大学以及坐落在角落的之江实验室.&lt;/p&gt;&#xA;&lt;h2 id=&#34;聊天过程&#34;&gt;聊天过程&lt;/h2&gt;&#xA;&lt;p&gt;称之为聊天过程而非是面试是因为一来本次主题是双选会, 二来本身也不指望能够凭借这场招聘会找到心仪的工作, 更多的是摸摸清楚自己到底几斤几两.&lt;/p&gt;&#xA;&lt;h3 id=&#34;江西铜业技术研究院&#34;&gt;江西铜业技术研究院&lt;/h3&gt;&#xA;&lt;p&gt;我上来盯着这家单位的布告栏看, 想看看这类研究院具体有个什么要求. 不过面前的小哥倒是很热情地邀请我坐下来聊一聊, 不过得知我对此并无兴趣之后我们就分道扬镳了.&lt;/p&gt;&#xA;&lt;h3 id=&#34;某医科单位&#34;&gt;某医科单位&lt;/h3&gt;&#xA;&lt;p&gt;很抱歉忘记具体的名字了, 负责招聘的人也是非常热情. 但是我说我的方向和医学可能八竿子打不着. 虽然上面也明确需要人工智能方面的人才, 但是大抵是需要医学图像的? 招聘人员拿着我的简历, 跳过了前面部分, 竟然直接跳到了&amp;quot;技能和语言&amp;quot;一栏, 说着 Python, PyTorch 和我们的需求挺符合的. 我感到奇怪, 我觉得这些是无关紧要的东西, 不过想来也能理解, 来招聘的多是为整个单位招人.&lt;/p&gt;&#xA;&lt;h3 id=&#34;南阳理工学院&#34;&gt;南阳理工学院&lt;/h3&gt;&#xA;&lt;p&gt;这个主要是跟在师妹边上旁听的, 只能说是大开眼界: 据招聘人来说, 他们学院可以非常痛快地给安家费, 然后每个月到手能到 13K (副教授待遇), 而且科研横向不收取管理费! 另外, 需要说明的是, 这 13K 如果后期考核不通过会降到 11K (但是招聘说这句话的时候明显咯噔了一些, 大概率存在猫腻).&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;优点:&lt;/strong&gt; 没有非升即走, 博士毕业点击就送, 安家费给的痛快, 考核要求很低&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;缺点:&lt;/strong&gt; 这类学院我个人都对他们是否能够长期维持存在疑问, 另外过于痛快总让我感觉其中有猫腻&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;丽水学院&#34;&gt;丽水学院&lt;/h3&gt;&#xA;&lt;p&gt;丽水学院的招聘老师人挺好, 也没啥架子. 比较有趣的是, 丽水学院里面设立了一个 &amp;ldquo;数学与计算机学院&amp;rdquo;, 我当时的第一个感觉就是这玩意儿可真有意思. 由于对地域不太感兴趣, 仅探听到了大约年薪十几万这个普遍的待遇条件.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Finite Scalar Quantization: VQ-VAE Made Simple</title>
      <link>http://localhost:1313/posts/fsq/</link>
      <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/fsq/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.mtandhj.com/posts/vqvae/&#34;&gt;VQ-VAE&lt;/a&gt; 提供了一种优雅的向量量化 (离散化表示) 的一种方式, 然而其中的 codebook 的训练以及前置的 encoder 的训练依赖 stop gradient ($\text{sg}(\cdot)$) 以及 straight-through estimator (STE) 操作, 这会导致训练起来比较困难. 具体来说, 可能:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;codebook 中的部分向量过于接近, 从而冗余;&lt;/li&gt;&#xA;&lt;li&gt;很多向量在训练过程中完全不会匹配到任何向量.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;习惯上, 我们称训练过程中发生了 Codebook Collapse 的问题.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;以及有不少文章注意到并且提出了一些解决方案 (包括本文), 我们对部分文章一笔带过:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;[1] 中对会对那些长期不产生匹配的向量进行重新初始化;&lt;/li&gt;&#xA;&lt;li&gt;[2] 主要正对 codebook 的初始化, 不似一般的随机初始化, 其提出根据初始的数据分布, 通过 K-Means++ 进行一个初步的初始化, 并且强调了 scaling 的重要性;&lt;/li&gt;&#xA;&lt;li&gt;[3] 中提出了一种随机量化的方法, 本质上用 Gumbel-softmax 替代 STE.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250312145029.png&#34; alt=&#34;20250312145029&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;注意到, 一般的向量量化 (VQ) 需要一个&lt;strong&gt;显式的可训练的&lt;/strong&gt; codebook $\mathcal{C} = \{c_k\}_{k=1}^K$, 然后给定一个隐变量 $z \in \mathbb{R}^d$, 通过&lt;/p&gt;&#xA;$$&#xD;&#xA;    z_q = \text{argmin}_{c \in \mathcal{C}} \|z - c\|&#xD;&#xA;    $$&lt;p&gt;来进行一个量化.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Taming Transformers for High-Resolution Image Synthesis</title>
      <link>http://localhost:1313/posts/vqgan/</link>
      <pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/vqgan/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在学习 VQGAN 之前, 请务必先了解 &lt;a href=&#34;https://www.mtandhj.com/posts/vqvae/&#34;&gt;VQ-VAE&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;核心思想&#34;&gt;核心思想&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250311144000.png&#34; alt=&#34;20250311144000&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Transformer 已经在 NLP 领域取得了巨大的进展, 本文想要开发其在图像生成领域的能力.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;part1-离散编码&#34;&gt;Part1: 离散编码&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;既然 Transformer 的成功依赖离散的 token, 那么通过它来生成图片很重要的一个点是如何将图片离散化? 于是乎, 作者引入了 VQGAN 来得到图片的离散编码.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;给定一个图片 $x \in \mathbb{R}^{H \times W \times 3}$, 首先通过一个 &lt;strong&gt;CNN&lt;/strong&gt; encoder $E$ 来得到初步的编码:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \hat{z} = E(x) \in \mathbb{R}^{h \times w \times n_z}.&#xD;&#xA;    $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;接着, element-wise 地为每一个&amp;rsquo;像素点&amp;rsquo;匹配它的 token:&lt;/p&gt;&#xA;$$&#xD;&#xA;    z_{\mathbf{q}} = \mathbf{q}(\hat{z}) := \bigg(\text{argmin}_{z_k \in \mathcal{Z}} \|\hat{z}_{ij} - z_k\| \bigg)_{ij},&#xD;&#xA;    $$&lt;p&gt;这里 $\mathcal{Z} = \{z_k\}_{k=1}^K \subset \mathbb{R}^{n_z}$, 俗称 codebook.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Discrete Representation Learning</title>
      <link>http://localhost:1313/posts/vqvae/</link>
      <pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/vqvae/</guid>
      <description>&lt;h2 id=&#34;预备知识&#34;&gt;预备知识&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;作者的目的是实现离散化的表示学习: 给定任意的模式, 编码成离散的表示.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;既然本文是居于 VAE (变分自编码) 的框架实现的, 我们得对变分自编码有一个初步的了解. VAE 主要包含三个模块:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Encoder $\phi$: 它将输入 $x \in \mathbb{R}^D$ 映射到一个分布:&lt;/p&gt;&#xA;$$&#xD;&#xA;        q(z|x; \phi).&#xD;&#xA;        $$&lt;p&gt;比如当服从的高斯分布, 实质上 $\phi(x) \rightarrow (\mu, \sigma) \rightarrow \mathcal{N}(\mu, \sigma^2)$, 然后 $z$ 从该分布中采样即可;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Decoder $\Phi$: 它将隐变量 $z$ 映射回 (通常来说) $x$ 的空间:&lt;/p&gt;&#xA;$$&#xD;&#xA;        p(x|z; \Phi);&#xD;&#xA;        $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;还有一个先验分布 $p(z)$ 用于辅助训练.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;VAE 的训练目标是极大似然的一个下界:&lt;/p&gt;&#xA;$$&#xD;&#xA;    \begin{align*}&#xD;&#xA;    \log p(x) &#xD;&#xA;    &amp;= \log \int p(x, z) \mathrm{d}z \\&#xD;&#xA;    &amp;= \log \int q(z|x; \phi) \cdot \frac{p(x, z)}{q(z|x; \phi)} \mathrm{d}z \\&#xD;&#xA;    &amp;= \log \int q(z|x; \phi) \cdot \frac{p(x| z; \Phi) p(z)}{q(z|x; \phi)} \mathrm{d}z \\&#xD;&#xA;    &amp;\ge \int q(z|x; \phi) \log \frac{p(x| z; \Phi) p(z)}{q(z|x; \phi)} \mathrm{d}z \\&#xD;&#xA;    &amp;= \int q(z|x; \phi) \log \frac{p(z)}{q(z|x; \phi)} \mathrm{d}z +&#xD;&#xA;    \int q(z|x; \phi) \log p(x|z; \Phi) \mathrm{d}z \\&#xD;&#xA;    &amp;= \underbrace{-\mathbf{KL}(q_{\phi}\| p(z)) +&#xD;&#xA;    \mathbb{E}_{z \sim q_{\phi}} \log p(x|z; \Phi)}_{\text{ELBO}}.&#xD;&#xA;    \end{align*}&#xD;&#xA;    $$&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ELBO 包括一个和先验分布的 KL 散度 (这部分通常是增加隐变量的 diversity 的), 以及一个正常的交叉熵 (如果 $p_{\Phi}$ 也是一个高斯, 则通常称之为重构损失).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Git</title>
      <link>http://localhost:1313/posts/git/</link>
      <pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/git/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.liaoxuefeng.com/wiki/896043488029600&#34;&gt;廖雪峰Git教程&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;初始化&#34;&gt;初始化&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;在你想要git的文件夹内 git bash here&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;接着注册&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git config --global user.name &amp;#34;XXXXXX&amp;#34;&#xD;&#xA;git config --global user.email &amp;#34;XXX@+++.com&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;配置别名&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git config --global alias.last &amp;#39;log -1&amp;#39;&#xD;&#xA;git config --global alias.lg &amp;#34;log --color --graph --pretty=format:&amp;#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&amp;lt;%an&amp;gt;%Creset&amp;#39; --abbrev-commit&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;上面的步骤是第一次使用git, 若不是可省略&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;将所在目录变成git可以管理的仓库&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git init&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;在所在目录添加 .gitignore 文件, 一般可以直接在&lt;a href=&#34;https://github.com/github/gitignore&#34;&gt;这儿&lt;/a&gt;选择所需要的就行, 特殊情况可以自己再加点定制&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git add .gitignore&#xD;&#xA;git commit -m &amp;#34;add .gitignore&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;远程仓库&#34;&gt;远程仓库&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;创建ssh key&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ssh-keygen -t rsa -C &amp;#34;xxx@+++.com&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后在主目录下找到.ssh目录里面的id_rsa.pub (公钥), 并复制文件里的内容.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;在GitHub的settings里面找到ssh keys (SSH and GPG keys)部分添加new ssh key&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
