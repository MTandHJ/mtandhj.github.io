---
date: "2025-05-18"
draft: false
title: "Connection Bottleneck in Attention"
author: MTandHJ
tags:
  - Slide
  - Attention
  - Positional Encoding
---

<section data-markdown>
## Connection Bottleneck in Attention
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

##  Attention

$$
A_{ij} = \frac{\exp(S_{ij})}{\sum_j \exp(S_{ij})}, \quad
S_{ij} = \textcolor{blue}{\langle \bm{q}_i, \bm{k}_j \rangle} / \sqrt{d}, \\ 
\bm{q} = W_Q\bm{x}, \bm{k} = W_K\bm{x} \in \mathbb{R}^d,
\quad i, j = 0, 1, \ldots, L-1.
$$

- ç‰¹æ®Šç±»å‹çš„ Attention çš„ Graph å½¢æ€:

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250518212008.png" 
  alt="Image" 
  style="max-width: 60%; height: auto;margin: 0 auto;">
</div>


Note:
Attention å®é™…ä¸Šå®šä¹‰äº†åºåˆ—ä¸­å„ä¸ªä½ç½®çš„ Connection å¼ºåº¦
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Over-Squashing in Graph Neural Networks


- å¹¿æ³›çš„è¿æ¥å¯¼è‡´è¿‡äºç‹­çª„çš„ä¿¡æ¯ä¼ é€’:

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250519150035.png" 
  alt="Image" 
  style="max-width: 100%; height: auto;margin: 0 auto;">
</div>

- æ„Ÿå—é‡éšç€å±‚æ•°å¢åŠ **æŒ‡æ•°**å¢åŠ  $\rightarrow$ éš¾ä»¥æ•è· Long-range çš„ä¿¡æ¯


<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">Alon U., et al. On the Bottleneck of Graph Neural Networks and Its Practical Implications. ICLR, 2021.</p>
</div>

Note:
éœ€è¦è¯´æ˜çš„æ˜¯, ä»è¿™ç¯‡æ–‡ç« å‡ºå‘, Causal Attention çš„ Bottleneck å¹¶ä¸ä¸¥é‡
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Over-Squashing in Graph Neural Networks


- å®éªŒ(å¿…é¡»ä¾èµ– $\textcolor{blue}{k}$-é˜¶é‚»å±…é¢„æµ‹æ ‡ç­¾):

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250519151436.png" 
  alt="Image" 
  style="max-width: 80%; height: auto;margin: 0 auto;">
</div>

- Attention (GAT) ä»¥åŠé—¨æ§ (GGNN) æœ‰åŠ©äºç¼“è§£ over-squashing

Note:
éšç€å±‚æ•°çš„å¢åŠ , ç”±äº over-squashing çš„å­˜åœ¨, GNN è¶Šæ¥è¶Šéš¾åˆ©ç”¨åˆ° long-range çš„'é‚»å±…'ä¿¡æ¯
</textarea>
</section>

<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## 'Over-Squashing' in Large Language Models

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250519155809.png" 
  alt="Image" 
  style="max-width: 100%; height: auto;margin: 0 auto;">
</div>


- **Over-Squashing:** Early tokens æœ‰æ›´å¤šçš„å½±å“
  - **Representational Collapse:** éšç€åºåˆ—é•¿åº¦å¢åŠ , è¡¨ç¤ºè¶‹è¿‘${}^{\tiny [1]}$
  - **Attention Sink:** LLMs æ€»æ˜¯å€¾å‘äºç»™äºˆ <bos> token å¾ˆé«˜çš„æƒé‡${}^{\tiny [2,3]}$

<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">[1] Barbero F., et al. Transformers need glasses! Information over-squashing in language tasks. NeurIPS, 2024.</p>
    <p style="margin: 2px 0;">[2] Barbero F., et al. Why do LLMs attend to the first token? arXiv, 2025.</p>
    <p style="margin: 2px 0;">[3] Wu X., et al. On the Emergence of Position Bias in Transformers. arXiv, 2025.</p>
</div>


Note:
åœ¨ GNN ä¸­, over-squashing æŒ‡çš„æ˜¯è†¨èƒ€çš„æ„Ÿå—é‡å¯¼è‡´æ¯ä¸ªé‚»å±…çš„è´¡çŒ®å¾ˆæœ‰é™;
è€Œåœ¨ LLM ä¸­, over-squashing æŒ‡çš„æ˜¯ early tokens ä¼šäº§ç”Ÿæ›´å¤šçš„å½±å“.
è™½ç„¶äºŒè€…å¯èƒ½éƒ½ä¼šå¯¼è‡´ç±»ä¼¼ representational collpase çš„ç°è±¡, ä½†æ˜¯ä¸¥æ ¼æ¥è¯´ä¸èƒ½æ··ä¸ºä¸€è°ˆ.
å®é™…ä¸Š, LLM ä¸­æ˜¯å¦å­˜åœ¨æ‰€è°“çš„ over-squashing é—®é¢˜ä¹Ÿæ˜¯ä¸ªæœªçŸ¥æ•°, å› ä¸º Causal Attention å®é™…ä¸Šå·²ç»æ˜¯ Graph é¢†åŸŸé‡Œä¸€ä¸ªæ¨èçš„æ–¹æ¡ˆäº†.
</textarea>
</section>

<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## 'Over-Squashing' in Large Language Models


**Theorem B.3** (Representational Collapse)
Let $X = [\bm{x}_0, \ldots, \textcolor{blue}{\bm{x}_{n-1}}] \in \mathbb{R}^{n \times d}$ and $X^* = [\bm{x}_0, \ldots, \textcolor{blue}{\bm{x}_{n-1}, \bm{x}_{n-1}}] \in \mathbb{R}^{(n + 1) \times d}$ be two sequences for a final repeated token $\bm{x}_{n-1}$, with 
1. All token representations bounded <span style="color: gray"> ($S_{i, j}$ is bounded) </span>;
2. Positional encodings decay with distance to 0 <span style="color: gray"> ($S_{n-1, j} \approx S_{n, j}^*$ as $j \rightarrow 0$) </span>.

Then, for <u>large enough</u> $n \in \mathbb{N}_+$, we have that the representations are under any $\epsilon$:

$$
\|\bm{x}_{n-1}^{(L)} - {\bm{x}_{n}^{*}}^{(L)} \|_1 \le \epsilon.
$$


Note:
è¯æ˜çš„å…³é”®æ˜¯ä¿è¯ Attention èƒ½å¤Ÿå°½å¯èƒ½ä¸€è‡´, å› è€ŒåŠ æƒå’Œä¹‹åçš„å‘é‡è¡¨ç¤ºä¹Ÿä¸€è‡´.
ç¬¬ä¸€ä¸ªæ¡ä»¶ä¸»è¦æ˜¯ä¿è¯è‡ªå·±å’Œè‡ªå·±çš„ score ç®—å‡ºæ¥ä¸ä¼šæ— é™å¤§, å¦åˆ™å°±ä¸€å®šæœ‰åŒºåˆ«, å¦ä¸€ä¸ªæ¡ä»¶ä¸»è¦æ˜¯ä¿è¯ Attention æ˜¯æ¸è¿›ä¸€è‡´çš„.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Copying

<div class="slide-cols">

<!-- left -->
<div class="slide-col-half">

- **First-token** copying: 
  - **Input:** '$\textcolor{red}{0}111\ldots 111$'; **Target:** '$0$'


</div>

<!-- right -->
<div class="slide-col-half">

- **Last-token** copying: 
  - **Input:** '$111\ldots 111\textcolor{red}{0}$'; **Target:** '$0$'


</div>

</div>

- é€æ­¥<u>å¢åŠ  '1' </u> ä»¥å¢åŠ åºåˆ—é•¿åº¦:
  - (B) <span style="color: gray"> Hint: Itâ€™s not necessarily a 1, check carefully </span>;
  - (C) <span style="color: gray"> '$0111 \ldots 11$' æ›¿æ¢ä¸º '$0111 \ldots 11 \: 0111 \ldots 11 \: \ldots$ </span>

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250511142544.png" 
  alt="Image" 
  style="max-width: 100%; height: auto;margin: 0 auto;">
</div>

Note:
Copying çš„ä¾‹å­æœ‰è¶£åœ¨äº: First-token copying æ¯”èµ· Last-token copying åè€Œæ›´å®¹æ˜“.
é€šè¿‡ 'over-squashing' è§£é‡Šå°±æ˜¯, first-token copying èƒ½å¤Ÿäº§ç”Ÿæ›´å¤šçš„å½±å“.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Counting

<div class="slide-cols">

<!-- left -->
<div class="slide-col-half">

$\textcircled{\small 1}$  **æ±‚å’Œ:** $1 + \cdots + 1$; 

$\textcircled{\small 2}$  **è®¡æ•°:** ç»Ÿè®¡ä¸€ä¸²å‡ä¸º 1 çš„åºåˆ—ä¸­æœ‰å¤šå°‘ä¸ª 1;


</div>

<!-- right -->
<div class="slide-col-half">

$\textcircled{\small 3}$ **è®¡æ•°:** ç»Ÿè®¡ä¸€ä¸² 0/1 åºåˆ—ä¸­æœ‰å¤šå°‘ä¸ª 1 (1 å‡ºç°çš„æ¦‚ç‡ä¸º 70%);

$\textcircled{\small 4}$ **å•è¯è®¡æ•°:** ç»Ÿè®¡ä¸€ä¸²åºåˆ—ä¸­æŸä¸ªè¯å‡ºç°çš„æ¬¡æ•°.


</div>

</div>

- ä¸‰ç§ç­–ç•¥:
    1. ç›´æ¥è¾“å‡ºç»“æœ (<span style="color: gray">No CoT</span>);
    2. æ€ç»´é“¾ (<span style="color: gray">CoT Zero-Shot</span>);
    3. ä¾‹å­ + æ€ç»´é“¾ (<span style="color: gray">CoT Few-Shot</span>).


</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Counting

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250511143754.png" 
  alt="Image" 
  style="max-width: 100%; height: auto;margin: 0 auto;">
</div>

- éš¾åº¦: $\textcircled{\small 3} < \textcircled{\small 1} \approx \textcircled{\small 4} < \textcircled{\small 2}$

- ç­–ç•¥: No CoT $\approx$ CoT Zero-Shot $>$ CoT Few-Shot

Note: 
è¿™ä¸ªä¾‹å­ä¸»è¦æ˜¯è¯´æ˜'é—´éš”'ç¬¦å·å¯¹äº Counting çš„å¸®åŠ©.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Positional Encoding

$$
A_{ij} = \frac{\exp(S_{ij})}{\sum_j \exp(S_{ij})}, \quad
S_{ij} = \textcolor{blue}{\langle \bm{q}_i, \bm{k}_j \rangle} / \sqrt{d}.
$$

- RoPE (Rotary Positional Encoding)

$$
  \langle \bm{q}_i, \bm{k}_j \rangle = (R_{i, \theta} \bm{q}_i)^T (R_{j, \theta} \bm{k}_j) = \bm{q}_i^T R_{j-i} \bm{k}_j, \\
  {}\\
  \tiny
  R_{i, \theta} := \left [
  \begin{array}{ccccccc}
  \cos (i\theta_0) & -\sin (i \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
  \sin (i \theta_0) & \cos (i \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
  0 & 0 & \cos (i\theta_1) & -\sin (i \theta_1) & \cdots & 0 & 0 \\
  0 & 0 & \sin (i \theta_1) & \cos (i \theta_1) & \cdots & 0 & 0 \\
  \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  0 & 0 &  0 & 0 & \cdots & \cos (i \theta_{d/2 - 1}) & -\sin (i \theta_{d / 2 - 1}) \\
  0 & 0 &  0 & 0 & \cdots & \sin (i \theta_{d/2 - 1}) & \cos (i \theta_{d / 2 - 1})  \\
  \end{array}
  \right ].
$$

- $\theta_i = b^{-2i / d}$ è¡¨ç¤ºåŸºæœ¬çš„æ—‹è½¬å•ä½, $b$<span style="color: gray">ase</span> è¶Šå¤§, æ—‹è½¬çš„è§’åº¦è¶Šå°. 


Note: 
ä½ç½®ç¼–ç æœ‰å¯èƒ½å¯ä»¥ç¼“è§£ Connection Bottleneck
</textarea>
</section>

<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Positional Encoding

<div class="slide-img">
  <img src="https://picx.zhimg.com/v2-595b69a2e3d6da57a7016f741d4bb8e1_r.webp?source=172ae18b&consumer=ZHI_MENG" 
  alt="Image" 
  style="max-width: 80%; height: auto;margin: 0 auto;">
</div>

- **ä½ç½®ç¼–ç **: ç»´åº¦é å‰ $\rightarrow$ é«˜é¢‘åŒºåŸŸ; ç»´åº¦é å $\rightarrow$ ä½é¢‘åŒºåŸŸ

<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">ç»å¯†ä¼å‡». ååˆ†é’Ÿè¯»æ‡‚æ—‹è½¬ç¼–ç (RoPE). çŸ¥ä¹, 2023.</p>
</div>

Note: 
æ³¨æ„, è¿™é‡Œçš„é«˜ä½é¢‘é’ˆå¯¹çš„æ˜¯ä½ç½®ç¼–ç è€Œä¸æ˜¯è¾“å…¥ä¿¡å· (query or key)
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Positional Encoding

- RoPE çš„è·ç¦»è¡°å‡:

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250512203336.png" 
  alt="Image" 
  style="max-width: 90%; height: auto;margin: 0 auto;">
</div>

- **Left:** RoPE ä¸‹çš„ Attention çš„<span style="color: blue">æŸä¸ªä¸Šç•Œ</span>éšç€ $|j - i|$ å¢åŠ è€Œè¡°å‡

- **Right:** é«˜æ–¯å™ªå£°ä¸‹, çœŸå®çš„ Attention å¹¶æ— è¡°å‡ç°è±¡

</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Positional Encoding

- ä¸ªäººçš„æµ‹è¯•:

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250512210829.png" 
  alt="Image" 
  style="max-width: 90%; height: auto;margin: 0 auto;">
</div>

- å³ä½¿ relative distance å¢åŠ åˆ° 100,000 ä¾ç„¶æ²¡æœ‰è·ç¦»è¡°å‡çš„ç°è±¡

Note: 
æ¨ªåæ ‡æ˜¯ç›¸å¯¹è·ç¦»
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## RoPE çš„é«˜é¢‘

- **çŒœæƒ³:** è¿‡å¤§çš„æ—‹è½¬è§’åº¦ä¼šå¯¼è‡´å¯¹åº”ç»´åº¦æ‰€å¾—ç»“æœè¶‹äºå™ªå£°

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250512211237.png" 
  alt="Image" 
  style="max-width: 80%; height: auto;margin: 0 auto;">
</div>


<div class="slide-cols">

<!-- left -->
<div class="slide-col-half">

$$
\underset{\text{Freq}\downarrow \quad \text{Norm} \uparrow}{\xrightarrow{\|\bm{q}_{0:1}\|, \|\bm{q}_{2:3}\|, \cdots, \|\bm{q}_{d-1:d}\|}}
$$

</div>

<!-- right -->
<div class="slide-col-half">

- **Exception**: **First** and **Last** Layers

</div>

</div>


<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">Barbero F., et al. Round and Round We Go! What makes Rotary Positional Encodings useful? ICLR, 2025.</p>
</div>

Note: 
è¿™æ˜¯ä¸€ä¸ªéšå¼çš„ä¾‹å­: ä½œè€…å°†ç»´åº¦ä¸¤ä¸¤åˆ†ç»„, å‡è®¾æ¨¡é•¿è¶Šå¤§è¶Šåå‘äºè¯­ä¹‰ä¿¡æ¯.
åœ¨ç»å¤§éƒ¨åˆ† layers ä¸­é«˜é¢‘éƒ¨åˆ†ä»…è¢«åˆ†é…äº†è¾ƒå°çš„æ¨¡é•¿, ä¾‹å¤–æ˜¯åˆå§‹çš„å’Œæœ€åçš„ä¸€äº›å±‚.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## RoPE çš„é«˜é¢‘

- **çŒœæƒ³:** é«˜é¢‘æœ‰åˆ©äºç‰¹æ®Š Attention å½¢æ€çš„æ„å»º

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250512211548.png" 
  alt="Image" 
  style="max-width: 80%; height: auto;margin: 0 auto;">
</div>

$\textcircled{\small 1}$ **Last Layers:** <span style="color: blue">Diagonal</span> attention $\textcircled{\small 2}$ **First Layers:** <span style="color: blue">Previous-token</span> attention



<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">Barbero F., et al. Why do LLMs attend to the first token? arXiv, 2025.</p>
</div>


Note: 
æœ€å¼€å§‹çš„å±‚å€¾å‘äº previous-token attention, è€Œæœ€åçš„å‡ å±‚åˆ™å€¾å‘äº diagonal attention.
Previous-token attention, å³ attention sink ç°è±¡åœ¨ä¸‹é¢çš„æ–‡çŒ®æœ‰æ‰€è®¨è®º.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## RoPE çš„é«˜é¢‘

<span style="font-size: 3rem;">â“</span> ä¸æ–½åŠ ä½ç½®ç¼–ç , æ˜¯å¦ä¾ç„¶èƒ½å½¢æˆç‰¹æ®Šçš„ Attention å½¢æ€

- ç»“è®º:
  1. åœ¨ä¸æ–½åŠ ä»»ä½•ä½ç½®ç¼–ç çš„å‰æä¸‹: å¯¹äºé‡å¤çš„åºåˆ—, å¿…<span style="color: red">ä¸å­˜åœ¨</span> 'Diagnoal' æˆ– 'Previous-token' ç±»å‹ Attention
  2. åœ¨æ–½åŠ  RoPE å‰æä¸‹: å¯¹äº<span style="color: red">ä»»æ„</span>åºåˆ—, æ¨¡å‹æ€»èƒ½é€šè¿‡å­¦ä¹ <span style="color: blue">ç‰¹å®šæ¨¡é•¿</span>æ¥å½¢æˆ 'Diagnoal' æˆ– 'Previous-token' ç±»å‹ Attention

- æ€»è€Œè¨€ä¹‹, ä½ç½®ç¼–ç èµ‹äºˆäº†æ¨¡å‹å…³æ³¨<u>ç‰¹å®šåŒºåŸŸ</u>çš„èƒ½åŠ›, æœ‰å¯èƒ½ç¼“è§£ connection bottleneck

</textarea>
</section>

<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## RoPE çš„ä½é¢‘

<span style="font-size: 3rem;">â“</span> $\theta_i = b^{-2i / d} \xrightarrow{\textcolor{blue}{b \uparrow}} \text{long-context ability} \uparrow$

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250513174554.png" 
  alt="Image" style="max-width: 90%; height: auto;margin: 0 auto;">
</div>

<span style="color: gray">1. Long-term decay of upper bound of attention score</span>

2. Long-term Decay of the Ability to Attend More to **Similar Tokens** than Random Tokens


<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">Men X., et al. Base of RoPE Bounds Context Length. NeurIPS, 2024.</p>
</div>

</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## RoPE çš„ä½é¢‘


- $\bm{q}, \bm{k}$ ç‹¬ç«‹åŒåˆ†å¸ƒ, $\mathbb{E}[\bm{\epsilon}] = 0$.

- å¸Œæœ› $\bm{q}, \bm{q} + \bm{\epsilon}$ çš„ attention ä¸¥æ ¼å¤§äº $\bm{q}, \bm{k}$ çš„éœ€è¦æ»¡è¶³:

$$
\small
\begin{align*}
& \frac{1}{2\sigma^2} \bigg(
    \mathbb{E}_{\bm{q}, \bm{\epsilon}} \Big [ \textcolor{blue}{\bm{q}}^T R_{m, \theta} (\bm{q} + \textcolor{blue}{\bm{\epsilon}})\Big]
    -\mathbb{E}_{\bm{q}, \bm{k}} \Big [ \bm{q}^T R_{m, \theta} \textcolor{red}{\bm{k}}\Big]
\bigg) \\
=& \sum_{i=0}^{d / 2 - 1} \cos(m \theta_i) > 0, \quad \forall m = 0,1,2,\ldots, L-1.
\end{align*}
$$

- ç†è®ºä¸Š base $b$ çš„ lower bound:

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250513205428.png" 
  alt="Image" style="max-width: 100%; height: auto;margin: 0 auto;">
</div>


</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Connection Bottleneck

<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250520165318.png" 
  alt="Image" style="max-width: 100%; height: auto;margin: 0 auto;">
</div>


</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Counting (314 '1')


||Length| '1...' | '1,1...' | '1,1,1,1,1;1,...' |
|--|:--:|:--:|:--:|:--:|
|Mistral Medium| 32k| 500  | 500 | 500 |
|Deepseek-R1| 64K| 264 | $\textcircled{\small 1}$ | 299 |
|GPT-4o|128k| 300 | 300 | 340 |
|Llama3.3-70b| 130K | 'The string 1111...' | 150 | '...The string is 1,1,1,1,1;1,...' |
|o4-mini|200k| 232 | 270 | 319 |

<p style="font-size:1rem">$\textcircled{\small 1}$ Given that, and since the sequence is uniform, the count is the number of '1's, which is the total numbers in the sequence.  Given that, and since counting manually is not feasible, the answer is that all numbers are '1's, hence the count is equal to the number of numbers in the sequence. But since the exact count isn't provided, perhaps the answer is to recognize that every number is '1'.</p>

</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## Counting


<div class="slide-img">
  <img src="https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250521161421.png" 
  alt="Image" style="max-width: 80%; height: auto;margin: 0 auto;">
</div>

**Case I:** "y,x,y,y,x,y,y,x,y,x,y,y,y,y,x,y,y,y,x"

**Case II:** "yxyyxyyxyxyyyyxyyyx" (ACC = 0.16%)

- 'é—´éš”ç¬¦' ç”¨å¤„å¾ˆå¤§, base $b$ ä¼¼ä¹æ²¡æœ‰æ˜æ˜¾çš„è§„å¾‹.

</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

## ğŸ’¡ Connection Bottleneck

- æ›´å¥½çš„ä½ç½®ç¼–ç ? <span style="color: red">No!</span>

![20250521163302](https://raw.githubusercontent.com/MTandHJ/blog_source/master/images/20250521163302.png)

- ç‰©ç†å±‚é¢çš„ Attention?

<div class="slide-ref">
    <div style="width: 100px; height: 1px; background: black; margin-bottom: 5px;"></div>
    <p style="margin: 2px 0;">Ye T., et al. Differential Transformer. ICLR, 2025.</p>
</div>

Note:
ä¸ªäººè®¤ä¸º, çº¯ç²¹çš„ä½ç½®ç¼–ç æ˜¯åº”ä»˜ä¸äº†éœ€è¦æç«¯æ³¨æ„åŠ›çš„æƒ…å†µçš„. æˆ‘ä»¬éœ€è¦ç‰©ç†å±‚é¢çš„å¹²é¢„, æ¥å¸®åŠ© LLM å‡å°‘æ— å…³ä¿¡æ¯.
</textarea>
</section>


<!-- --------------------------------------------------------- -->

<section data-markdown>
<textarea data-template>

<div style="
  display: flex;
  justify-content: center;
  align-items: center;
  height: 40%;
  font-size: 10rem;
">
  Thanks!
</div>

</textarea>
</section>